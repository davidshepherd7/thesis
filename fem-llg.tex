\chapter{A finite element method for the Landau-Lifshitz-Gilbert Equation}
\chaptermark{Finite elements for the LLG}
\label{sec:galerk-meth-llg}

In \cref{sec:intr-finite-ele-diff} we give a general introduction to the finite element method as applied to the magnetostatic potential equation \cref{eq:Hms} in one spatial dimension.
Then in \cref{sec:llg-initial-equations,sec:time-discretisation-resi} we extend this discussion to the full (non-linear) Landau-Lifshitz-Gilbert equation with magnetostatics in three spatial dimensions.
The linearisation of the LLG equation is handled using the Newton-Raphson method (see \cref{sec:newt-raph}), hence we need to derive an analytical expression for the Jacobian matrix.
This is done in \cref{sec:llg-jacobian-calculation}.

Finally in \cref{sec:nodal-integration} we show how the geometric integration properties of IMR (as discussed in \cref{sec:time-discretisation}) can be extended from the simple ODE case to the semi-discretised FEM.

For the purposes of \thisref{sec:galerk-meth-llg} we will assume that suitable boundary conditions on the magnetostatic potential have been already been determined; the details of this process are discussed in \cref{sec:hybr-finit-elem}.

\section{Introduction to the finite element method}
\label{sec:intr-finite-ele-diff}

The following simple problem is used to illustrate the finite element method:
Find $\phim(x)\in C^{2}(0,1)$\footnote{The set $C^n(\magd)$ is the set of all functions $f$ such that $f$ and its first $n$ derivatives are continuous on $\magd$.} such that
\begin{equation}
  -\phim''(x) = f(x) \qquad x \in [0,1],
  \label{eq:poisson1}
\end{equation}
with boundary conditions
\begin{equation}
  \phim(0)=\phim_{a},\quad \phim(1)=\phim_{b},
\end{equation}
and where $f(x)\in C^0([0,1])$.
Setting $f = -\div \mv = -\pd{m_x}{x}$ results in the magnetostatic potential problem \cref{eq:Hms} with Dirichlet boundary conditions.

Before we can continue we need some definitions:
The function space $L^{2}(\magd)$ consists of all functions $g$ such that the norm $\norm{g}_\ltwo = \sqrt{\intd{\abs{g(x)}^{2}}}$ is finite.
This can be thought of as the set of functions such that their squares are integrable.

The $n$-th Sobolev space, $\sob^n(\magd)$, is the space of all functions $g$ such that $g$ and all its first $n$ derivatives are in $L^2(\magd)$, \eg for real valued functions on the interval the second Sobolev space is
\begin{equation}
  \label{eq:H1}
  \sob^2([0,1]) = \setst{ g : [0,1] \rightarrow \mathbb{R} }{ g, \pd{g}{x}, \pd{^2g}{x^2} \in L^2([0,1])}.
\end{equation}
Note that in higher dimensions \emph{all combinations} of derivatives in all directions (\eg $\pd{^2\phim}{x\partial y}$) must be integrable.
Another notation sometimes used for Sobolev spaces is $W^{n,p}(\magd)$, which denotes the set of functions $g$ such that $g$ and its first $n$ derivatives are in $L^p(\magd)$, \ie the extension of $\sob^n(\magd)$ to other $L^p(\magd)$ norms.

We write the Jacobian of a vector function $\av$ (with $n$ values) with respect to the vector $\bv$ (with $m$ values) as
\begin{equation}
  \pd{\av(\bv, \ldots)}{\bv} = \begin{pmatrix}
    \pd{a_0}{b_0} & \pd{a_0}{b_1} & \cdots & \pd{a_0}{b_m} \\
    \pd{a_1}{b_0} & \ddots & & \vdots \\
    \vdots &  & \ddots & \vdots \\
    \pd{a_n}{b_0} & \cdots & \cdots & \pd{a_n}{b_m}
  \end{pmatrix}.
  \label{eq:jac-def}
\end{equation}


\subsection{A weak formulation}
\label{Derivation-of-weighted-residuals}

The formulation of the problem in \cref{eq:poisson1} (called a strong or classical formulation) is too restrictive for our purposes.
It disallows some useful approximating functions and restricts the geometries that can be modelled \cite[14]{HowardElmanDavidSilvester2006}.
For ``well behaved'' functions and domains a \emph{weak formulation} is equivalent but without these restrictions.
The weak formulation of \cref{eq:poisson1} is: find $\phim\in V_{s}$ such that
\begin{equation}
  -\intui{\phim'' v} = \intui{f v} \qquad \forall v \in V_{T},
  \label{eq:26}
\end{equation}
where $V_{T}$ is some appropriate space of \emph{test functions}.
We require that $V_{T}\subseteq \sob^0([0,1])$ and $V_s \subseteq \sob^2([0,1])$ in order to ensure that the integrals in \cref{eq:26} are well defined.
So we as the solution space we choose
\begin{equation}
  \begin{aligned}
    V_{s} &= \setst{\phim \in \sob^2([0,1])}{\phim(0) = \phim_{a},\, \phim(1) = \phim_{b}},
  \end{aligned}
\end{equation}
\ie the space of functions that are sufficiently smooth for the integrals to be finite and that satisfy the boundary conditions.
Since the boundary conditions are automatically enforced by the choice of $V_s$ we choose test functions that are zero on the boundary, then an appropriate test function space is:
\begin{equation}
  \label{eq:28}
  \begin{aligned}
    V_{T} &= \setst{ v \in \sob^0([0,1]) }{ v(0) =0,\; v(1)=0 }.
  \end{aligned}
\end{equation}

To see (in an intuitive sense) why \cref{eq:26} is equivalent to \cref{eq:poisson1} consider the case when $f(x) + \phim''(x) > 0$ for some interval $X \subset (0,1)$, \ie when the strong form \cref{eq:poisson1} is not satisfied on $X$.
Since \cref{eq:26} must hold for all (sufficiently smooth) functions in we can choose a function which is non-zero only on $X$ and \cref{eq:26} will fail.
The rigorous extension of this argument also covers the case when $f(x) + \phim''(x)$ is non-zero on a set of measure zero (\eg at a finite set of points). ??ds fill in: roughly why is it ok?

The smoothness restrictions on the solutions of our weak form approximation can be further reduced by integrating the left hand side of \cref{eq:26} by parts to obtain
\begin{equation}
  -\intui{ \phim''v } = -\bigs{\phim' v}_{0}^{1} + \intui{ \phim'v' } = \intui{f v}.
  \label{eq:29}
\end{equation}
Note that the square bracketed term in \cref{eq:29} is zero because the test functions, $v$, are set to zero on the boundary.
Hence we are left with the problem: Find $\phim \in V_S'$ such that
\begin{equation}
  \intui{ \phim'v' } = \intui{ fv } \quad \forall v \in V'_T,
  \label{eq:symmetric-weak-poisson}
\end{equation}
where
\begin{equation}
  \begin{aligned}
    V'_S &= \setst{\phim \in \sob^1([0,1]) }{  \phim(0) = \phim_{a},\, \phim(1) = \phim_{b}}, \\
     V'_T &= \setst{ \in \sob^1([0,1]) }{  v(0) =0,\; v(1)=0  }.
  \end{aligned}
\end{equation}
Note that our test and solution spaces are now identical apart from the boundary values.

An alternative class of boundary condition, called Neumann conditions, specify the values of $\phim'$ at the boundary.
In this case we do not set the test functions to be zero on the boundary since the boundary \emph{values} must still be determined.
However, since we are given the value of $\phim'$, we can calculate the term $\bigs{\phim' v}_{0}^{1}$ from \cref{eq:29} directly.

\subsection{Discretisation}

\Cref{eq:symmetric-weak-poisson} is still continuous (\ie the test and solution spaces are still infinite dimensional) and so it cannot be implemented as a computational method.
To convert our continuous weak form to a discrete version we approximate the infinite dimensional test function space by an $N$ dimensional subset $V_{T}^{h} \subset V'_T$ such that as $N \rightarrow \infty$ we have $V_{T}^{h} \rightarrow V'_T$.
Since $V_{T}^{h}$ is finite dimensional we can write
\begin{equation}
  V_{T}^{h} = \setst{\testh }{ \testh = \sum_{n=0}^{N}\alpha_{\ndi} \tbf_{\ndi},\, \alpha_{\ndi} \in \real},
  \label{eq:30}
\end{equation}
for some finite set of linearly independent test basis functions $\tbf_{\ndi}$ (with the obvious extension for Dirichlet boundary conditions).
Note that
Similarly, for $V_S^h \subset V'_S$:
\begin{equation}
  \label{eq:34}
  V_{S}^{h} = \setst{\phimh }{ \phimh = \sum_{l=0}^{N_l}\phimdis_l \sbf_l,\, \phimdis_l \in \real},
\end{equation}
with the equivalent requirement when $N \rightarrow \infty$.
We call $\sbf_l$ the solution basis functions but they are often known as the shape functions.
Different choices of $\tbf_{\ndi}$ and $\sbf_l$ lead to different methods, the choice corresponding to the finite element method will be discussed in \cref{sub:Actual-Finite-Elements}.
For now we continue with a description of the conversion to a general discrete form.


Replacing $\testh$ from \cref{eq:poisson1} by $\tbf_{\ndi}$ gives an equivalent set of conditions
\begin{equation}
  \intui{  \phim'_h \tbf_{\ndi}'  }  = \intui{ f\tbf_{\ndi} } \qquad n=0,\ldots,N,
  \label{eq:discrete_weak_test_fns_replaced}
\end{equation}
Since $\phimh\in V_{S}^{h}$ we can replace $\phimh$ by a linear combination of the solution basis functions, \ie
\begin{equation}
  \phimh = \sum_{l=0}^{N_l} \phimdis_l\sbf_l,
  \label{eq:y-spans}
\end{equation}
where $\phimdis_l \in \real$ are the unknown coefficients which define the approximate solution $\phimh$.
Substituting \cref{eq:y-spans} into \cref{eq:discrete_weak_test_fns_replaced} gives
\begin{equation}
  \sum_{l=0}^{N_l} \phimdis_l\intui{ \sbf_l'\tbf'_{\ndi} } =\intui{ f\tbf_{\ndi} }
  \qquad n=0,\ldots,N.
  \label{eq:31}
\end{equation}

The functions $f$, $\tbf$ and $\sbf$ are known and so the integrals in \cref{eq:31} can be evaluated.
Hence we introduce a matrix $\Am$ and a vector $\bv$ containing the results of these integrals:
\begin{equation}
  \begin{aligned}
    \Am_{l\ndi} &= \intui{ \sbf_l'\tbf'_{\ndi} }, \\
    b_{j} &= \intui{ f\tbf_{\ndi} }, \\
    \label{eq:Aij_bj}
  \end{aligned}
\end{equation}
and the problem is reduced to a system of linear equations.
Writing $\phimdis$ for the vector of $\phimdis_l$ the system is
\begin{equation}
  \Am \phimdis = \bv.
  \label{eq:final_galerkin}
\end{equation}
Assuming that $\Am$ is non-singular this system can be solved to find $\phimdis$, which can then be substituted into \cref{eq:y-spans} to give an approximation for $\phim(x)$.


\subsection{Finite Elements in One Dimension}
\label{sub:Actual-Finite-Elements}

To create a usable implementation of the methodology derived in so far in \thisref{sec:galerk-meth-llg} we must need to define basis functions for the spaces $V_{T}^{h}$ and $V_S^h$.
We choose the test and solution basis functions to be the same except at the boundaries.
This choice defines a Galerkin method and results in a symmetric matrix $\Am$ \cite[215]{Zeinkiewicz1967}.

The aim now is to approximate an arbitrary function by a linear combination of a finite set of easy-to-work-with functions, $\tbf_\ndi$.
A good class of functions to satisfy aim is the polynomials: they are trivial to add, subtract and differentiate; and can be easily numerically integrated (see \cref{sec:numer-eval-integrals}).

Additionally it is desirable that the final matrix $\Am$ be sparse (to allow efficient linear algebra).
For this to happen we need $\intui{ \sbf_l'\tbf'_{\ndi} } = 0$ for most pairs of $l, \ndi$.
An obvious way to achieve this is to have each basis function only overlap (in space) with a few other basis functions, for example by having each basis function non-zero only on a portion of the domain.
Basis functions with this property are said to have ``finite support''.

These two ideas lead us to the use of a carefully constructed set of \emph{piecewise polynomials} as our basis functions.
We first split the domain into $N_e$ non-overlapping regions called \emph{elements}, each with a \emph{node} at either end.
We then define two ``local'' basis functions within each element as
\begin{equation}
  \tbf^{(e)}_{0}=\dfrac{x-x^{(e)}_{1}}{x^{(e)}_{0}-x^{(e)}_{1}},\qquad
  \tbf^{(e)}_{1}=\dfrac{x-x^{(e)}_{0}}{x^{(e)}_{1}-x^{(e)}_{0}},
  \label{eq:simple_lagrange}
\end{equation}
where $x^{(e)}_0$ and $x^{(e)}_1$ are the positions of the nodes of element $e$.
This definition results in following useful properties of $\tbf^{(e)}_{\ndi}(x)$:
\begin{equation}
  \label{eq:35}
  \tbf^{(e)}_{\ndi}(x_{\ndi}) =
  \begin{cases}
    1 & k = \ndi, \\
    0 & k \neq \ndi.
  \end{cases}
\end{equation}
This means that the coefficient of each basis function in \cref{eq:y-spans} corresponds directly to the value of the solution at the node where that test function is non-zero
In other words the list of coefficients, $\phimdis$, is just a list of the nodal values of $\phim(x)$.

Next we define the $\ndi$th (global) basis function as:
\begin{equation}
  \label{eq:33}
  \tbf_\ndi(x) =
  \begin{cases}
    \tbf_\ndi^{(e)}(x) & \text{if $x \in [x^{(e)}_0, x^{(e)}_1]$ and node $\ndi$ is in element $e$,} \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation}
So each global basis function is only non-zero on the elements neighbouring the node where it has a maximum value.
We call the set of such polynomial basis functions $\tsbasis$.
The relationship between the local and global test functions is illustrated in \cref{fig:local_global_functions}.

\begin{figure}
  \center
  \includegraphics[width=0.8\textwidth]{./images/local_global_functions}
  \caption{The local and global piecewise polynomial basis functions and numbering schemes for a one-dimensional finite element model.  ??ds numbering: change $e_i$ to element $e$, $e_{i+1}$ to $e+1$, $x_i$ to $x_\ndi$, $x^(i)$ to $x^(e)$ $L_i$ to $\tbf_\ndi$
  }
  \label{fig:local_global_functions}
\end{figure}

The global basis functions are clearly in $\sob^1([0,1])$ since they are just piecewise polynomials.
So to obtain basis functions which satisfy the requirements on $V_T^h$ and $V_S^h$ we merely need to take the intersection with $\Dfs_0$ or $\Dfs$ respectively (\ie we discard the basis functions which do not obey the conditions on the boundary).

It is advantageous to work with such low order polynomials even for very complex models because the evaluation of integrals and interpolated values is simple and cheap to compute.
Also note that an arbitrary level of accuracy can be achieved when approximating a smooth function by piecewise polynomials by increasing the number of polynomials used (\ie by adding more nodes and elements to the mesh in appropriate places).

The description here generalises to higher dimensions fairly easily \cite[20]{HowardElmanDavidSilvester2006}.
In 2D square or triangle elements are typically used and nodes are placed at every corner of each element.
In 3D ``brick'' or tetrahedral elements are used in the same way.
In both cases the polynomial basis functions are replaced by higher dimensional ones with equivalent properties.

The method can also be generalised to higher order polynomials, although more nodes are required inside each element in order to provide enough values to fully define the polynomials.
The use of higher order polynomials increases the order of accuracy at the cost of speed: the resulting linear systems become more dense as the order of the polynomials is increased due to more basis functions overlapping with each other.

To calculate the matrix $\Am$ and the vector $\bv$ from \cref{eq:Aij_bj} we first calculate the local contributions from each element: $\Am^{(e)}$ and $\bv^{(e)}$.
We then assemble the global $\Am$ and $\bv$ by summing the appropriate local contributions.
For our one-dimensional Poisson example the calculation of $\Am^{(e)}$ is quite
simple: substituting the two local basis functions \cref{eq:simple_lagrange} into
\cref{eq:Aij_bj} and writing $h=x_{1}^{(e)} - x_{0}^{(e)}$ we are left with
\begin{equation}
  \Am^{(e)} = \frac{1}{h}
    \begin{pmatrix}
      1 & -1 \\
      -1 & 1
    \end{pmatrix},
\end{equation}
where $h$ is the element size, $h = x_{1}^{(e)}-x_{0}^{(e)}$ (note that $h$ can vary between elements).
Similarly for $\bv^{(e)}$ we obtain
\begin{equation}
  \bv^{(e)}=\frac{1}{h}
    \begin{pmatrix}
      -\int_{x_{0}^{(e)}}^{x_{1}^{(e)}}(x-x_{1}^{(e)})\, f(x)\, dx\\
      \int_{x_{0}^{(e)}}^{x_{1}^{(e)}}(x-x_{0}^{(e)})\, f(x)\, dx
    \end{pmatrix}
\end{equation}
which we compute numerically since the entries depend on $f(x)$.



\subsection{Numerical evaluation of integrals}
\label{sec:numer-eval-integrals}

In the above example the integrals for $\Am$ were fixed and so could be calculated analytically but this is not usually the case (\eg the integrals for the vector $\bv$), so we need some method to evaluate the integrals computationally.
The most flexible way to evaluate integrals is numerically using a quadrature scheme.
A quadrature scheme is an approximation to an integral of the form
\begin{equation}
  \begin{aligned}
    \intdx{f(\xv)} &\approx  \sum_i w_i f(\xv_i),
  \end{aligned}
\end{equation}
where $\xv_i$ are the \emph{knots} (or evaluation points) and $w_i$ the weights.
The choice of $\xv_i$ and $w_i$ defines the quadrature scheme.

There exist many quadrature schemes suited for different applications.
Since we only need to evaluate integrals of polynomial functions a single simple and accurate class of quadrature is sufficient: Gaussian quadrature \cite[492]{Kincaid2002}.
A Gaussian quadrature scheme using $n$ knots is able to exactly evaluate integrals of polynomials up to order $2n + 1$, the highest possible order, because the knots and weights are chosen to minimise the error for polynomial integration.

In higher dimensions the integration is not quite so simple.
Typically a quadrature scheme only applies for integration over a specific shape of element (or at they least must be non-trivially modified for alternative geometries).
To get around this issue we use a coordinate transformation to a reference element \cite[29]{HowardElmanDavidSilvester2006}, \eg any four sided element is mapped to the square $[-1, 1] \times [-1, 1]$, meaning that standard quadrature schemes can be used.

We call the coordinates on the reference element the ``local coordinates'' and denote them as $\sv$, we call the original coordinates the ``global coordinates''.
We use a transformation (from local to global coordinates) that can be written similarly to \cref{eq:y-spans}:
\begin{equation}
  \xv(\sv) = \sum_j \xv_j \sbf_j(\sv),
\label{eq:88}
\end{equation}
this is known as an isoparametric transformation.
The final difficulty is to calculate the Jacobian of this transformation:
\begin{equation}
  J = \abs{ \pd{\sv}{\xv}} = \frac{1}{\abs{\pd{\xv}{\sv}}},
\end{equation}
using the notation defined in \cref{eq:jac-def} for the Jacobian of derivatives of a function.
Note that the elements of the Jacobian matrix $\pd{\xv}{\sv}$ can be calculated by simply differentiating \cref{eq:88}.
When using linear polynomial basis functions the Jacobian is constant for 1D line elements, 2D triangular elements and 3D tetrahedral elements.

So the final integration scheme is
\begin{equation}
  \begin{aligned}
    \intdx[\magd_e]{f(\xv)} &= \intds[\magd_e]{f(\xv(\sv)) J_e(\sv)}, \\
    &\approx  \sum_{i=0}^N w_i f(\xv(\sv_i)) J_e(\sv_i).
  \end{aligned}
\end{equation}

Gaussian quadratures for square and cube elements can be constructed from the one dimensional quadrature using a tensor product formula.
Similar quadratures for triangular and tetrahedral elements exist \eg \cite{Dunavant1985} \cite{Rathod2006}.




\section{The FEM applied to the LLG equation}
\label{sec:llg-initial-equations}

We now apply the finite element method as discussed in \cref{sec:intr-finite-ele-diff} (together with the Newton-Raphson discussed in \cref{sec:newt-raph}) to the solution of the Landau-Lifshitz-Gilbert equation.
We use the Gilbert form of the LLG, \cref{eq:Gilbert}, with the effective fields described in \cref{sec:energy-magnetic-body} and use the potential method described in \cref{sec:magstat-field-calc-pote} to calculate the magnetostatic field.

We choose the Gilbert form of the LLG even though it is less intuitive than the Landau-Lifshitz form because it only contains single cross product terms, reducing the complexity of derivations.
A side effect of this choice is that explicit time integration schemes cannot be used since the time derivative is only defined implicitly.

\subsection{The strong form}

After non-dimensionalisation (see \cref{sec:land-lifsh-gilb-normalisation}) the strong form of our system of equations is given by
\begin{equation}
  \begin{aligned}
    \dmdt &= - \mv \times \hv + \dampc \mv \times \dmdt, \\
    \hv &= \happ - \grad \phim + \lap \mv + \kone (\mv \cdot \ev) \ev, \\
    \lap \phim &= \div \mv.
    \label{eqn:ndllg-starting}
  \end{aligned}
\end{equation}
We prefer the vector notation used here over tensor (\ie subscript) notation throughout this thesis because of the common occurrence of cross-products and the complexity of representing a cross-product as a dummy summation over the Levi-Civita tensor.


Recall that we are using the Newton-Raphson method to find a solution to the non-linear problem.
Hence we need to rearrange \cref{eqn:ndllg-starting} into the residual form
\begin{align}
  \label{eq:llgR}
  \Rllg(\mv, \phim) &= \dmdt + \mv \times \hv - \dampc \mv \times \dmdt,  \\
  \Rphi(\mv, \phim) &= \lap \phim - \div \mv, \label{eq:phiR}
\end{align}
where $\hv$ is as in \cref{eqn:ndllg-starting}.
A pair of functions $\mv \in (C^2(\magd))^3$, $\phim \in C^2(\magd)$ is a solution to this strong form of the equations if $\Rllg(\mv, \phim)=0$ and $\Rphi(\mv, \phim)=0$ for all $\xv \in \magd$.

For now we consider the magnetostatic potential only within the magnetic domain, $\magd$, with Neumann or Dirichlet boundary conditions on the boundary, $\boundd$.
Details of the extension to include the effects of the external region using the FEM/BEM method are given in \cref{sec:hybr-finit-elem}, for which both the Dirichlet and Neumann cases will be required.
We define $\boundd_\Dir$ and $\boundd_\Neu$ respectively to represent the part of the boundary where a Dirichlet condition or Neumann is imposed on the potential,\footnote{These must be such that $\boundd_\Neu \cap \boundd_\Dir = \emptyset$ and $\boundd_\Neu \cup \boundd_\Dir = \boundd$.} \ie
\begin{equation}
  \begin{aligned}
    \phim(\xv) &= g_\Dir(\xv) \quad  \xv \in \boundd_\Dir, \\
    \grad \phim(\xv) \cdot \nv(\xv) &= g_\Neu(\xv) \quad  \xv \in \boundd_\Neu,
  \end{aligned}
\end{equation}
where $\nv(\xv)$ is the outward unit normal.
We also define the following function spaces for convenience
\begin{equation}
\begin{aligned}
  \label{eq:037}
  \Dfs & = \setst{ v }{ v(\xv) = g_\Dir(\xv) \;  \xv \in \boundd_\Dir }, \\
  \Dfs_0 &= \setst{ v }{ v(\xv) = 0 \;  \xv \in \boundd_\Dir }.
\end{aligned}
\end{equation}

From \cref{sec:magn-bound-cond} we have that the boundary condition on the magnetisation is
\begin{equation}
  \label{eq:m-bc}
  \mv \times \dmdn = \zerov \quad \xv \in \boundd.
\end{equation}


\subsection{The weak form}
\label{sec:weak-form-residuals}

We now convert \cref{eq:llgR,eq:phiR} into the residual weak form as described in \cref{Derivation-of-weighted-residuals}.
In the following we will need the identity\footnote{This can be easily derived by applying the product rule to $\div (v \grad \phim)$.}
\begin{equation}
  (\lap \phim) \test = \div (v \grad \phim) - \grad \phim \cdot \grad v.
  \label{eq:20}
\end{equation}
We will also make use of the divergence theorem
\begin{equation}
  \intd{\div \fv} = \intb{\fv \cdot \nv}.
  \label{eq:div-thm}
\end{equation}

Multiplying \cref{eq:phiR} by a test function and integrating over the magnetic domain we obtain the weak form residual for the magnetostatic potential
\begin{gather}
  \rphi(\phim, \test) = \intd{ (\lap \phim) \test }
  - \intd{ (\div \mv) \test}, \label{eqn:phires1}
\end{gather}
where $\phim \in V_{S\phim} = \setst{\phim \in \sob^2(\magd)}{\phim(\xv) = g_\Dir(\xv) \,\forall \xv \in \boundd_\Dir }$ and $\mv \in \sob^1(\magd)$.
Recall that aim is to find some $\phim$ such that $\rphi(\phim, \test) = 0$ for all test functions $\test \in V_T = \setst{\phim \in \sob^0(\magd)}{\phim(\xv) = 0 \,\forall \xv \in \boundd_\Dir }$.
Note that while the use of the Newton-Raphson method is unnecessary for the magnetostatic potential alone (since \cref{eqn:phires1} is linear in $\phim$) it will be necessary later for the coupled problem.

We now reduce the order of the derivatives on $\phim$, as discussed in \cref{Derivation-of-weighted-residuals}, by ``transferring'' the derivatives onto the test functions.
Integrating \cref{eq:20} over the domain and applying the divergence theorem \cref{eq:div-thm} we obtain
\begin{equation}
  \begin{aligned}
    \intd{ (\lap \phim) \test} &=  \intd{\div (\test \grad \phim)}
           - \intd{\grad \phim \cdot \grad \test},\\
    &= \intb{\test (\grad \phim \cdot \nv)}
    - \intd{ \grad \phim \cdot \grad \test}.
    \label{eqn:identitygauss}
  \end{aligned}
\end{equation}
Substituting \cref{eqn:identitygauss} into \cref{eqn:phires1} gives
\begin{equation}
  \rphi(\phim, \test) = \intb{ \test (\grad \phim \cdot \nv) }
  - \intd{ \grad \test \cdot \grad \phim }
  - \intd{ (\div \mv) \test },
\end{equation}
where the function spaces are now $V'_{S\phim} = \setst{\phim \in \sob^1(\magd)}{\phim(\xv) = g_\Dir(\xv) \,\forall \xv \in \boundd_\Dir }$ and $V'_T = \setst{\phim \in \sob^1(\magd)}{\phim(\xv) = 0 \,\forall \xv \in \boundd_\Dir }$.

Note that the boundary integral is always zero on $\boundd_\Dir$ by the definition of the test functions.
Hence the boundary integral is only non-zero over $\boundd_{\Neu}$ where $\grad \phim \cdot \nv = g_{\Neu}$ and we have
\begin{equation}
  \rphi(\phim, \test) = \int_{\boundd_\Neu} \test g_\Neu \d \boundd
  - \intd{ \grad \test \cdot \grad \phim}
  - \intd{ (\div \mv) \test}.
  \label{res:contphi}
\end{equation}

Next we apply the same procedure to the Landau-Lifshitz-Gilbert residual \cref{eq:llgR}.
For now we sidestep the details of time discretisation by assuming $\dmdt$ to be just another (sufficiently smooth) function of $\xv$.
The weak form of the LLG residual is
\begin{equation}
  \begin{aligned}
    \rllgv(\mv, \test) = \int_\magd &\Big( \dmdt
    + (\mv \times \hca) + (\mv \times \happ) \\
    &- (\mv \times \grad \phi) + (\mv \times \lap \mv) - \dampc \left( \mv
      \times \dmdt \right) \Big) \test \d\magd.
    \label{res:contllg}
  \end{aligned}
\end{equation}
The problem is solved by $\mv \in \sob^2(\magd)$ such that $\rllgv(\mv, \test) = \zerov$ $\forall \test \in \sob^0(\magd)$.

Note that in \cref{res:contllg} the residual is a vector.
However, we can also express the requirement for each component of \cref{res:contllg} to be zero as
\begin{equation}
  \rllg(\mv, \testv) = \int_\magd \Big( \ldots  \Big) \cdot \testv \d\magd = 0,
\end{equation}
where $\testv \in (\sob^0(\magd))^3$ and the bracketed term is exactly the same as in \cref{res:contllg}.
To see that $\rllg(\mv, \testv) = 0\; \forall \testv \in (\sob^0(\magd))^3$ implies $\rllgv(\mv, \test) = 0 \; \forall \test \in \sob^0(\magd)$ consider the vector test function $(\test, 0, 0)$.
Clearly $\rllg(\mv, (\test, 0, 0)) = \bigs{\rllgv(\mv, \test)}_x$, and similarly for the $y$ and $z$ cases.
To see the inverse relationship note that if $\rllg(\mv, \test) = 0\; \forall \test \in \sob^0(\magd)$ then each term of every dot product in $\rllg(\mv, \testv)$ is zero, implying that the total residual is zero $\forall \testv \in (\sob^0(\magd))^3$.
The use of vector test functions makes certain manipulations simpler so, for the remainder of \thisref{sec:weak-form-residuals}, we will work with vector test functions.

We again wish to reduce the order of the required derivatives, this time on $\mv$ in the exchange term.
First we rearrange to find that
\begin{equation}
  \begin{aligned}
    I &= \intd{\lap \mv \cdot (\testv \times \mv)}, \\
      &= \intd{\lap \mv \cdot \bv}, \\
      &= \sum_{i=1}^3 \intd{ b_i (\lap m_i)},
  \end{aligned}
\label{eq:73}
\end{equation}
where $\bv = (\testv \times \mv)$.
Note that each term of the sum in \cref{eq:73} is very similar to \cref{eqn:phires1}, and so we follow the same procedure to obtain
\begin{equation}
  I = \sum_{i=1}^3 \bigs{\intb{b_i (\grad m_i \cdot \nv)} - \intd{\grad b_i \cdot \grad m_i}}.
\label{eq:75}
\end{equation}
Rearranging the first term of \cref{eq:75} we find that
\begin{equation}
  \begin{aligned}
    \sum_{i=1}^3 \intb{b_i (\grad m_i \cdot \nv)}
    &= \intb{\bv \cdot \pd{\mv}{\nv}}, \\
    &=  \intb{(\testv \times \mv) \cdot \pd{\mv}{\nv}}, \\
    &=  \intb{(\mv \times \pd{\mv}{\nv}) \cdot \testv},
  \end{aligned}
\end{equation}
which exactly the weak form of \cref{eq:m-bc} (the Neumann-like boundary condition on $\mv$).
Hence the first term of \cref{eq:75} is zero in this case.

Alternatively if we have periodic boundary conditions we can write this integral as a sum over opposite boundaries $\boundd_j$ and $\hat{\boundd}_j$.
On opposite boundaries $\nv$ becomes $-\nv$ but the geometry and $\mv$ values must be identical, so we have
\begin{equation}
  \begin{aligned}
    \intb{(\mv \times \pd{\mv}{\nv}) \cdot \testv}
    &= \sum_j \bigs{\intd[\boundd_j]{\mv \times \pd{\mv}{\nv}}
    + \intd[\hat{\boundd}_j]{\mv \times \pd{\mv}{(-\nv)}}}, \\
    & = \sum_j \quad 0 ,
    \label{eq:95}
  \end{aligned}
\end{equation}
which is identical to the Neumann condition.

Next we need an expression for $\grad b_i$.
To avoid the requirement for tensor notation it is helpful to define, for a vector $\av$
\begin{equation}
\gradv \av = \threevec{\grad a_0}{\grad a_1}{\grad a_2}.
\end{equation}
Then using the product rule, we have
\begin{equation}
  \begin{aligned}
    \gradv \bv &= \gradv \bigb{\testv \times \mv}, \\
    & = (\gradv \testv) \times \mv + \testv \times (\gradv \mv).
  \end{aligned}
\end{equation}
Using this notation we can write
\begin{equation}
  I =  \\,
  - \intd{\gradv \mv : \bigb{\gradv \testv \times \mv}}
      - \intd{\gradv \mv : \bigb{\testv \times \gradv \mv}},
      \label{eq:72}
\end{equation}
where ``$:$'' is the component-wise scalar product, \ie $\gradv \av : \gradv \cv = \grad a_0 \cdot \grad b_0 + \grad a_1 \cdot \grad b_1 + \grad a_2 \cdot \grad b_2$.

Note that the $:$ operator obeys the usual triple product identity
\begin{equation}
  \av : (\av \times \bv) = \zerov.
  \label{triple:-identity}
\end{equation}
% \cref{triple:-identity}, because
% \begin{equation}
%   \begin{aligned}
%     \threevecnum{\av} : \bigb{\threevecnum{\av} \times \bv} &= \av_0 \cdot (\av_0 \times \bv) + \ldots, \\
%     & = 0 + 0 + 0
%   \end{aligned}
% \end{equation}
Hence the second term of \cref{eq:72} is zero and after reordering the first term we are left with
\begin{equation}
  \label{eq:final-lap-residual}
  \intd{ (\mv \times \lap \mv) \cdot \testv } = - \intd{\bigb{\mv \times \gradv \mv} : \gradv \testv}.
\end{equation}

The final residual for the LLG equation, using a vector test function, is
\begin{equation}
  \begin{aligned}
    \label{eq:llg-res-final}
    \rllg(\mv, \testv) = \int_\magd &\dmdt\cdot \testv
    + (\mv \times \hca)\cdot \testv
    + (\mv \times \happ)\cdot \testv
    - (\mv \times \grad \phi)\cdot \testv \\
    & - \bigb{\mv \times \gradv \mv} : \gradv \testv
    - \dampc \left( \mv \times \dmdt \right)\cdot \testv
    \d\magd.
  \end{aligned}
\end{equation}

The final weak formulation is:
\begin{equation}
  \eqpar{Find $\mv \in (\tsinf)^3$, $\phim \in V'_{S\phim}$ such that $\rllg(\mv, \testv) = 0$ for all $\testv \in (\tsinf)^3$ and $\rphi(\phim, \test) = 0$ for all $\test \in V'_T$.}
\label{eq:inf-dim-problem}
\end{equation}




\subsection{Spatial Discretisation}
\label{sec:spat-discr-resi}

The next step is to choose a finite dimensional approximation for the infinite dimensional test and solution function spaces such that the residuals \cref{res:contphi,eq:llg-res-final} become discrete in space.
Just as in \cref{sub:Actual-Finite-Elements} we choose to represent the finite dimensional spaces using a set of piecewise linear polynomials, $\tsbasis$, as basis functions.

The approximations to the unknown functions $\mv$ and $\phim$ are represented as a sum over the solution basis functions $\sk \in \tsbasis$ (or,  for $\phim$, $\sk \in \tsbasis[\phim] = \setst{\sk \in \tsbasis}{\sk(\xv) = 0 \forall \xv \in \boundd_\Dir}$) as
\begin{equation}
  \begin{aligned}
    \mv(\xv) &\approx \mvh = \sum_{k = 0}^{N} \sk \, \mvdis_k, \\
    \phim(\xv) &\approx \phimh = \sum_{k = 0}^{N} \sk \, \phim_{k}.
    \label{eq:unknowns-basis}
  \end{aligned}
\end{equation}
Similarly the approximations to the test functions are represented as a sum over the test basis functions, $\tn \in \tsbasis$ (or $\tsbasis[\phim]$ as applicable), as
\begin{equation}
  \label{eq:47}
  \test \approx \testh = \sum_{\ndi = 0}^{N} \tn \, a_\ndi.
\end{equation}
Note that in our method $\sbf_k \equiv \tbf_k$ except for the basis functions of  $\phim$ at the Dirichlet boundary.
Despite this we continue to write the test and solution basis functions differently for generality.
In one dimension the $\sk$ and $\tn$ are as given in \cref{eq:simple_lagrange}.

So substituting the basis representations, \cref{eq:unknowns-basis,eq:47} into the continuous residuals we obtain approximations for the residuals in terms of a finite number of nodal values.
Next we replace the requirement that the residuals be zero for all test functions in the infinite dimensional set $\tsinf$ with the equivalent requirement for all the test basis functions in the finite set $\tsbasis$.
This results in a finite number of conditions (the residuals) on a finite number of variables (the nodal values) and so the spatial discretisation is complete.

% As described in \cref{sub:Actual-Finite-Elements} we also convert this ``global'' representation into a ``local'' representation on each element.

% Let $\magd_\eli$ represent the volume of element $e$, let $\boundd_\eli$ represent any part of the boundary of the element which is on $\boundd$ (nothing for most elements).
% Then the contribution of element $\eli$ to the residuals at node $\ndi$ is exactly as given in equations~\cref{res:tintro}-\cref{res:tllg} except that the integrations are performed over $\magd_\eli$ and $\boundd_\eli$ rather than $\magd$ and $\boundd$.
% Also note that the sums only need to consider values of $k$ such that node $k$ is in element $\eli$ and that residual contributions only need to be calculated for nodes $\ndi$ such that node $\ndi$ is in element $\eli$.


\section{Time Discretisation}
\label{sec:time-discretisation-resi}

In \thisref{sec:time-discretisation-resi} we discuss how to deal with the time derivative in the LLG residual \cref{eq:llg-res-final}.
As discussed in \cref{sec:numer-meth-micr}, we will apply standard integration methods widely used for ODEs.
This technique of applying independent spatial and time discretisations is known as semi-discretisation (or the method of lines).

However, we cannot simply apply the time integration methods in the way they are written in \cref{sec:some-implicit-time-integrators} (and in most text books) because our LLG residual \cref{eq:llg-res-final} is an \emph{implicit} function of the time derivative and so cannot be written in the form $\pd{\yv}{t} = \fv(t, \yv)$.
Instead we rearrange the time integration method to give discrete substitutions for the time derivative, the unknown function $\yv$ and $t$.

Let $\dtn$ be the size of the $n$th time step and $x_n$ denote value of $x$ at the $n$-th time-step.\footnote{This notation has some potential to be confused with the nodal value notation of \cref{sec:spat-discr-resi} but the meaning will be clear from the context.}
The simplest case is the first order backwards difference method (BDF1), which is defined by the substitution
\begin{equation}
    \pd{\yv}{t} \rightarrow \frac{\yv_{n+1} - \yv_n}{\dtn}, \quad
    \yv \rightarrow \yv_{n+1}, \quad t \rightarrow t_{n+1}.
    \label{eq:impl-bdf1}
\end{equation}
This substitution can be obtained by simple algebraic rearrangements of \cref{eq:bdf1-definition}.
Similarly, the second order backwards difference method (BDF2) can be found (with some more complex algebra) to be:
\begin{equation}
  \begin{aligned}
    \pd{\yv}{t} &\rightarrow \yv_{n+1}\bigb{\frac{1}{\dtn} + \frac{1}{\dtn + \dtx{n-1}}}
    - \yv_n \frac{\dtn + \dtx{n-1}}{\dtn\dtx{n-1}}
    + \yv_{n-1} \frac{\dtn}{(\dtn + \dtx{n-1})\dtx{n-1}}, \\
    \yv &\rightarrow \yv_{n+1}, \quad t \rightarrow t_{n+1}.
    \label{eq:impl-bdf2}
  \end{aligned}
\end{equation}

The implicit midpoint rule (IMR) is defined by the substitutions
\begin{equation}
  \begin{aligned}
    \pd{\yv}{t} &\rightarrow \frac{\yv_{n+1} - \yv_n}{\dtn}, \quad
    \yv &\rightarrow \frac{\yv_{n+1} + \yv_n}{2}, \quad
    t &\rightarrow \frac{t_{n+1} + t_n}{2}.
    \label{eq:impl-imr}
  \end{aligned}
\end{equation}
Alternatively, as mentioned in \cref{sec:some-implicit-time-integrators}, we can define the implicit midpoint rule in terms of a step of BDF1 with $\dtn^\bdfo = \dtn^\imr/2$ to obtain $\yv_{n+\half}$ followed by the algebraic update
\begin{equation}
  \begin{aligned}
    \label{eq:imr-bdf1}
    \yv_{n+1} &= 2\yv_{n+\half} - \yv_n.
  \end{aligned}
\end{equation}

The trapezoid rule (TR) is defined (recursively) by
\begin{equation}
  \begin{aligned}
    \pd{\yv}{t} &\rightarrow \evalatb{\pd{\yv}{t}}{n+1}
    = \frac{2(\yv_{n+1} - \yv_n)}{\dtn} - \evalatb{\pd{\yv}{t}}{n}, \\
    \yv &\rightarrow \yv_{n+1}, \quad t \rightarrow t_{n+1}.
    \label{eq:impl-tr}
  \end{aligned}
\end{equation}

By applying one of the substitutions \cref{eq:impl-bdf1}-\cref{eq:impl-tr} to the system of spatially discretised residuals we obtain a fully discrete problem.
The properties of these time discretisations are exactly as discussed in \cref{sec:time-discretisation}.

\subsection{The fully discretised residuals}

Here we write the residual using the scalar test function form because it is the suitable for use in a computer implementation.
We define the effect of $\gradv$ on a scalar function $\tbf$ to be $\gradv \tbf = (\grad \tbf, \grad \tbf, \grad \tbf)$, this allows us to use scalar and vector test functions in the same way in the following.

We write $\dmdth$ for the fully discrete time derivative approximations at step $n+1$ and $\mvh$, $\phimh$ for the fully discrete approximations to $\mv$, $\phim$ as required for the time derivative.\footnote{For BDF, TR methods $\mvh$ is as defined in \cref{eq:unknowns-basis}. For IMR it is the average of $\mvh$ at step $n+1$ and $\mvh$ at step $n$.}
Then the fully discrete residuals are
\begin{equation}
  \label{res:llg-discrete}
  \begin{aligned}
    \rllgv_n = \int_{\magd}
    & \dmdth \tbf_n + \mvh \times \left( \happ + \hca - \grad \phim - \dampc \dmdth
    \right) \tbf_n \\
    &- (\mvh \times \gradv \mvh) : \gradv \tbf_n)
    \d\magd,
  \end{aligned}
\end{equation}
and
\begin{equation}
  \label{res:phi-discrete}
  \begin{aligned}
    \rphi_n = \int_{\boundd_\Neu} \tbf_n g_\Neu \d \boundd
    - \intd{ \grad \tbf_n \cdot \grad \phimh}
    - \intd{ (\div \mvh) \tbf_n}.
  \end{aligned}
\end{equation}
for each test basis function $\tbf_n$.

\section{The Newton-Raphson Jacobian}
\label{sec:llg-jacobian-calculation}

To solve our fully discrete system by the Newton-Raphson method (see \cref{sec:newt-raph}) we also need to know the Jacobian matrix of each residual differentiated with respect to each variable at the $(n+1)$th time step.
This can be handled by numerical differentiation (\eg by approximating each derivative using a finite difference method), but analytical Jacobians are typically faster to compute and give better convergence properties than such numerical approximations.
Additionally, for the solution of the linear systems, it is often useful to know analytically the block structure of the system being solved.

Note that the ``real'' implementation of the implicit midpoint rule \cref{eq:impl-imr} introduces a factor of $1/2$ in various places due to the substitution $ \yv \rightarrow \frac{\yv_{n+1} + \yv_n}{2}$, the derivations below do not include this ??ds yet, fix it!

Throughout \thisref{sec:llg-jacobian-calculation} we use the notation of $\pd{\av(\bv, \ldots)}{\bv}$ for the Jacobian of derivatives of $\av$ with respect to $\bv$ as shown in \cref{eq:jac-def}.

We first note that the effect of differentiation by a nodal value on the approximation function $\phimh$ is quite simple:
\begin{equation}
  \begin{aligned}
    &\pd{\phimh}{\phimdis_l} = \pd{}{\phimdis_l} \bigs{ \sum_k \phimdis_k \sk } = \sbf_l, \\
    &\pd{\phimh}{\mvdis_l} = \pd{}{\mvdis_l} \bigs{ \sum_k \phimdis_k \sk } = \zerov^T, \\
  \end{aligned}
\end{equation}
and similarly for the interpolated magnetisation, $\mvh$:
\begin{equation}
  \begin{aligned}
    &\pd{\mvh}{\mvdis_l} = \pd{}{\mvdis_l} \bigs{ \sum_k \mvdis_k \sk } = \sbf_l \Idm_3, \\
    &\pd{\mvh}{\phimdis_l} = \pd{}{\phimdis_l} \bigs{ \sum_k \mvdis_k \sk } = \zerov, \\
\end{aligned}
\end{equation}
where $\Idm_3$ is the $3\times3$ identity matrix.

\subsection{Poisson Jacobian}
\label{sec:poisson-jacobian}

Starting from the Poisson residual, \cref{res:contphi}, and omitting terms that contain no dependence on $\phim_i$ we get
\begin{equation}
  \begin{aligned}
    \label{eq:poisson-jacobian}
    \Am_{n,l} &= \pd{}{\phimdis_l} \sum_k \intd{ -(\grad \tbf_n \cdot \grad \sbf_k) \phim_{k}}, \\
    &= -\intd{\grad \tbf_n \cdot \grad \sbf_l}.
  \end{aligned}
\end{equation}
This block of the Jacobian is comparatively simple because the Poisson equation is linear.
It corresponds to the well known discrete Laplacian operator \cite[57]{HowardElmanDavidSilvester2006}.


\subsection{LLG Jacobian}
\label{sec:llg-jacobian}

Unfortunately in most of the Jacobian calculations we have multiple terms depending on $\mvh$ joined together by a cross product.
The process of differentiating these terms can be made much easier by making use of the ``skew operator'' which represents a cross product as a matrix-vector multiplication.
For $\av = (a_0, a_1, a_2)^T$ the skew operator is given by
\begin{equation}
  \skewm{\av} = \text{skew}(\av) =
  \begin{pmatrix}
    0 & -a_2 & a_1 \\
    a_2 & 0 & -a_0 \\
    -a_1 & a_0 & 0
  \end{pmatrix}.
  \label{eqn:skew}
\end{equation}
Note that
\begin{equation}
  \skewm{\av} \cdot \bv
  = \begin{pmatrix}
    0 & -a_2 & a_1 \\
    a_2 & 0 & -a_0 \\
    -a_1 & a_0 & 0
  \end{pmatrix}
  \cdot \threevec{b_0}{b_1}{b_2}
  = \threevec{-a_2b_1 + a_1b_2}{a_2b_0 - a_0b_2}{-a_1b_0 + a_0b_1}
  = \av \times \bv.
\end{equation}

Some useful properties of this skew operator are:
\begin{itemize}
\item The skew operator commutes with differentiation, \ie
  \begin{equation}
    \pd{}{x} \skewm{\av} = \skewm{ \pd{\av}{x} }.
  \end{equation}

\item The skew operator is linear
  \begin{equation}
    \skewm{\av + c \bv} = \skewm{\av} + c \skewm{\bv}.
  \end{equation}

\item The Jacobian of a cross-product can be easily represented using the skew operator as
  \begin{equation}
    \pd{}{\av} \bigb{ \skewm{\av} \cdot \bv } = \begin{pmatrix}
      0 & b_2 & -b_1 \\
      -b_2 & 0 & b_0 \\
      b_1 & -b_0 & 0
    \end{pmatrix} = -\skewm{\bv}.
    \label{eq:61}
  \end{equation}
  To see this we calculate the derivative with respect to $a_0$:
  \begin{equation}
    \pd{}{a_0} \bigb{ \skewm{\av} \cdot \bv } = \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & -1 \\
      0 & 1 & 0
    \end{pmatrix}\bv = \threevec{0}{-b_2}{b_1},
  \end{equation}
  this is the first column of $-\skewm{\bv}$.
  Repeating the calculation for the derivatives with respect to $a_1$ and $a_2$ results in \cref{eq:61}.
  This simplification is the main advantage of the skew operator notation in the following Jacobian calculations.
\end{itemize}

Due to the linearity of the skew operator we can write the Jacobian of a cross product including $\mvh$ as
\begin{equation}
  \begin{aligned}
    \pd{}{\mvdis_l} \bigb{\skewm{\mvh} \bv} &= \pd{}{\mvdis_l} \skewm{\intp{\mv}} \bv, \\
    &= \sum_k \sbf_k \pd{}{\mvdis_l} \bigb{\skewm{\mvdis_k} \bv},
  \end{aligned}
\end{equation}
then using the fact that $\pd{\mvdis_l}{\mv_{k}} = \delta_{lk} \pd{\mvdis_l}{\mvdis_l}$ we get
\begin{equation}
  \pd{}{\mvdis_l} \bigb{\skewm{\mvh} \bv}  = \sbf_l \pd{}{\mvdis_l} \bigb{\skewm{\mvdis_l} \cdot \bv}.
\end{equation}
Next, by using \cref{eq:61} we obtain
\begin{equation}
  \pd{}{\mvdis_l} \bigb{\skewm{\mvh} \bv} = - \sbf_l\skewm{\bv},
  \label{eq:diff-skew-interp}
\end{equation}
and finally by combining \cref{eq:diff-skew-interp} with the product rule we obtain the identity
\begin{equation}
  \begin{aligned}
    \pd{}{\mvdis_l} \bigb{\skewm{\mvh}\cdot \gv(\mvh)}
    &= \skewm{\mvh}\cdot \pd{\gv(\mvh)}{\mvdis_l} - \sbf_l \skewm{\gv(\mvh)}.
    \label{eq:68}
  \end{aligned}
\end{equation}
This provides us with a simple representation for almost all of the terms in the LLG Jacobian.

% As a simple example of how this can be used we first examine the differentiation of the $\mvh \times (\happ + \hms)$ term of the Landau-Lifshitz equation residual (note that these two fields are not directly dependent on $\mv$).
% \begin{equation}
%\begin{aligned}
%   \circled{c} &= \mv \times (\happ + \hms),\\
%               &= \skewm{\mv} \cdot (\happ + \hms).
% \end{aligned}
%\end{equation}
% Now it is easy to see that applying \cref{eq:68} with $\gv = \happ + \hms$ gives us the Jacobian contribution
% \begin{equation}
%\begin{aligned}
%   \pd{}{\mvdis_l} \circled{c} = - \sbf_l \skewm{\happ + \hms},
% \end{aligned}
%\end{equation}
% since $\pd{\gv(\mv)}{\mvdis_l} \equiv 0$.

Now we calculate the Jacobian itself.
Written in terms of the skew operator the fully discrete LLG residual, \cref{res:llg-discrete}, becomes
\begin{equation}
  \begin{aligned}
    \rllgv_n = \rllgv(\mvh, \tbf_n) = \int_{\magd}
    & \dmdth \tbf_n + \skewm{\mvh} \cdot \left( \happ + \hca - \grad \phim - \dampc \dmdth
    \right) \tbf_n \\
    &- (\skewm{\mvh} \grad \mvh) : \grad \tbf_n)
    \d\magd.
  \end{aligned}
\end{equation}

To simplify the following calculations we write the Jacobian as the sum of the individual contributions:
\begin{equation}
  \begin{aligned}
    \Fm_{n,l} &= \pd{\rllgv_n}{\mvdis_l} = \Am_{n,l} + \Bm_{n,l} + \Cm_{n,l} + \Dm_{n,l}, \\
    \Am_{n,l} &= \pd{}{\mvdis_l} \intd{\dmdth \tbf_n}, \\
    \Bm_{n,l} &= \pd{}{\mvdis_l} \intd{- \skewm{\mvh} \cdot (\grad \mvh \compdot \grad \tbf_n)}, \\
    \Cm_{n,l} &= \pd{}{\mvdis_l} \intd{ \skewm{\mvh} \cdot \hca  \tbf_n}, \\
    \Dm_{n,l} &= \pd{}{\mvdis_l} \intd{  \skewm{\mvh} \cdot \bigb{ \happ - \grad \phim - \dampc \dmdth} \tbf_n}.
  \end{aligned}
\end{equation}

The first contribution is simple
\begin{equation}
  \begin{aligned}
    \Am_{n,l} &= \pd{}{\mvdis_l} \intd{ \dmdth \tbf_n } \\
           &= \jts \Idm_3 \intd{ \sbf_l \tbf_n},
  \end{aligned}
\end{equation}
where $\jts$ is the constant that multiplies $\yv_{n+1}$ in the approximation of the time derivative, for example when using the implicit midpoint rule $\cts = 1$.
So $\Am$ is a $3\times3$ block diagonal matrix where each block is a mass matrix multiplied by a constant depending on the time integration scheme.

Next we calculate the contribution from the damping, applied field and magnetostatic field.
If we write the damping term in the form of \cref{eq:68} we have $\gv(\mvh) = \dmdth$, and
\begin{equation}
  \label{eq:69}
  \pd{\gv(\mvh)}{\mvdis_l} = \pd{}{\mvdis_l} \left(\pd{\mvh}{t} \right) = \jts \sbf_l \Idm_3.
\end{equation}
Now using \cref{eq:68} with \cref{eq:69} we immediately obtain
\begin{equation}
  \begin{aligned}
    \Dm_{n,l} &= \pd{}{\mvdis_l} \intd{  \skewm{\mvh} \cdot
      \left( \happ - \grad \phim - \dampc \dmdth
      \right) \tbf_n}, \\
    &= \int_\magd \tbf_n \big( -\sbf_l\skewm{\happ} + \sbf_l\skewm{\grad \phi} + \dampc\sbf_l \skewm{\dmdth} - \dampc\sbf_l \jts \skewm{\mvh} \Idm_3
    \big) \d\magd, \\
    &= \intd{ \tbf_n \sbf_l \bigb{ \skewm{-\happ + \grad \phi
          + \dampc \dmdth - \dampc \jts \mvh}}}.
    \label{eq:field-damping-jacobian}
  \end{aligned}
\end{equation}
This is a $3\times3$ skew-symmetric matrix where each element is a mass matrix element multiplied by a collection of field/magnetisation components.

We turn now to the contribution due to magnetocrystalline anisotropy.
Writing the residual in the form of \cref{eq:68} we have
\begin{equation}
  \gv(\mvh) = \hca = \kone (\mvh \cdot \ev) \ev,
\end{equation}
and thus
\begin{equation}
  \begin{aligned}
    \pd{\gv(\mvh)}{\mvdis_l} &= \pd{}{\mvdis_l} \bigb{ \kone (\mvh \cdot \ev) \ev}, \\
    &= \kone \bigb{ \pd{}{m_{x,l}} (\mvh \cdot \ev),
      \pd{}{m_{y,l}} (\mvh \cdot \ev),
      \pd{}{m_{z,l}} (\mvh \cdot \ev) }  \tensorprod \ev, \\
    &= \kone \sbf_l \; \ev  \tensorprod \ev.
    \label{eq:70}
  \end{aligned}
\end{equation}
Then combining \cref{eq:68,eq:70} gives
\begin{equation}
  \begin{aligned}
    \Cm_{n,l} &= \pd{}{\mvdis_l} \intd{ \skewm{\mvh} \cdot \hca  \tbf_n}, \\
    &= \underbrace{\intd{ -\tbf_n \sbf_l\skewm{\hca} }}_{\Cm'_{n,l}}
    + \underbrace{\kone \intd{ \tbf_n \sbf_l \skewm{\mvh} \; (\ev  \tensorprod \ev) }}_{\Jmca_{,n,l}}.
    \label{eq:hca-jacobian}
  \end{aligned}
\end{equation}
Note that $\Cm'$ is of the same form as $\Dm$ and $\Jmca$ is a $3\times3$ block matrix with no empty blocks in the general case.
However, if $\ev$ is aligned with any of the Cartesian axes (as is typical) then only one entry of the matrix $(\ev \tensorprod \ev)$ is non-zero and only one block of $\Jmca$ is non-empty.

Finally we calculate the exchange contribution.
We first derive the identity
\begin{equation}
  \begin{aligned}
    \pd{}{\mvdis_l} \left(\gradv \mvh \compdot \gradv \tbf\right)
    &= \pd{}{\mvdis_l} \threevecdup{\left( \sum_k \mvdis_k \grad \sbf_k \right) \cdot \grad \tbf}, \\
    &=  \sum_k \pd{\mvdis_k}{\mvdis_l} \threevecdup{\grad \sbf_k \cdot \grad \tbf}, \\
    &=  \Idm_3 \bigb{ \grad \sbf_l \cdot \grad \tbf }.
    \label{eq:71}
  \end{aligned}
\end{equation}
Then using \cref{eq:68,eq:71} we get
\begin{equation}
  \begin{aligned}
   \Bm_{n,l} &=  \pd{}{\mvdis_l} \intd{- \skewm{\mvh} \cdot (\gradv \mvh \compdot \gradv \tbf_n)} ,\\
    &= \intd{ \sbf_l \skewm{\gradv \mvh \compdot \gradv \tbf_n}
       - \skewm{\mvh} \left( \grad \sbf_l \cdot \grad \tbf_n \right)}, \\
     &= \intd{\skewm{ \sbf_l \gradv \mvh \compdot \gradv \tbf_n
       - \mvh \left( \grad \sbf_l \cdot \grad \tbf_n \right)}},
   \label{eq:exc-jacobian}
   \end{aligned}
 \end{equation}
 which is a skew-symmetric $3\times 3$ matrix with each element reminiscent of an element of the discrete Laplacian \cref{eq:poisson-jacobian}.

When written in block form the total LLG Jacobian is
\begin{equation}
  \Fm =
  \begin{pmatrix}
    \jts \Mm    & -\Km_z       & \Km_y \\
    \Km_z         & \jts\Mm    & -\Km_x \\
    -\Km_y        & \Km_x        & \jts \Mm
  \end{pmatrix} + \Jmca,
  \label{eq:llg-jacobian}
\end{equation}
where $\Mm$ is the mass matrix
\begin{equation}
  \label{eq:mass-matrix}
  \Mm_{i,j} = \intd{\tbf_i \sbf_j};
\end{equation}
$\Km_i$ are the skew-symmetric contributions $\Bm$, $\Cm'$ and $\Dm$ of \cref{eq:exc-jacobian,eq:hca-jacobian,eq:field-damping-jacobian}; and $\Jmca$ is from \cref{eq:hca-jacobian}.


\subsection{LLG-magnetostatics coupling Jacobians}
\label{sec:llg-magn-coupl}

The non-linear problem as written in \cref{eq:inf-dim-problem} is the ``monolithic'' form, \ie all equations are solved simultaneously.
The Jacobian for this complete non-linear system is
\begin{equation}
  \Jm =
  \begin{pmatrix}
    \Fm   & \Pm \\
    \Qm   & \Am \\
  \end{pmatrix},
  \label{eq:monolithic-jac}
\end{equation}
where $\Am$ and $\Fm$ have been derived in \cref{sec:poisson-jacobian} and \cref{sec:llg-jacobian} respectively; $\Pm = \pd{\rllgv}{\phim}$ and $\Qm = \pd{\rphi}{\mvh}$.
In \thisref{sec:llg-magn-coupl} we derive expressions for $\Pm$ and $\Qm$.
Note that it is also possible (and common) to solve the non-linear problem \cref{eq:inf-dim-problem} as two separate systems: one for $\mvh$ then one for $\phim$.
More details and the advantages of each approach are discussed in \cref{sec:solution-strategies}.

Now we derive $\Pm_{n,l}$.
It is a $3 \times 1$ block matrix because we are dealing with a vector of three residuals differentiated by one variable ($\phimdis_l$).
Most of the terms in the LLG residual are zero when differentiated with respect to any $\phimdis_l$ so the calculation is quite simple.
From \cref{res:llg-discrete}, using discrete approximations and dropping the zero terms we have
\begin{equation}
  \begin{aligned}
    \Pm_{n,l} &= \pd{\rllgv_n}{\phimdis_l}
    = \pd{}{\phimdis_l} \intd{ -\tbf_n \mvh \times \grad \phimh}, \\
    &= -\intd{ \tbf_n \mvh \times \grad \sbf_l}, \\
    % &= -\intd{ \tbf_n \threevec{m_y\pd{\sbf_l}{z} - m_z \pd{\sbf_l}{y}}
    %   {-m_x\pd{\sbf_l}{z} + m_z \pd{\sbf_l}{x}}
    %   {m_x\pd{\sbf_l}{y} - m_y \pd{\sbf_l}{x}}}.
  \end{aligned}
\end{equation}

Next we derive $\Qm_{n,l}$, which is a $1 \times 3$ block matrix because we differentiate by the three components of $\mvh$.
From \cref{res:phi-discrete} we have
\begin{equation}
  \begin{aligned}
    \Qm_{n,l} &= \pd{\rphi_n}{\mvdis_l} = \pd{}{\mvdis_l} \intd{ -\tbf_n \div \mvh}, \\
    &= \sum_j \pd{}{\mvdis_l} \intd{ -\tbf_n \left( \pd{\sbf_j}{x} m_{h,x,j}
        + \pd{\sbf_j}{y} m_{h,y,j} + \pd{\sbf_j}{z} m_{h,z,j} \right) }, \\
    &= -\intd{ \tbf_n \left( \pd{\sbf_l}{x},
        \pd{\sbf_l}{y}, \pd{\sbf_l}{z} \right) }, \\
    &= -\intd{\tbf_n (\grad \sbf_l)^T}.
  \end{aligned}
\end{equation}


So in block form the complete Jacobian is
\begin{equation}
  \Jm =
  \begin{pmatrix}[ccc|c]
    \jts \Mm    & -\Km_z       & \Km_y      & \Pm_x \\
    \Km_z         & \jts\Mm    & -\Km_x     & \Pm_y \\
    -\Km_y        & \Km_x      & \jts \Mm & \Pm_z \\
    \hline
    \Qm_x       & \Qm_y      & \Qm_z    & \Am     \\
  \end{pmatrix} + \Jmca,
  \label{eq:full-monolithic-jac}
\end{equation}
where $\Jmca$ is appropriately zero padded.



\section{Geometric integration with the finite element method}
\label{sec:nodal-integration}

In weak form based methods it turns out that the magnetisation length conservation property of the IMR does not automatically apply.
The problem is that (outside of the infinite node limit) the weak form equations enforce properties of the \emph{integral} of the solution, so with FEM and IMR $\int_{\magd_e} \abs{\mvh} -1 \d\magd_e = 0$ is enforced rather than the equivalent nodal property.
The solution is to use evaluate the integrals using a specific quadrature scheme which directly links the nodal values with the integral values.
The downside of such a scheme is that the accuracy of the evaluation of integrals is reduced (since the standard quadratures are chosen for their optimal accuracy).

In the micromagnetics literature this choice of quadrature is known as reduced integration \cite{Cimrak2008}.
However in other finite element literature ``reduced integration'' refers to the use of a Gaussian quadrature whose order is not sufficient to evaluate the integrals exactly (typically this is done to reduce the computational cost of the quadrature).
\emph{Nodal integration} is the standard term for quadrature schemes where the nodal values are used directly in the quadrature (typically with mesh-free methods \eg \cite{Puso2008}), so we will use this terminology.

In \thisref{sec:nodal-integration} we first demonstrate that $\abs{\mv_j}=1$ is conserved when a nodal integration scheme is used.
Then we demonstrate that the energy property of IMR (as discussed in \cref{sec:energy-cons}) applies in our FEM + IMR model.
Finally we show in detail why the conservation of magnetisation length is lost with standard quadrature schemes.


\subsection{A local nodal integration scheme}
\label{sec:local-nodal-integr}

In order to regain conservation properties in a weak-form-based method we introduce a nodal integration scheme as used by Bartels et. al. \cite{Bartels2006}:
\begin{equation}
  \label{eq:nodal-integration}
  \int_{\magd_e} f(\xv) \d \xv \approx \sum_{l\in \set{\text{nodes in $\magd_e$}}} \beta_l f(\xv_l),
\end{equation}
where $\beta_l$ is a weight.
This is simply the weighted sum of the value of the integrand at the finite element nodes.
As an additional benefit this can greatly simplify the calculations since no interpolation of the values to the integration points is needed.

We now need to derive suitable values for the $\beta_l$.
To do so we represent the quadrature scheme as an integral of an appropriate interpolating polynomial (in fact the polynomial defines the quadrature scheme), then rearrange the equation to find the weights (see, \eg \cite[480]{Kincaid2002}).
In FEM codes quadrature is typically done in local coordinates, as discussed in \cref{sec:numer-eval-integrals}, so we calculate weights applicable in a local coordinate quadrature.

A set of quadrature-interpolation-polynomials which give the nodal integration property, \cref{eq:nodal-integration}, are the test basis functions.
So using these we have:
\begin{equation}
  \label{eq:nodal-quad-weights}
  \begin{aligned}
    \int_{\magd_e} f(\xv) \d \xv &= \int_{\magd_e} f(\sv) J(\sv) \d \sv, \\
    &\approx \int_{\magd_e} \bigb{\sum_l f(\sv_l) \tbf_l(\sv)} J(\sv) \d \sv, \\
    &\approx \sum_l f(\sv_l) \int_{\magd_e} \tbf_l(\sv) J(\sv) \d \sv,
  \end{aligned}
\end{equation}
where $J = \determinant{\pd{\sv}{\xv}}$ is the Jacobian of the transformation from local to global coordinates.
Comparing \cref{eq:nodal-integration} with \cref{eq:nodal-quad-weights} we see that
\begin{equation}
  \begin{aligned}
    \beta_l &= \int_{\magd_e} \tbf_l(\sv) J(\sv) \d \sv \bigb{= \int_{\magd_e} \tbf_l \d\xv}.
  \end{aligned}
\end{equation}

If $f(\sv)$ is a first order polynomial then our nodal quadrature scheme is exact.
This is worse than the Gaussian quadrature with the same number of quadrature points, where polynomials of up to 3rd order are integrated exactly.
As such most terms of the LLG residual are not integrated exactly.
However the exchange residual contribution is only a first order polynomial, for example the $x$ component is
\begin{equation}
  \begin{aligned}
    r_{\text{exch},x} = - \intd{ m_y \bigb{\grad \tbf \cdot \grad m_z}} + \intd{ m_z \bigb{\grad \tbf_n \cdot \grad m_y}},
  \end{aligned}
\end{equation}
where the gradient terms are constants ($m_i$ and $\tbf_n$ are linear polynomials so their first derivatives are constant).
Hence the exchange contribution is integrated exactly.

Note that with this quadrature scheme we have that
\begin{equation}
  \intd{\sbf_k \tbf_n} = c \delta_{kn},
\end{equation}
meaning that the mass matrix, \cref{eq:mass-matrix}, is diagonal.
This makes the nodal quadrature scheme a form of \emph{mass-lumping} which is often used in the FEM in combination with explicit time integration to remove the requirement for a linear solve.

\subsection{Conservation of $\abs{\mv}$}
\label{sec:weak-cons-absmv}

After reducing the norm of the discrete LLG residual to tolerance $\ntol$ we have for each $j$ (using vector test function notation):
\begin{equation}
  \norm{\intd{ \bigs{\dmdth  + \mvh \times \bigb{\hv - \dampc \dmdth}} \cdot \tbfv_j + \bigb{\mvh \times \grad \mvh} : \grad \tbfv_j}} < \ntol,
  \label{eq:81}
\end{equation}
where $\norm{\cdot}$ is the norm used to assess the convergence of the linearisation method (in our implementation this is $\norm{\cdot}_\infty$) and $\tbfv_j$ is the vector test basis function corresponding to node $j$.
Hence, by writing any test function $\testv$ as a linear combination of the basis functions and then using the linearity of $\tbfv$ in \cref{eq:81} and the triangle inequality, we have
\begin{equation}
  \norm{\intd{ \bigs{\dmdth + \mvh \times \bigb{\hv - \dampc \dmdth}} \cdot \testv + \bigb{\mvh \times \grad \mvh} : \grad \testv}} < \ntol  \sumbasiscoeff,
  \label{eq:82}
\end{equation}
for all $\testv \in (\ts)^3$ where $\sumbasiscoeff = \sum_j  \abs{c_{x,j}} + \abs{c_{y,j}} + \abs{c_{z,j}}$ is the sum of all coefficients in the basis representation of $\testv$.

\newcommand{\mvmp}[1][j]{\mv_{{#1},\half}}
\newcommand{\hvmp}[1][j]{\hv_{{#1},\half}}
\newcommand{\dmmp}[1][j]{\dmdt_{{#1},\half}}

In \thisref{sec:weak-cons-absmv} we write the midpoint value (in time) of the magnetisation at the $j$th node as $ \mvmp = (\mv_{j,n} + \mv_{j,n+1})/2$.
Similarly for the magnetic field we write $\hvmp = \hv((t_n+t_{n+1})/2, \xv_j, \mv_j)$ and the time derivative of the magnetisation $\dmmp = (\mv_{j,n+1} - \mv_{j,n})/\dtn$.

For the purposes of this derivation we choose the test function to be $\testv = \mvmp \tbf_j$, \ie the midpoint value of the magnetisation at node $j$ multiplied by the corresponding test basis function.
We know that $\mvmp\tbf_j \in  (\ts)^3$ because it is simply a vector valued multiple of a basis function of $\ts$.

Using the new notation and this test function we have that
\begin{equation}
  \begin{aligned}
    \Bigg|\Bigg| \sum_l \beta_l \bigg[& \bigb{\dmmp[l] + \mvmp[l] \times \bigb{\hvmp[l] - \dampc \dmmp[l]}} \cdot \mvmp\tbf_j(\xv_l) \\
    &+ \bigb{\mvmp[l] \times \grad \mvmp[l]} : \grad (\mvmp \tbf_j(\xv_l)) \Big] \Bigg|\Bigg| < \ntol \sumbasiscoeff.
  \end{aligned}
\end{equation}
Using $\tbf_j(\xv_l) = \delta_{jl}$ we can eliminate the summation, leaving only
\begin{equation}
  \begin{aligned}
    \norm{\beta_j \bigg[ \bigb{\dmmp  + \mvmp \times \bigb{\hvmp  - \dampc \dmmp}} \cdot \mvmp
    + \bigb{\mvmp \times \grad \mvmp} : \grad (\mvmp)
    \bigg] } < \ntol \sumbasiscoeff.
  \end{aligned}
\end{equation}
Now, by the properties of the triple product, the precession and damping terms vanish leaving
\begin{equation}
  \norm{\beta_j \dmmp \cdot  \mvmp } < \ntol \sumbasiscoeff.
\end{equation}
Substituting in the formula for the IMR from \cref{eq:impl-imr} gives the desired conservation result:
\begin{equation}
  \begin{aligned}
    \norm{\frac{\beta_j}{2\dtn}(\mv_{j,n+1} - \mv_{j,n}) \cdot (\mv_{j, n+1} + \mv_{j, n})} &< \ntol \sumbasiscoeff, \\
    \norm{\abs{\mv_{j, n+1}}^2 - \abs{\mv_{j, n}}^2} &< \frac{2\dtn}{\beta_j} \ntol \sumbasiscoeff.
  \end{aligned}
\end{equation}

This is applicable for all nodes $j$.
Note that for this test function $\sumbasiscoeff \sim \abs{\mvmp}^2 = \order{1}$ assuming that the solver has not gone catastrophically wrong.\footnote{Even if $\abs{\mv}=1$ does not hold (and for the purposes of this proof we cannot assume that it does), the length of the magnetisation cannot be too far away from one.}
Hence all nodal magnetisation lengths are conserved up to an error which can be controlled by decreasing the tolerance of the linearisation method.



\subsection{Energy properties}

% Midpoint values:
\newcommand{\midpoint}[1]{\hat{#1}}
\newsubcommand{\mvm}{\midpoint{\mv}}{n}
\newcommand{\tm}{\midpoint{t}_n}
\newcommand{\dtop}{\delta}
\newcommand{\pdsub}[3]{\mathrlap{\pd{#1\mathrlap{_{#2}}}{#3}}\phantom{\pd{#1_{#2}}{#3}}}
\newcommand{\dmdtm}{\dtop \mv}
\newcommand{\dmdtml}{\dtop \mv_{n,l}}
\newcommand{\dmdtmj}{\dtop \mv_{n,j}}

\newcommand{\ipg}[2]{\intd{{#1} \cdot {#2}}}

\newcommand{\nodal}{\textrm{nodal}}

We now show that the IMR/FEM discretisation discussed above also retains the energy conservation property of the IMR applied to an ODE discussed in \cref{sec:energy-cons}.

For now we assume an additional property of the solution and test function spaces: all functions must be in $\sob_h^2(\magd)$ (\ie the second derivatives are integrable).
This constraint is required so that we can use a simpler form of the LLG residual and can be relaxed by using nodal quadrature and more technical arguments which are discussed following the derivation.
We also assume that $\happ$ can be written as a linear combination of basis functions.
This is a reasonable assumption because if this is not true the spatial discretisation error will be large.
Finally we make use of the fact that the test and solution spaces coincide for the magnetisation (due to the lack of any Dirichlet boundary conditions).

As in the previous section we start from the LLG residual:
\begin{equation}
  \begin{aligned}
    &\norm{\intd{ \bigs{\dmdth + \mvh \times \bigb{\hv - \dampc \dmdth}} \cdot \testv}} < \ntol \sumbasiscoeff(\testv), \\
      &\hv = \lap \mvh - \nabla \phim + \happ(t, \xv) + \hca(\mvh),
    \end{aligned}
  \label{eq:85}
\end{equation}
and choose as the test function
\begin{equation}
  \testv = \hv - \dampc \dmdth.
  \label{eq:84}
\end{equation}
The term $\dampc \dmdth$ belongs to the space of test functions since it is a linear combination of $\mvh_{,n}$ and $\mvh_{,n+1}$, each of which is in the solution space.
Since we are using $\testv \in \sob_h^2(\magd)$ the function $\lap \mvh$ is clearly in the test function space.
The $\hca$ effective field is similarly contained in the space of test functions as it is a linear function of $\mvh$.
Finally, since the space of test functions for $\mv$ is non-zero on the boundary (\ie there are no Dirichlet conditions on $\mv$) we can represent $\grad \phim$ using them, for any boundary conditions on $\phim$.

We substitute \cref{eq:84} into \cref{eq:85} and apply the triple product identity to find that
\begin{equation}
  \norm{\intd{\dmdth \cdot \hv} - \dampc \intd{\dmdth \cdot \dmdth}} < \ntol \sumbasiscoeff.
  \label{eq:46}
\end{equation}
After substituting the IMR approximations and using a midpoint approximation for the applied field in \cref{eq:46} the expression inside the norm coincides with \cref{eq:54} of the derivation of the energy loss property for the strong form of the LLG.
Hence we proceed similarly to \cref{sec:proof-energy-prop}: we substitute  \cref{eq:50,eq:76,eq:52} into \cref{eq:46} to obtain
\begin{equation}
  \begin{aligned}
    \Bigg|\Bigg| &\frac{\e_{n+1} - \e_n}{\dtn} + \dampc \ltnorm{\frac{\mvh_{,n+1} - \mvh_{,n}}{\dtn}}^2 +
      \ltip{\frac{\happ(t_{n+1}) +\happ(t_n)}{\dtn}}{\frac{\mvh_{,n+1} +
          \mvh_{,n}}{2}} \\
      &- \ltip{\happerror}{\frac{\mvh_{,n+1} - \mvh_{,n}}{\dtn}} \Bigg|\Bigg| \quad < \ntol \sumbasiscoeff.
  \end{aligned}
\end{equation}
Finally, by using the triangle inequality, we have
\begin{equation}
  \begin{aligned}
    &\norm{ \frac{\e_{n+1} - \e_n}{\dtn} + \dampc \ltnorm{\frac{\mvh_{,n+1} - \mvh_{,n}}{\dtn}}^2 } \\
    &< \norm{\ltip{\frac{\happ(t_{n+1}) + \happ(t_n)}{\dtn}}{\frac{\mvh_{,n+1} + \mvh_{,n}}{2}} - \ltip{\happerror}{\frac{\mvh_{,n+1} - \mvh_{,n}}{\dtn}} }
  + \ntol \sumbasiscoeff.
  \label{eq:86}
  \end{aligned}
\end{equation}
The inequality \cref{eq:86} has the same interpretations as in \cref{sec:proof-energy-prop}.
In particular: if $\dampc = 0$ and $\happ$ is constant in time we have energy conservation (up to an error which can be controlled using the linearisation tolerance).

Note that the energy integrals and $\ltwo$ norms in \cref{eq:86} are not necessarily exact because we are using nodal quadrature.
The exchange energy is exact for the same reasons as discussed in \cref{sec:local-nodal-integr}, and the energy due to magnetostatics is exact for the same reason.
In contrast the magnetocrystalline anisotropy energy and $\dmdt$ norm integrals are not exact and
the Zeeman energy depends on the spatial variation of the applied field: if the field is constant in space then the conserved energy will be exact.

Also note that for the test function given in \cref{eq:84} the value of $\sumbasiscoeff$ is fairly large
\begin{equation}
  \begin{aligned}
    \sumbasiscoeff &= \order{N_n \bigb{1 + \kone + \max_{\xv} \happ(t, \xv)} }, \\
    &= \order{N_n},
  \end{aligned}
\end{equation}
and so the bound may be lose for very large numbers of nodes.
However this a pessimistic bound, the actual error is not likely to grow this large.

The restriction of $\testv \in \sob_h^2(\magd)$ introduced at the start of this section can be relaxed by considering an implicitly defined discrete Laplacian operator defined for $\av \in \sob^1(\magd)$, $\bv \in \ts$ as
\begin{equation}
  -\ip{\disclap \av(\xv)}{\bv(\xv)}_\nodal = \ltip{\grad \av(\xv)}{\grad \bv(\xv)},
\end{equation}
where
\begin{equation}
  \ip{\av}{\bv}_\nodal = \sum_{j \in nodes} \beta_j \av(\xv_j) \cdot \bv(\xv_j),
\end{equation}
\ie the nodally-integrated version of the $L^2$ inner product.
It can be shown that \cite{Bartels2007}:
\begin{equation}
  \norm{\disclap \av(\xv)}_{\infty} = \ip{\disclap \av(\xv)}{\disclap \av(\xv)}_\infty < \infty,
\end{equation}
so $\norm{\disclap \av(\xv)}_{\ltwo} < \infty$ and the required integrals are finite when calculated using nodal quadrature.
Note that $\ip{\disclap \mvh}{\mvh}_\nodal = \ltip{\grad \mvh}{\grad \mvh} = \eex$.
This means that we can replace the Laplacian operator everywhere in the derivation by the discrete Laplace operator to obtain the derivation without the restriction $\testv \in \sob_h^2(\magd)$.

\subsection{Non-conservation with Gaussian quadrature}
\label{sec:non-cons-gaussian}

We now show (by a counterexample) that the conservation result does not hold with a standard quadrature scheme.
To obtain our result we examine the case where $\testv = \mvh$.
Note that $\mvh \in (\ts)^3$ because we are using identical test and solution function spaces and each component of $\mvh$ is a linear combination of solution basis functions.

For \thisref{sec:non-cons-gaussian} we assume that the non-linear solver has converged to a tolerance of exactly zero for simplicity, \ie $\ntol = 0$.
By substituting $\testv = \mvh$ and $\ntol = 0$ into \cref{eq:82} then using the triple product identities we obtain
\begin{equation}
  \label{eq:23}
  \begin{aligned}
    0 &= \ipg{\dmdth}{\mvh}, \\
    &= \frac{1}{2\dtn} \intd{(\mvh_{,n+1} + \mvh_{,n}) \cdot (\mvh_{,n+1} - \mvh_{,n})}, \\
    &= \frac{1}{2\dtn} \intd{\abs{\mvh_{,n+1}}^2 - \abs{\mvh_{,n}}^2}.
  \end{aligned}
\end{equation}
\Cref{eq:23} suggests that we have conservation of magnetisation length.
However it is an \emph{integral} relationship, meaning that the values of the integrand at the nodes are not necessarily zero.\footnote{Since $\intd{ f(\xv)} = 0 \nRightarrow f(\xv) \equiv 0$ on $\magd$.}

We can see this in more detail by writing out the Gaussian quadrature.
Dropping the constant factor of $2\dtn$ and assuming that $\abs{\mvh_{,n}} = 1$ everywhere this gives
\begin{equation}
  \begin{aligned}
    1 &= \intd{\abs{\sum_k \mv_{k, n+1} \tbf_k(\xv)}^2}, \\
    &= \sum_l w_l \abs{\sum_k \mv_{k, n+1} \tbf_k(\xv_l)}^2.
    \label{eq:83}
  \end{aligned}
\end{equation}
\Cref{eq:83} can be satisfied without the requirement that all $\abs{\mv_{k, n+1}} = 1$ $\forall k$.
For example if we set the number of nodes to two (\ie a 1D problem with linear basis functions) and assume magnetisation only along the $z$-axis (\ie  $\mvh_{,n+1} = (0, 0, m)$) then condition \cref{eq:83} becomes (dropping the subscript $n+1$):
\begin{equation}
  \begin{aligned}
    1 &= \sum_l w_l \abs{\sum_k \mv_{k} \tbf_k(\xv_l)}^2, \\
    &= w_0 (m_{0} a + m_{1} b)(m_{0} a + m_{1} b) + w_1 (m_{0} c + m_{1} d)(m_{0} c + m_{1} d), \\
    &= m_{0}^2 (w_0 a^2 + w_1 c^2) + m_{1}^2 (w_0 b^2 + w_1 c^2) + m_{0} m_{1} (2w_0ab + 2w_1cd), \\
    &= m_{0}^2 \alpha + m_{1}^2 \beta + m_{0} m_{1} \gamma, \\
  \end{aligned}
\end{equation}
where $a,b,c,d$ are the values of basis functions at the integration points and $\alpha, \beta, \gamma$ are constants.
So given any $m_{0}$ we can solve the above expression to find an $m_{1}$ that satisfies the constraint.
Since we have chosen $\mv = (0, 0, m)$ this means we can vary the magnetisation length at node 0 arbitrarily while still satisfying the constraint.

Finally, it is interesting to note that if the magnetisation is constant in space at times $t_n$ and $t_{n+1}$ then the integral condition in \cref{eq:23} \emph{is} sufficient to give conservation of the magnetisation length at nodes.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
