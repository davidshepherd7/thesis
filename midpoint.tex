

\chapter{An adaptive implicit-midpoint-rule time-integrator}


%*** other names?

%% To make the following derivations more readable we write:
%% \begin{align}
%%   \thf &= \frac{
%%   \yvhf &= \yv(\thf), %\notag\\
%%   %% \yvhfest &= \frac{\yv_{n+1} + \yv_n}{2},
%% \end{align}
%% and we denote derivatives of $\yv$ by $\yv'$ etc.

\section{Fixed step implicit midpoint rule}

Let $\yv(t)$ be a vector function, let $\yv_n$ denote an estimate to $\yv(t)$ at $t = t_n$.
Let $\dtn = t_{n+1} - t_n$ be the $n$th time step (or just ``step'').
Then given a system of equations of the form
\begin{equation}
  \yv'(t) = \fv(t, \yv(t)),
  \label{eq:43}
\end{equation}
the implicit midpoint rule (IMR) is
\begin{equation}
    \yv_{n+1} = \yv_n + \dtn \fv(\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}).
    \notag
\end{equation}
We write
\begin{equation}
  \begin{aligned}
    \frac{t_{n+1} + t_n}{2} &= \thf, \\
    \frac{\yv_{n+1} + \yv_n}{2} &= \yvm,
  \end{aligned}
\end{equation}
for readability, giving
\begin{equation}
  \yv_{n+1} = \yv_n + \dtn \fv(\thf, \yvm).
  \label{eq:basic-midpoint}
\end{equation}

Note that unlike multistep methods, such as the second order backwards difference (BDF2), this is valid for both constant and variable step sizes because there is no dependence on previous steps.


\section{Implicit midpoint rule local truncation error}
\label{sec:deriv-local-trunc}

The local truncation error (LTE) of a time integration scheme is the error due a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$ into the approximation for the next time-step then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$.

Using this definition the local truncation error of IMR is
\begin{align}
  \lte^\IMP &= \yv(t_{n+1}) - \yv_{n+1}^\IMP, \notag\\
  &= \yv(t_{n+1}) - \yv(t_n) - \dtn \fv\left( \thf, \frac{\yv(t_n) + \yv_{n+1}^\IMP}{2} \right).
  \label{eq:trunc-start}
\end{align}

We choose to Taylor expand everything about the midpoint, $\thf$, because it reduces the complexity of the result (and allows easier calculations).\footnote{If instead chose to expand about $t_n$ there would be an additional term in $\yv_n''$ in equation~\eqref{eq:trunc-mid}.}
We assume throughout that $\yv(t)$ is ``sufficiently smooth'' to have a Taylor series expansion. Then its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf['] + \frac{\dtn^2}{8} \yvhf[''] + \frac{\dtn^3}{48} \yvhf['''] \porder{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
%% It is well known that the local truncation error of the midpoint rule is $\order{\dtn^3}$ (\ie it is second order)\cite{??ds}, so we can safely ignore $\order{\dtn^4}$ terms.
Similarly the expansion at $t_n$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf['] + \frac{\dtn^2}{8} \yvhf[''] - \frac{\dtn^3}{48} \yvhf['''] \porder{\dtn^4}.
  \label{eq:taylorn}
\end{equation}

Substituting equations~\eqref{eq:taylornp1} and \eqref{eq:taylorn} into equation~\eqref{eq:trunc-start} gives
\begin{equation}
  \lte^\IMP = \yv(t_{n+1}) - \yv_{n+1}^\IMP
  = \frac{\dtn^3}{24} \yvhf[''']  + \dtn  \left[ \yvhf[']
  - \fv\left( \thf, \frac{\yv(t_n) + \yv_{n+1}}{2} \right) \right]  \porder{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to this error: the first term (with $\yv'''_n$) is fairly standard in second order time integrators.
However the second term is more complex, applying a Taylor expansion approach here would result in a Jacobian-like matrix of derivatives of $\fv$ with respect to $\yv$.
Hence we avoid expanding it further.
See Section~\ref{sec:full-imr-lte-calculation} for details of the rest of the Taylor expansion, which also proves that IMR is indeed a second order method.


\section{Construction of an LTE estimate}

Most truncation error estimators for implicit integrators (e.g. trapezoid rule, BDF2) use a Milne-device based method.\cite[pgs 707-716]{Gresho-Sani}
This means that they compute two estimates of the value at $t_{n+1}$ to the same order of accuracy.
They then use algebraic rearrangements of the two LTE expressions to calculate an approximation of the LTE (typically to one order of accuracy better than the original calculations).
However due to the complexity of the local truncation error of the implicit midpoint rule there are some difficulties with this approach.
In particular the midpoint rule's LTE has a term giving the error due to the approximation $\yvm \sim \yvhf$.
This term cannot appear in the LTE expression for any time integrator that does not use the midpoint approximation (\ie any other useful integrator) and so cannot be easily approximated using a Milne-device-like method.

Instead we take the approach commonly used in Runge-Kutta time integrators: we cheaply repeat the calculation at a higher accuracy and compare the two answers directly to get an error estimate.
Such approaches usually rely on hard to find Runge-Kutta pairs: pairs of RK methods which share a number of evaluation points but have different orders of accuracy.
One example of this technique is the Dormandâ€“Prince (order 4/5), as used in MATLAB's \texttt{ode45} function.
However IMR uses a single evaluation at the optimal point in the time step to cause cancellation of higher order terms.
Hence there is no way to reuse this evaluation in a higher order method without at least two additional evaluations, due to the destruction of symmetry.

Instead we use a little known explicit version of the third order backwards difference method (eBDF3).
This requires only 3 history values and a single (explicit) derivative function evaluation at time $t_n$ in order to compute a 3rd order accurate step.
Alternatively a 3rd order Adams-Bashforth scheme could be used, this would require one less start-up step but would require storage of an additional derivative value.

\subsection{The variable step explicit backwards difference 3 method}

Hairer and Wanner\cite{Hairer book} give...

Used sympy\cite{??ds sympy} to generate scheme

Code at \cite{??ds github}

\subsection{Computing the size of the next step}

We use a standard method for computing the next step from the local truncation error and a target local truncation error \toltt:\cite[pg.268]{Gresho-Sani}
\begin{equation}
\dtx{n+1} = \dtn \left( \frac{\toltt}{\lte}  \right) ^{\frac{1}{\texttt{order}+1}}.
\end{equation}
For implicit midpoint rule this is
\begin{equation}
  \dtx{n+1} = \dtn \left( \frac{\toltt}{\lte^\IMP}  \right) ^{\frac{1}{3}}.
\end{equation}


\section{Numerical experiments}

\subsection{Implementation}

To increase relevance to realistic models including spatial discretisation we use a Newton method to solve \emph{all} equations (even linear ones).

Implemented in python using scipy, this allowed for rapid prototyping of new schemes. However little attention has been paid to optimisation, so wall-clock timing results would not be relevant (and so are not presented).

All code used has been open sourced and is available on github.\cite{simple-ode-github}


??ds update
Unless otherwise specified all examples in this section use a Newton tolerance of $1\E{-10}$ and an adaptive integrator tolerance of $\toltt = 1\E{-4}$.


\subsection{Test functions}

To demonstrate the effectiveness of the adaptation scheme we use a simple ODE with known exact solution
\begin{align}
  f(t,y) &= - \beta e^{-\beta t} \sin(\omega t) + \omega e^{-\beta t} \cos(\omega t) \\
  y(0) &= 0
\end{align}
which gives the exact solution
\begin{equation}
  \label{eq:59}
  y(t) = e^{-\beta t} \sin(\omega t).
\end{equation}
The numerical experiments below all use $\omega = 2 \pi$ and $\beta = 0.3$.
The exact solution for this case is shown in Figure~\ref{fig:mp-ode-exact}.
Plots of the solutions given by the various numerical calculations below are all visually indistinguishable from Figure~\ref{fig:mp-ode-exact} (without zooming in) at the tolerances used, so they are not shown.


\begin{figure}[ht!]
  \centering
  \includegraphics{images/osc_damped_ode_exact}
  \caption{A plot of equation~\eqref{eq:59}, the exact solution of our example ODE.}
  \label{fig:mp-ode-exact}
\end{figure}

\subsection{Results}

\begin{figure}[ht!]
  \centering
  \includegraphics{images/placeholder}
  \caption{Adaptive IMR vs adaptive BDF2 for a simple ODE.}
  \label{fig:mp-vs-bdf2}
\end{figure}

Figure~\ref{fig:mp-vs-bdf2} shows the performance of the adaptive midpoint scheme derived above compared to a standard adaptive BDF2 scheme.\cite[pg. 715]{Gresho-Sani}
The step size selection of the adaptive midpoint scheme closely follows that of the BDF2 scheme.


??ds discuss more about third derivative!
The pattern of step sizes is as expected.
There is a general trend of increasing step size as the solution is gradually damped out.
Additionally there is an oscillation which peaks at integer and half-integer times, this corresponds to the peaks in $\cos(\omega t)$ and $\sin(\omega t)$.
This is relevant because the truncation error is proportional to $y'''(t)$ which consists of terms in these two functions.

Also of note is that the accuracy of IMR is much higher than that of BDF2 for the same step size in this example.
??ds why? (Jacobian is zero)


Figure~\ref{fig:mp-tols} compares mean error norms and step sizes of the adaptive midpoint scheme (with two interpolation points) for varying tolerances.
The IMR can be seen to have quadratic convergence by comparing with the $y=x^2$ line shown.
Additionally it can be seen that decreasing the tolerance smoothly decreases the mean error (by smoothly decreasing the mean step size).

\begin{figure}[ht!]
  \centering
  \includegraphics{images/placeholder}
  \caption{Adaptive midpoint mean dt vs mean error for varying tolerance. The line is $y = x^2$.}
  \label{fig:mp-tols}
\end{figure}


\section{Additional Notes}

\subsection{Implicit ODEs}
\label{sec:extens-impl-odes}

We sometimes wish to solve a system of equations where $\yv'(t)$ only given implicitly\footnote{This use of ``implicit'' is unrelated to the notion of implicitness in the time integration scheme.} (for example the Gilbert form of the Landau-Lifshitz-Gilbert equation), in this case equation~\eqref{eq:43} becomes
\begin{equation}
  \fv(t, \yv(t), \yv'(t)) = 0.
\end{equation}

We note that equation~\eqref{eq:basic-midpoint} can also be written in the from
\begin{equation}
  \yv'(\thf) = \frac{\yv_{n+1} - \yv_n}{\dtn} =  \fv(\thf, \frac{\yv_n + \yv_{n+1}}{2}).
\end{equation}
So the obvious equivalent for IMR is
\begin{equation}
  \fv(\thf, \frac{\yv_{n+1} + \yv_n}{2}, \frac{\yv_{n+1} - \yv_n}{\dtn}) = 0.
\end{equation}

However the adaptive scheme requires an additional function evaluation.
For implicitly defined functions this is expensive, so this adaptive method is not efficient for equations which can only be defined in such a way.


\subsection{Order reduction}

It is known that implicit Runge-Kutta methods (such as the implicit midpoint rule) are susceptible to reduced accuracy when used to solve certain types of stiff problem.
This is represented in the second term of the full LTE expression~\eqref{eq:trunc-final}.
If $\dfdyhf \cdot \yvhf['']$ is large then the error could be much larger than expected.
This effect will be automatically detected and adjusted for by the error estimator. ??ds experimental results?

However in practice this seems to only occur in rare cases...
LLG doesn't look like it will suffer from it.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "./main"
%%% End:
