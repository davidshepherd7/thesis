

\chapter{An adaptive implicit-midpoint-rule time-integrator}


%*** other names?

%% To make the following derivations more readable we write:
%% \begin{align}
%%   \thf &= \frac{
%%   \yvhf &= \yv(\thf), %\notag\\
%%   %% \yvhfest &= \frac{\yv_{n+1} + \yv_n}{2},
%% \end{align}
%% and we denote derivatives of $\yv$ by $\yv'$ etc.

\section{Fixed step implicit midpoint rule}

Let $\yv(t)$ be a vector function, let $\yv_n$ denote an estimate to $\yv(t)$ at $t = t_n$.
Let $\dtn = t_{n+1} - t_n$ be the $n$th time step (or just ``step'').
Then given a system of equations of the form
\begin{equation}
  \yv'(t) = \fv(t, \yv(t)),
  \label{eq:43}
\end{equation}
the implicit midpoint rule (IMR) is
\begin{equation}
    \yv_{n+1} = \yv_n + \dtn \fv(\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}).
    \notag
\end{equation}
We write
\begin{equation}
  \begin{aligned}
    \frac{t_{n+1} + t_n}{2} &= \thf, \\
    \frac{\yv_{n+1} + \yv_n}{2} &= \yvm,
  \end{aligned}
\end{equation}
for readability, giving
\begin{equation}
  \yv_{n+1} = \yv_n + \dtn \fv(\thf, \yvm).
  \label{eq:basic-midpoint}
\end{equation}

Note that unlike multistep methods, such as the second order backwards difference (BDF2), this is valid for both constant and variable step sizes because there is no dependence on previous steps.


\section{Implicit midpoint rule local truncation error}
\label{sec:deriv-local-trunc}

The local truncation error (LTE) of a time integration scheme is the error due a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$ into the approximation for the next time-step then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$.

Using this definition the local truncation error of IMR is
\begin{align}
  \lte^\IMP &= \yv(t_{n+1}) - \yv_{n+1}^\IMP, \notag\\
  &= \yv(t_{n+1}) - \yv(t_n) - \dtn \fv\left( \thf, \frac{\yv(t_n) + \yv_{n+1}^\IMP}{2} \right).
  \label{eq:trunc-start}
\end{align}

We choose to Taylor expand everything about the midpoint, $\thf$, because it reduces the complexity of the result (and allows easier calculations).\footnote{If instead chose to expand about $t_n$ there would be an additional term in $\yv_n''$ in equation~\eqref{eq:trunc-mid}.}
We assume throughout that $\yv(t)$ is ``sufficiently smooth'' to have a Taylor series expansion. Then its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf['] + \frac{\dtn^2}{8} \yvhf[''] + \frac{\dtn^3}{48} \yvhf['''] \porder{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
%% It is well known that the local truncation error of the midpoint rule is $\order{\dtn^3}$ (\ie it is second order)\cite{??ds}, so we can safely ignore $\order{\dtn^4}$ terms.
Similarly the expansion at $t_n$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf['] + \frac{\dtn^2}{8} \yvhf[''] - \frac{\dtn^3}{48} \yvhf['''] \porder{\dtn^4}.
  \label{eq:taylorn}
\end{equation}

Substituting equations~\eqref{eq:taylornp1} and \eqref{eq:taylorn} into equation~\eqref{eq:trunc-start} gives
\begin{equation}
  \lte^\IMP = \yv(t_{n+1}) - \yv_{n+1}^\IMP
  = \frac{\dtn^3}{24} \yvhf[''']  + \dtn  \left[ \yvhf[']
  - \fv\left( \thf, \frac{\yv(t_n) + \yv_{n+1}}{2} \right) \right]  \porder{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to this error: the first term (with $\yv'''_n$) is fairly standard in second order time integrators.
However the second term is more complex, applying a Taylor expansion approach here would result in a Jacobian-like matrix of derivatives of $\fv$ with respect to $\yv$.
Hence we avoid expanding it further.
See Section~\ref{sec:full-imr-lte-calculation} for details of the rest of the Taylor expansion, which also proves that IMR is indeed a second order method.


\section{Construction of an LTE estimate}

Most truncation error estimators for implicit integrators (e.g. trapezoid rule, BDF2) use a Milne-device based method.\cite[pgs 707-716]{Gresho-Sani}
This means that they compute two estimates of the value at $t_{n+1}$ to the same order of accuracy.
They then use algebraic rearrangements of the two LTE expressions to calculate an approximation of the LTE (typically to one order of accuracy better than the original calculations).
However due to the complexity of the local truncation error of the implicit midpoint rule there are some difficulties with this approach.
In particular the midpoint rule's LTE has a term giving the error due to the approximation $\yvm \sim \yvhf$.
This term cannot appear in the LTE expression for any time integrator that does not use the midpoint approximation (\ie any other useful integrator) and so cannot be easily approximated using a Milne-device-like method.

Instead we take the approach commonly used in Runge-Kutta time integrators: we cheaply repeat the calculation at a higher accuracy and compare the two answers directly to get an error estimate.
Such approaches usually rely on hard to find Runge-Kutta pairs: pairs of RK methods which share a number of evaluation points but have different orders of accuracy.
One example of this technique is the Dormandâ€“Prince (order 4/5), as used in MATLAB's \texttt{ode45} function.
However IMR uses a single evaluation at the optimal point in the time step to cause cancellation of higher order terms.
Hence there is no way to reuse this evaluation in a higher order method without at least two additional evaluations, due to the destruction of symmetry.

Instead we use a little known explicit version of the third order backwards difference method (eBDF3).
This requires only 3 history values and a single (explicit) derivative function evaluation at time $t_n$ in order to compute a 3rd order accurate step.
Alternatively a 3rd order Adams-Bashforth scheme could be used, this would require one less start-up step but would require storage of an additional derivative value.

\subsection{The variable step explicit backwards difference 3 method}

% backwards version:
% \begin{equation}
%   \begin{aligned}
%     \bdiff^0 f_n &= f_n, \\
%     \bdiff^{j+1} f_n &= \bdiff^{j} f_n - \bdiff^{j} f_{n-1}, \\
%     q(t) &= q(t_n + sh), \\ 
%     &= f(t_n) + s \bdiff f(t_n) + \frac{s(s+1)}{2} \bdiff^2 f(t_n) + \cdots \frac{s(s+1)\cdots(s+n-1)}{n!}\bdiff^n f(t_n), \\
%     &= \sum_{j=1}^{n} (-1)^j \begin{pmatrix} -s\\ j \\ \end{pmatrix}
%     \bdiff^j f(t_n).
%   \end{aligned}
% \end{equation}

The explicit bdf methods are explicit time integration methods derived using the same techniques as the usual implicit BDF methods.
The idea is to write down a divided difference representation of an interpolating polynomial, $q(t)$, through the points $y_i$, $i=n-k+1, \ldots, n+1$ (backward differences can be used for constant time steps, hence the name).
The derivative of the polynomial is then equated to the derivative $y'(t, y) = f(t, y)$ given as the ODE definition\cite[pg. 400]{HairerNorsettWanner}.
Equating the derivative at time $t_{n+1}$ gives the familiar implicit BDF methods.
If we instead equate the derivative at a previous time we obtain the explicit BDF methods \cite[pg. 364]{HairerNorsettWanner}.

The Newton divided differences are defined recursively by
\begin{equation}
  \label{eqn:divided-diff}
  \begin{aligned}
    y[t_{n+1}] &= y_{n+1}, \\
    y[t_{n+1}, t_n] &= \frac{y[t_{n+1}] - y[t_n]}{t_{n+1} - t_n}, \\
    y[t_{n+1}, t_n, t_{n-1}] &= \frac{y[t_{n+1}, t_n] - y[t_n, t_{n-1}]}{t_{n+1} - t_{n-1}}, \\
    \vdots
  \end{aligned}
\end{equation} 
The Lagrange interpolation polynomials can be expressed using these divided differences \cite[pg. 124]{BurdenFaires}, \cite[pg. 400]{HairerNorsettWanner} as
\begin{equation}
  \label{eqn:divided-diff-intp}
  \begin{aligned}
    p_k(t) &= y_{n+1} + \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=0}^{j-1} (t - t_{n+1-i}), \\
    &= y_{n+1} + y[t_{n+1}, t_n](t - t_{n+1}) + y[t_{n+1}, t_n, t_{n-1}](t - t_{n+1})(t - t_n) \\
    &\quad + y[t_{n+1}, t_n, t_{n-1}, t_{n-2}](t - t_{n+1})(t - t_n)(t - t_{n-1}) + \ldots \\
    &\quad + y[t_{n+1}, \ldots, t_{n+1-k}](t-t_{n+1})\cdots(t-t_{n-k}).
  \end{aligned}
\end{equation}

Differentiating with respect to $t$ gives (divided differences are constants, product term is handled with the chain rule)
\begin{equation}
  \begin{aligned}
    p_k'(t) &= y[t_{n+1}, t_n] + y[t_{n+1}, t_n, t_{n-1}]\bigb{(t - t_n) + (t - t_{n+1})} \\
    &\quad + y[t_{n+1}, t_n, t_{n-1}, t_{n-2}]\bigb{(t - t_n)(t - t_{n-1}) + (t - t_{n+1})(t - t_{n-1}) + (t - t_{n+1})(t - t_n)} \\
    &\quad + \ldots \\
    &\quad + y[t_{n+1}, \ldots, t_{n+1-k}]\bigb{(t-t_{n})\cdots(t-t_{n-k}) + \ldots + (t-t_{n+1})\cdots(t-t_{n-k+1})}, \\
    &= \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \sum_{l=0}^{j-1} \prod_{i=0, i \neq l}^{j-1} (t - t_{n+1-i}).
  \end{aligned}
\end{equation} 

Equating the derivative at time $t_n$ results in
\begin{equation}
  \begin{aligned}
    f(t_n, y_n) &= \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \sum_{l=0}^{j-1} \prod_{i=0, i \neq l}^{j-1} (t_n - t_{n+1-i}).
  \end{aligned}
\end{equation} 
This can be greatly simplified by noticing that the product $ \prod_{i=0, i \neq l}^{j-1} (t_n - t_{n+1-i})$ is zero whenever it contains $i=1$, \ie whenever $j \geq 2$ and $l \neq 1$.
However when $j=1$ the only terms in the sum of products are $l=0$ and $i=0$, so we only need to consider the case of $l=1$.
This gives
\begin{equation}
  \begin{aligned}
    f(t_n, y_n) &= \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=0, i \neq 1}^{j-1} (t_n - t_{n+1-i}), \\
    &= y[t_{n+1}, t_n] + \sum_{j=2}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=0, i \neq 1}^{j-1} (t_n - t_{n+1-i}), \\
    &= y[t_{n+1}, t_n] -\dtn \sum_{j=2}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=2}^{j-1} (t_n - t_{n+1-i}). \\
  \end{aligned}
\end{equation}

For $k=1$ we get
\begin{equation}
  \begin{aligned}
    f(t_n, y_n) &= y[t_{n+1}, t_n], \\
    &= \frac{y_{n+1} - y_n}{\dtn}, \\
    y_{n+1} &= y_n + \dtn f(t_n, y_n), \\
  \end{aligned}
\end{equation} 
which is forward Euler.

For $k=2$ we have
\begin{equation}
  \begin{aligned}
    f(t_n, y_n) &= y[t_{n+1}, t_n] - \dtn y[t_{n+1}, t_n, t_{n-1}] , \\
    &=  \frac{y_{n+1} - y_n}{\dtn} 
    - \frac{\dtn}{\dtn + \dtx{n-1}} \bigb{ \frac{y_{n+1} - y_n}{\dtn} - \frac{y_n - y_{n-1}}{\dtx{n-1}}}.
  \end{aligned}
\end{equation}
After some algebra this gives
\begin{equation}
  \label{eq:emr-derivation-rearranged}
  y_{n+1} = y_n + \bigb{1 + \frac{\dtn}{\dtx{n-1}}}\dtn f(t_n, y_n)
    - \frac{\dtn^2}{\dtx{n-1}^2}(y_n - y_{n-1}),
\end{equation}
which is the variable step equivalent of the leapfrog method (explicit midpoint rule) \cite[pg. 715]{GreshoSani}.

For larger values of $k$ the messyness of the algebra grows explosively.
Unfortunately we are interested in the third order method, so:
\begin{equation}
  \begin{aligned}
    f(t_n, y_n) &= y[t_{n+1}, t_n] - \dtn y[t_{n+1}, t_n, t_{n-1}] -\dtn y[t_{n+1}, t_n, t_{n-1}, t_{n-2}], \\
    &=  \frac{y_{n+1} - y_n}{\dtn} 
    - \frac{\dtn}{\dtn + \dtx{n-1}} \bigb{ \frac{y_{n+1} - y_n}{\dtn} - \frac{y_n - y_{n-1}}{\dtx{n-1}}} \\
    &\quad - \frac{\dtn}{\dtn + \dtx{n-1} + \dtx{n-2}}
         \bigs{
           \frac{\frac{y_{n+1} - y_n}{\dtn} - \frac{y_n - y_{n-1}}{\dtx{n-1}}}
                {\dtn +\dtx{n-1}}
           -
           \frac{\frac{y_{n} - y_{n-1}}{\dtn} - \frac{y_{n-1} - y_{n-2}}{\dtx{n-1}}}
                {\dtx{n-1} +\dtx{n-2}}
         }.
  \end{aligned}
\end{equation}
Doing this by hand is complex and error prone so a script was written to carry out algebraic rearrangement into coefficient form using the \texttt{python} programming language with the \texttt{sympy} library\cite{sympy}.
The output is \texttt{c++} source code for the coefficient vector (to avoid translation errors).
The resulting method (machine translated from the source code\cite[src/generic/explicit_timesteppers.cc:EBDF3::set_weights]{oomph-lib-source}) is
\begin{equation}
  \label{eqn:ebdf-coeffs}
  \begin{aligned}
    a y_{n+1} &= b f(t_n, y_n) + c_n y_n + c_{n-1} y_{n-1} + c_{n-2} y_{n-2}, \\
    a &= \dtx{n-1}^4 \dtx{n-2}  + \dtx{n-1}^3 \dtx{n-2}^2  
    + \dtx{n-1}^2 \dtx{n-2}^3 , \\
    b &= -(-\dtn^3 \dtx{n-1}^2 \dtx{n-2}  - \dtn^3 \dtx{n-1} \dtx{n-2}^2  
    - \dtn^2 \dtx{n-1}^3 \dtx{n-2}  
    - \dtn^2 \dtx{n-1}^2 \dtx{n-2}^2  \\
     & \quad - \dtn^2 \dtx{n-1} \dtx{n-2}^3  - \dtn \dtx{n-1}^4 \dtx{n-2}  
     - 2\dtn \dtx{n-1}^3 \dtx{n-2}^2  
     - \dtn \dtx{n-1}^2 \dtx{n-2}^3), \\ 
    c_n &= -(\dtn^3 \dtx{n-1} \dtx{n-2}  + \dtn^3 \dtx{n-2}^2  
    + \dtn^2 \dtx{n-1}^2 \dtx{n-2}  
    + \dtn^2 \dtx{n-1} \dtx{n-2}^2  + \dtn^2 \dtx{n-2}^3 \\
    &\quad - \dtx{n-1}^4 \dtx{n-2}  - \dtx{n-1}^3 \dtx{n-2}^2  
    - \dtx{n-1}^2 \dtx{n-2}^3 ), \\
    c_{n-1} & = -(-\dtn^3 \dtx{n-1}^2  - \dtn^3 \dtx{n-1} \dtx{n-2}  
    - \dtn^3 \dtx{n-2}^2  - \dtn^2 \dtx{n-1}^3  
    - \dtn^2 \dtx{n-1}^2 \dtx{n-2} \\
    &\quad - \dtn^2 \dtx{n-1} \dtx{n-2}^2  
    - \dtn^2 \dtx{n-2}^3 ), \\
    c_{n-2} &= -(\dtn^3 \dtx{n-1}^2  + \dtn^2 \dtx{n-1}^3 ).
  \end{aligned}
\end{equation}

??ds wrong, doesn't match up! Make the script emit latex?
Include code generation script in oomph-lib?

For constant time steps the result is much simpler:
\begin{equation}
  \begin{aligned}
    a &= 1/3, \\
    b &= \dtn,\\ 
    c_n &= -1/2,\\
    c_{n-1} & = 1,\\
    c_{n-2} &= -1/6.
  \end{aligned}
\end{equation} 
This matches with the result in \cite[pg. 364]{HairerNorsettWanner}.

It should be noted that as a time integrator the eBDF3 scheme is completely unstable.
However the way it is used here means that this does not matter--only single steps of eBDF3 are ever taken.
Subsequent steps are not based on previous eBDF3 outputs but instead on the outputs of the (very stable) IMR.

It should also be noted that the calculation of the coefficients in equation~\eqref{eqn:ebdf-coeffs} would be quite expensive ($\sim \order{100}$ operations assuming no compiler optimisation of the multiplications) if the number of y-values was small.
However, when used in finite-element or finite-difference calculations, the number of values is typically well into the tens of thousands.
So the cost of equation~\eqref{eqn:ebdf-coeffs} is trivial by comparision with, for example, the additional calculation of $f(t_n, \yv_n)$.


\subsubsection{Implicit BDF derivations}
??ds don't really need this but helpful for comparisions, appendix?

Equating the derivative at time $t_{n+1}$ results in
\begin{equation}
  \begin{aligned}
    f(t_{n+1}, y_{n+1}) &= \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \sum_{l=0}^{j-1} \prod_{i=0, i \neq l}^{j-1} (t_{n+1} - t_{n+1-i}).
  \end{aligned}
\end{equation} 
This can be greatly simplified by noticing that the product $ \prod_{i=0, i \neq l}^{j-1} (t_{n+1} - t_{n+1-i})$ is zero whenever it contains $i=0$, \ie all terms in the sum with $l \neq 0$.
This gives
\begin{equation}
  \begin{aligned}
    f(t_{n+1}, y_{n+1}) &= \sum_{j=1}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=0, i \neq 0}^{j-1} (t_{n+1} - t_{n+1-i}), \\
    &=  y[t_{n+1}, t_n] + \sum_{j=2}^k y[t_{n+1}, \ldots, t_{n+1-j}] \prod_{i=1}^{j-1} (t_{n+1} - t_{n+1-i}). \\
  \end{aligned}
\end{equation}

For $k=1$ we get
\begin{equation}
  \begin{aligned}
    f(t_{n+1}, y_{n+1}) &= \frac{y_{n+1} - y_n}{\dtn}, \\
    y_{n+1} &= y_n + \dtn f(t_{n+1}, y_{n+1}),
  \end{aligned}
\end{equation}
which is backward Euler as expected.

For $k=2$
\begin{equation}
  \label{eqn:bdf2-derivation}
  \begin{aligned}
    f(t_{n+1}, y_{n+1}) &= \frac{y_{n+1} - y_n}{\dtn} +  y[t_{n+1}, t_{n}, t_{n-1}](t_{n+1} - t_n), \\
    &= \frac{y_{n+1} - y_n}{\dtn} +  \frac{\dtn}{\dtx{n+1} + \dtn} \bigb{\frac{y_{n+1} - y_n}{\dtn} - \frac{y_n - y_{n-1}}{\dtx{n-1}}}. \\
  \end{aligned}
\end{equation}
After some algebra this is
\begin{equation}
  \label{eqn:bdf2-derivation-rearranged}
  \begin{aligned}
    \frac{y_{n+1} - y_n}{\dtn} = \frac{\dtn}{2\dtn + \dtx{n-1}} \frac{y_n - y_{n-1}}{\dtx{n-1}} 
    + \frac{\dtn + \dtx{n-1}}{2\dtn + \dtx{n-1}} f(t_{n+1}, y_{n+1}).
  \end{aligned}
\end{equation}
as expected for BDF2 \cite[pg. 715]{GreshoSani}.


\subsection{Computing the size of the next step}

We use a standard method for computing the next step size from the local truncation error and a target local truncation error \toltt:\cite[pg.268]{Gresho-Sani}
\begin{equation}
\dtx{n+1} = \dtn \left( \frac{\toltt}{\lte}  \right) ^{\frac{1}{\texttt{order}+1}}.
\end{equation}
For implicit midpoint rule this is
\begin{equation}
  \dtx{n+1} = \dtn \left( \frac{\toltt}{\lte^\IMP}  \right) ^{\frac{1}{3}}.
\end{equation}

If the ratio of step size to desired step size is too low, \ie if the step just taken was much too large to attain the desired accuracy then we reduce the step size by a factor of 2 and repeat the step.

Actually we (should) include a slight safety factor to reduce the number of step rejections:

??ds but actually oomph-lib doesn't...


Additionally we impose a limitation on the maximum step size scaling to help improve stability ??ds does this actually do anything?


??ds this is still flakey, test things? Use reltol? Ask Matthias?

A final modification is to include limitations on the step size that prevent issues with numerical accuracy.
We assume that the equations being modelled are non-dimensionalised so that the initial Newton residual (see Section~\ref{sec:newt-raph}) is $\sim \order{\dtn}$ (The initial guess is the value at the previous step, the change in the solution value in one step should be of size somewhere around $\dtn$).
In this case the minimum time step we can take is approximately the Newton tolerance (otherwise the Newton method will converge ``early'' because the residual is small to begin with).
Similarly the maximum time step we can take is $\sim \order{\frac{\texttt{newtontol}}{10^{-16}} = 10^{8}}$ before the Newton method will have difficulty converging due to numerical errors.


Note that some alternative strategies try to minimise the number of modifications to the time step size\cite[chap. 6]{Iserles2009} \cite[Sec. 2.1]{cvode-manual}.
The motivation for this is the use of a modified Newton-Raphson method where the Jacobian is not recalculated at each Newton step.
In the case of ODE solvers this is extremely beneficial because the Jacobian is usually dense and direct solvers are used to solve it--by keeping the Jacobian constant the number of solves and Jacobian assemblys are greatly reduced.
The downside is a reduction in the convergence speed of the Newton method.
Keeping the step size constant for as long as possible is required here because modifications to the time step cause modifications to the Jacobian.
However for PDE solvers, and in particular for iterative PDE solvers there is little-to-no benefit in using this technique (the reduction in convergence speed is worse than the gain of not assembling/solving the sparse Jacobian\cite[pg. 128]{Iserles2009}.


\subsection{Implementation}

To increase relevance to realistic models including spatial discretisation we use a Newton method to solve \emph{all} equations (even linear ones).

Implemented in python using scipy, this allowed for rapid prototyping of new schemes. However little attention has been paid to optimisation, so wall-clock timing results would not be relevant (and so are not presented).

All code used has been open sourced and is available on github.\cite{simple-ode-github}


??ds update
Unless otherwise specified all examples in this section use a Newton tolerance of $1\E{-10}$ and an adaptive integrator tolerance of $\toltt = 1\E{-4}$.


\subsection{Test functions}

To demonstrate the effectiveness of the adaptation scheme we use a simple ODE with known exact solution
\begin{align}
  f(t,y) &= - \beta e^{-\beta t} \sin(\omega t) + \omega e^{-\beta t} \cos(\omega t) \\
  y(0) &= 0
\end{align}
which gives the exact solution
\begin{equation}
  \label{eq:59}
  y(t) = e^{-\beta t} \sin(\omega t).
\end{equation}
The numerical experiments below all use $\omega = 2 \pi$ and $\beta = 0.3$.
The exact solution for this case is shown in Figure~\ref{fig:mp-ode-exact}.
Plots of the solutions given by the various numerical calculations below are all visually indistinguishable from Figure~\ref{fig:mp-ode-exact} (without zooming in) at the tolerances used, so they are not shown.


\begin{figure}[ht!]
  \centering
  \includegraphics{images/osc_damped_ode_exact}
  \caption{A plot of equation~\eqref{eq:59}, the exact solution of our example ODE.}
  \label{fig:mp-ode-exact}
\end{figure}

\subsection{Results}

\begin{figure}[ht!]
  \centering
  \includegraphics{images/placeholder}
  \caption{Adaptive IMR vs adaptive BDF2 for a simple ODE.}
  \label{fig:mp-vs-bdf2}
\end{figure}

Figure~\ref{fig:mp-vs-bdf2} shows the performance of the adaptive midpoint scheme derived above compared to a standard adaptive BDF2 scheme.\cite[pg. 715]{Gresho-Sani}
The step size selection of the adaptive midpoint scheme closely follows that of the BDF2 scheme.


??ds discuss more about third derivative!
The pattern of step sizes is as expected.
There is a general trend of increasing step size as the solution is gradually damped out.
Additionally there is an oscillation which peaks at integer and half-integer times, this corresponds to the peaks in $\cos(\omega t)$ and $\sin(\omega t)$.
This is relevant because the truncation error is proportional to $y'''(t)$ which consists of terms in these two functions.

Also of note is that the accuracy of IMR is much higher than that of BDF2 for the same step size in this example.
??ds why? (Jacobian is zero)


Figure~\ref{fig:mp-tols} compares mean error norms and step sizes of the adaptive midpoint scheme (with two interpolation points) for varying tolerances.
The IMR can be seen to have quadratic convergence by comparing with the $y=x^2$ line shown.
Additionally it can be seen that decreasing the tolerance smoothly decreases the mean error (by smoothly decreasing the mean step size).

\begin{figure}[ht!]
  \centering
  \includegraphics{images/placeholder}
  \caption{Adaptive midpoint mean dt vs mean error for varying tolerance. The line is $y = x^2$.}
  \label{fig:mp-tols}
\end{figure}


\section{Additional Notes}

\subsection{Application to Implicit ODEs}
\label{sec:extens-impl-odes}

We sometimes wish to solve a system of equations where $\yv'(t)$ only given implicitly\footnote{This use of ``implicit'' is unrelated to the notion of implicitness in the time integration scheme.} (for example the Gilbert form of the Landau-Lifshitz-Gilbert equation), in this case equation~\eqref{eq:43} becomes
\begin{equation}
  \fv(t, \yv(t), \yv'(t)) = 0.
\end{equation}

We note that equation~\eqref{eq:basic-midpoint} can also be written in the from
\begin{equation}
  \yv'(\thf) = \frac{\yv_{n+1} - \yv_n}{\dtn} =  \fv(\thf, \frac{\yv_n + \yv_{n+1}}{2}).
\end{equation}
So the obvious equivalent for IMR is
\begin{equation}
  \fv(\thf, \frac{\yv_{n+1} + \yv_n}{2}, \frac{\yv_{n+1} - \yv_n}{\dtn}) = 0.
\end{equation}

However the adaptive scheme requires an additional function evaluation.
For implicitly defined functions this is expensive, so this adaptive method is not efficient for equations which can only be defined in such a way.


%% This doesn't seem to actually show up ever in practice, maybe only in the infinitely stiff limit
% \subsection{Order reduction}

% It is known that implicit Runge-Kutta methods (such as the implicit midpoint rule) are susceptible to reduced accuracy when used to solve certain types of stiff problem.
% This is represented in the second term of the full LTE expression~\eqref{eq:trunc-final}.
% If $\dfdyhf \cdot \yvhf['']$ is large then the error could be much larger than expected.
% This effect will be automatically detected and adjusted for by the error estimator. ??ds experimental results?

% However in practice this seems to only occur in rare cases...
% LLG doesn't look like it will suffer from it.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "./main"
%%% End:
