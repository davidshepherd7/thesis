\chapter{Numerical Methods for Dynamic Micromagnetic Modelling}
\chaptermark{General numerical methods}
\label{sec:numer-meth-micr}

\section{Introduction}

In \thisref{sec:numer-meth-micr} we give an overview of numerical methods that have previously been used in dynamic micromagnetic calculations.
These calculations involve solving some form of the LLG equation \cref{eq:LLG} with effective fields determined by energy derivatives as discussed in \cref{sec:cont-micromag}.
These systems of partial differential equations (PDEs) can only be solved analytically in a few simple cases \cite{Aharoni1996}, so numerical solution methods are almost always necessary.


\subsection{Components of a numerical method for solving PDEs}

Many numerical methods for solving non-linear PDEs, such as the LLG \cref{eq:LLG}, can be thought of as a combination of various component methods, each of which handles a different part of the conversion from a continuous PDE into an algorithm which can be performed by a computer\footnote{Technically proofs of convergence etc. should be carried out for every different combination of component methods \cite[382]{Iserles2009}; in practice surprises seem to be rare, at least within micromagnetics.}.

The essential components of a numerical method for solving time-dependent PDE problems are:
\begin{itemize}
\item Spatial discretisation: convert spatial derivatives into algebraic relationships, typically between discrete points in space, \eg finite elements, finite differences, macrospin models.
\item Time discretisation (a.k.a. time integration): convert time derivatives into algebraic relationships, again typically between points in time, \eg Runge-Kutta methods, backwards difference formulae.
\end{itemize}
For some choices of time and space discretisations additional component methods are needed:
\begin{itemize}
\item Linearisation procedure: Solve a system of non-linear algebraic equations, often by an iterative procedure which requires the solution of a sequence of systems of linear algebraic equations, \eg Newton-Raphson method, Picard/fixed-point iteration.
\item Linear solver: solve a system of linear algebraic equations, \eg LU decomposition, Krylov solvers, multigrid methods.
\end{itemize}

Additionally in micromagnetic modelling the calculation of the magnetostatic field is required.
If the integral form of the magnetostatic field, given in \cref{eq:Hmsint}, is used then an additional integral evaluation method is needed to handle it.
Alternatively, if the equivalent potential formulation, given in \cref{eq:Hms,eq:cont-phi-bound}, is used then the magnetostatic field calculation simply requires solving additional PDEs, although there are some issues with the application of the boundary condition at infinity which are discussed in \cref{sec:magstat-field-calc-pote}.


\subsection{Desirable properties of a numerical method}
\label{sec:desir-prop-numer}

Ideally all numerical methods would run quickly on cheap hardware and with sufficient accuracy.
Unfortunately this is not the case in general and we are forced to make trade-offs.
In \thisref{sec:desir-prop-numer} we roughly describe some \emph{ideal} properties of a numerical method (although obtaining all of these properties in a single method is almost certainly impossible).

The accuracy of a simulation is generally proportional to the ``fineness''  of the discretisations used (\ie the number of points used in the approximation), but increasing the fineness also increases the computation time.
However, careful choice of algorithms can improve the accuracy at no additional cost or improve the speed with no loss of accuracy.
The following properties of methods are important in this respect:
\begin{itemize}
\item The discretisation fineness in time should be dictated only by what is needed to accurately resolve the physics and not by numerical properties of the discretisation.
  In particular improved spatial fineness should not require improved time fineness.

\item The optimal fineness in both time and space should be determined automatically, and should be allowed to vary as needed.
  This allows us to get the best ``value'' for our computational effort when increasing the fineness of the discretisation.

\item The computation time should increase as little as possible as fineness in space or time increases.
  The minimum increase is linear scaling ($\order{N}$, where $N$ is the number of discrete points involved) because we always have to do \emph{something} with each value.
  Attaining this property for spatial discretisation usually impacts the choice of linear solver and/or magnetostatic calculation: many methods do not scale this well.

\item The method should be able to handle non-trivial geometries without extortionate computational cost.
\end{itemize}

Additionally we would like our algorithms to be robust, that is they should reliably be able to give a correct answer no matter what parameters are used (up to any tolerances specified).
In other words there should be few edge cases or ``interesting'' surprises when applying the method to various physical problems.
Some important properties which improve robustness are:
\begin{itemize}
\item The method should obey any qualitative physical properties of the continuous equation.

\item Mathematical evidence that all parts of the method will ``work'' on any well defined problem (\eg proofs of convergence, etc.).

\item The numerical method should automatically detect difficulties and apply the required effort to obtain the correct answer.
\end{itemize}


In addition to these properties there are a few practical considerations.
The method should be reasonably simple: people need to understand a method before they can implement it, and even users need a surface level understanding.
Finally the method should be as general as possible to reduce implementation effort and to simplify testing (by allowing a greater selection of example problems).


\section{Spatial Discretisation}
\label{sec:spat-discr}

\subsection{Macrospin models}
\label{sec:sd-macrospins}

In a granular magnetic material (a material consisting of magnetic grains separated by a non-magnetic material) the simplest way to handle spatial variation in the problem is to assume that within each grain the exchange effective field is so strong that the magnetisation is constant, \ie that each grain behaves as a single \emph{macrospin}.
We assign a single value of $\mv$ to each macrospin and proceed to calculate energy, effective field and/or magnetisation of each macrospin as required.
One caveat is that the effects of the magnetostatic field of a grain on the grain itself is not automatically accounted for since there is no modelling of intra-grain effects.
Hence it must be calculated and included separately to the magnetostatic interactions between grains.
When included in this way the magnetostatic self field is often called the \emph{shape anisotropy} since it is dependent on the shape of the grain and acts in a similar way to the magnetocrystalline anisotropy.

The obvious downside of a macrospin approach is that it only applies to fairly specific geometries, although the case of a granular media has been of much interest for magnetic data storage.
Additionally, if there are nonuniformities in magnetisation within the regions where it has been assumed constant the model may be inaccurate.

However it is often simpler to construct a macrospin model than to use the more general methods described in \cref{sec:sd-finite-diff-meth,sec:sd-finite-elem-meth}.
Also the assumption that each grain has uniform magnetisation can reduce the number of calculations needed.

Closely related to macrospin models are atomistic models \cite{Evans2014}.
In these models the assumption of a continuous magnetisation $\mv(\xv, t)$ is dropped in favour of an assumption that each atom is the location of a single macrospin.
As such they allow the modelling of some systems that are inaccessible using micromagnetics, such as anti-ferromagnetic materials.
However, the required spatial resolution for an atomistic model comes with an increase of a few orders of magnitude in the computational cost.
Additionally such methods are only accurate for materials where the magnetic moment is localised to an atom, which is not always the case.


\subsection{Finite Difference Methods}
\label{sec:sd-finite-diff-meth}

A widely used method of spatial discretisation is the finite difference method: a single magnetisation vector is assigned to each point on (or ``cell'' in) a square/cubic grid which covers the entire domain.
Spatial derivatives in the PDE are then approximated by using truncated Taylor series expansions.

The finite difference method works well for very simple geometries when the grid can be lined up with all geometric features.
For example when we are interested in how the magnetisation evolves over time in a non-granular cuboid-shaped thin film of magnetic material a finite difference method will be sufficient (\eg in the \mumag standard problems \cite{mumag-website}).

However when the geometry involves curves, diagonals, hexagonal grains, bit patterned media or any other more complex geometric system alternative methods may be better suited.


\subsection{Finite Element Methods}
\label{sec:sd-finite-elem-meth}

A more complex method of spatial discretisation is the finite element method \cite{HowardElmanDavidSilvester2006}.
Here the magnetic body is divided up into a finite number of non-overlapping \emph{elements}, which can vary in both shape and size.
Within each of these elements the spatial variation of the magnetisation is approximated by a polynomial function, typically a linear polynomial.
Spatial derivatives are easily calculated by differentiating these polynomial functions, although higher derivatives can require some tricks.
In contrast to macrospin and finite difference methods the actual equations solved by the finite element method are a \emph{weak} form of the standard, or \emph{strong}, form of the PDE.
More details of the weak form and the finite element method are given in \cref{sec:intr-finite-ele-diff}.

The main advantage of the finite element method is that it can cheaply and accurately (compared to the finite difference method) approximate non-trivial geometrical features by an using an appropriate mesh of elements.
An additional advantage is that the size of elements, and thus the accuracy of the approximation, can be varied arbitrarily as needed to give better accuracy in more complex or important regions.
The choice of element size can be done automatically using \emph{adaptive mesh refinement}: after each calculation an error estimate is calculated.
If the error is determined to be too high in a region the mesh is refined near that region and the calculation is repeated (and similarly if the error is very in a region low the mesh can be unrefined).
Hence, given the desired error and a method to estimate the error, a mesh giving an efficient and accurate approximation can be automatically generated \cite{Schrefl1999}.\footnote{Adaptive mesh refinement can also be done in finite difference methods, but as it results in a non-uniform mesh this removes their main advantage of simplicity and uniformity.}

The major downside of finite element models is that the underlying mathematics is more complex than that required for other methods.
Also the set up time and memory usage can be greater because of the additional ``bookkeeping'' required to keep track of the more complex meshes.


\section{Magnetostatic Field Calculations}
\label{sec:magn-field-calc}

Methods for the calculation of magnetostatic fields belong to two categories: integral and potential methods.
Integral methods are based on efficiently approximating the very large number of integrals required by \cref{eq:Hmsint}.
Potential methods are based on the solution of the PDE \cref{eq:Hms} with standard spatial discretisation methods combined with some special treatment is required for the boundary condition at infinity.

In \thisref{sec:magn-field-calc} we use the term ``discrete magnetisations'' to mean the macrospins/cells/nodes as appropriate for the spatial discretisation.
We use $N$ to denote the number of discrete magnetisations.

% More recently models have been developed which extend the magnetic field calculations to include all of Maxwells equations (the electric field and its effects on the magnetic field as well).
% Such models greatly increase the number of degrees of freedom in the problem: 3 magnetisation components, 3 magnetic field components and 3 electrical field components are needed, whereas for magnetostatic calculations only the magnetisation and possibly up to 2 potentials are needed.
% Since, for not seem to have a substantial effect on the results for many problems of interest \cite{} we will not consider these full Maxwell models further.

\subsection{Integral Methods}
\label{sec:magstat-field-calc-inte}

% \begin{figure}
%   \centering
%   \begin{tikzpicture}[level 1/.style={sibling distance=5.4cm},
%     level 2/.style={sibling distance=3.6cm}]

%     \node[block] {\textbf{Magnetostatic Calculations}}
%     child {node[block,text width=6cm] {Scalar Potential Formulation (with some spatial discretisation)}
%       child{node[block,text width=4cm,xshift=-1cm] {Asymptotic Boundary Conditions}}
%       child{node[block,text width=4.3cm] {Hybrid Finite/Boundary Element Method}}
%     }
%     child {node[block,yshift=-2.7cm] {Integral Formulation}
%       child{node[block,text width=3.2cm] {Full Calculation}}
%       child{node[block,text width=3.2cm] {Fast Fourier Transform}}
%       child{node[block,text width=3.2cm] {Fast\\ Multipole\\ Method}}
%     };
%   \end{tikzpicture}
%   \caption{Methods of magnetostatic field calculation that have been used in micromagnetic models.}
%   \label{fig:types-mag-stat}
% \end{figure}

Integral methods are based on the expression
\begin{equation}
  \Hms(\xv) = \frac{1}{4 \pi} \bigs{
    - \int_\magd \nabla' \cdot \Mv(\xv') \frac{(\xv - \xv')}{\abs{\xv -\xv'}^3} \d^3 \xv'
    + \int_\boundd \Mv(\xv') \cdot \nv(\xv') \frac{(\xv - \xv')}{\abs{\xv - \xv'}^3} \d^2 \xv' }.
  \label{eq:Hmsint2}
\end{equation}

A naive way to evaluate the integrals in \cref{eq:Hmsint2} would be to calculate the field at each node due to each other node.
So for each node a sum over all other nodes must be calculated.
Hence this results in a computation time that scales as $\order{N^2}$, which is usually unacceptably slow for reasonably large $N$.
Instead, more advanced techniques are used which treat the large number of integral evaluations in a more efficient way.


\subsubsection{Fast Fourier transform methods}

The fast Fourier transform method (FFT) is a simple and efficient method for calculating the magnetostatic field when the discrete magnetisations are positioned on a regular lattice.

% ??ds more detail "convolution" -- Andrew
The calculation of $\Hms$ in \cref{eq:Hmsint} can be thought of as applying a convolution to the list of discretised $\mv$ values.
The matrix corresponding to this convolution is only dependent on the geometry, hence it can be precomputed and its Fourier transform precalculated.
Then all that is needed to calculate the magnetostatic field is to apply a Fourier transform to $\Mv$, compute the convolution and transform the result back into the time domain by applying the inverse Fourier transform.
Because of the regularity, applying the convolution in the frequency domain is very fast and hence the complexity of the calculation is limited by the complexity of a fast Fourier transform.
This results in an overall complexity of $\order{N \log(N)}$ \cite{Jones1997}.

The downside of this method is that points to be calculated must be on a regular lattice, similar to the finite difference method.
Hence, it is most useful in combination with models using a finite difference spatial discretisation.

Alternatively the FFT may be used as part of a method applicable to less regular meshes.
In such ``non-uniform FFT'' methods distant discrete magnetisations are approximated by magnetisations on a regular lattice so that an FFT method can be applied while the effects of nearby magnetisations are evaluated directly \cite{Jones1997}.


\subsubsection{Fast multipole method}
\label{sec:fast-mult-meth}

An alternative method of calculation of the magnetostatic field is the fast multipole method (FMM) \cite{Beatson1997}.
The fast multipole method takes advantage of the fact that distant magnetic charge has a much smaller contribution to the total field at a point than a nearby magnetic charge due to the $\frac{1}{(\xv - \xv')^2}$ scaling in \cref{eq:Hmsint2}.
Hence much less accuracy is needed in the calculation of these distant contributions in order to obtain the same overall accuracy.

For the field calculation at a specific point the full calculation is only performed for nearby discrete magnetisations.
Groups of more distant magnetisations are approximated (lumped) as a single multipole placed at the centre of the group.
The trick for quickly calculating fields at a large number of points is to pre-calculate multipole approximations for a range of accuracies over all space.
Then the calculation of a field at a single point only requires the full calculation of effects from a few nearby points and from the appropriate multipoles.

The main advantage of this method over the fast Fourier transform is that it allows for arbitrary geometries.


\subsection{Scalar potential methods}
\label{sec:magstat-field-calc-pote}

Potential methods are based on the formulation
\begin{equation}
  \begin{aligned}
    \label{eq:Hms2}
    \hms &= - \grad \phim, \\
    \lap \phim &= \div \mv,
  \end{aligned}
\end{equation}
with the boundary conditions
\begin{equation}
  \begin{aligned}
    \label{eq:cont-phi-bound2}
    \phim^\inte - \phim^\exte &= 0 \quad \xv \in \boundd, \\
    \pd{\phim^\inte}{\nv} - \pd{\phim^\exte}{\nv} &= \mv \cdot \nv \quad \xv \in \boundd, \\
  \end{aligned}
\end{equation}
and
\begin{equation}
  \begin{aligned}
    \label{eq:phi-inf-bound}
    \phim \rightarrow 0 \text{ as } &\abs{\xv} \rightarrow \infty,
  \end{aligned}
\end{equation}
see \cref{sec:magnetostatic-field} for details.

Inside the domain $\phim$ can be solved for simply by applying finite element or finite difference discretisation methods.
However \cref{eq:phi-inf-bound}, the boundary condition on $\phim$ at infinity, is problematic.
We obviously can not apply this condition directly using standard methods, since that would involve either an infinite number of elements or infinitely large elements (actually these are  possible but impose heavy limitations on the allowed mesh geometries see \eg \cite{Alouges2001} and \cite{Fidler2000} respectively).
Hence more advanced techniques must be used, such techniques are the subject of the rest of \thisref{sec:numer-meth-micr}.

The system of equations for the internal field calculations results in a well known linear algebra problem which can be solved in $\order{N}$ time by well studied linear solvers (more details are given in \cref{sec:solution-lin-sys,sec:solution-strategies}).
However applying or calculating boundary conditions can require additional processing time.



\subsubsection{Asymptotic boundary conditions}
\label{sec:asymptot-bcs}

One way to avoid an infinite domain is to truncate the external region at some finite distance from the magnetic domain.
A similar but more sophisticated and accurate approach is to use asymptotic boundary conditions \cite{Yang1997}.
The idea here is to use a truncated external region to calculate the boundary conditions on the magnetic domain that correspond to \cref{eq:phi-inf-bound} being applied at infinity.
Additionally the fact that any solution to the Poisson equation~\cref{eq:Hms2} can be represented as an infinite series of harmonic functions is used to improve the accuracy.

Unfortunately the accuracy of this approach is still quite low, even for large truncation distances \cite{Bottauscio2008}.


\subsubsection{The hybrid finite element method/boundary element method}
\label{sec:bound-elem-meth}

The idea of the hybrid finite element method/boundary element method (FEM/BEM) is to replace the external domain by a dipole layer placed on the surface of the magnetic domain which gives the correct boundary condition at infinity \cite{Fredkin1990}.
This removes the need to truncate or discretise the infinite external domain.
The name comes from the close relationship between the use of a dipole layer and the boundary element method, and the fact that the standard finite element method is used to calculate the required potentials in the bulk.
The full details of the method applied to magnetostatic calculations is discussed in \cref{sec:hybr-finit-elem}.

A comparison by Bottauscio \cite{Bottauscio2008} found that using the FEM/BEM method was more accurate than applying asymptotic boundary conditions (ABCs) for a calculation of the time evolution of the magnetisation of a sphere with zero exchange coupling.
Even with a truncation distance of four times the size of the magnetic sphere (\ie the radius of the computational domain was $4$ times larger then that of the sphere) the accuracy when using asymptotic boundary conditions was worse\footnote{After one precession cycle the relative error in $M_x$ obtained using ABC method was around 1, compared to around $0.1$ for FEM/BEM.} and did not improve between truncation distances of three and four times the magnetic sphere radius.
Even when exchange coupling was added (giving an easier test) the ABC method was worse\footnote{A relative error of around $0.2$.} than the FEM/BEM method.

The main downside of the FEM/BEM method is that it involves dense matrix of size $N_b \times N_b$ where where $N_b$ is the number of boundary nodes (see \cref{sec:hybr-finit-elem} for details).
This matrix must be stored in memory\footnote{In theory individual regions of the matrix could be recomputed on the fly when needed but the computational cost would be inordinately high.}, and multiplied by a vector for the calculation of the boundary conditions.
The speed of calculation of the boundary values in the method is limited by the dense matrix multiplication which, the cost of which is $\order{N_b^2}$.
Similarly, the memory usage is $\order{N_b^2}$, which can limit the size of problems.
The use of hierarchical matrix techniques can reduce both the speed and the memory usage to $\order{N_b \log(N_b)}$ \cite{Knittel2009}.

In 3D structures which are roughly spherical $N_b = \order{N^{2/3}}$ and so the use of a hierarchical matrix gives optimal computation speed\footnote{With hierarchical matrix techniques the speed is $\order{N^{2/3}\log(N^{2/3})}$ but $\log(x) \ll x^{1/2}$ for large $x$, hence $\order{N^{2/3}\log(N^{2/3})} \ll \order{N}$, \ie optimal complexity.} but for extremely flat structures such as thin films the number of boundary nodes can be as large as $N_b = N$.
Hence the speed of the FEM/BEM method depends on the geometry.

Another downside of FEM/BEM is the increase in the complexity of the model: some components of the FEM/BEM method are fundamentally different to FEM.
As such substantial additional code and mathematical knowledge may be needed for its implementation when standard FEM libraries are used as a basis.


\section{Time Integration}
\label{sec:time-discretisation}

Time integration schemes are used to convert the time derivatives of a PDE into a form that can be handled computationally.
When discussing time integration schemes it is helpful to use a (vector) initial value ordinary differential equation (ODE) as an example:
\begin{equation}
  \begin{aligned}
  \frac{\textrm{d} \yv}{\textrm{d}t} &= \ffv{t, \yv}, \\
  \yv(0) &= \yv_0,
  \label{eq:45}
  \end{aligned}
\end{equation}
where $\fv(t,\yv)$ is a known function and $\yv_0$ is the known initial condition.
After the application of one of the spatial discretisations described above a time-dependent PDE is typically reduced to the form \cref{eq:45}; this is known as a \emph{semi-discretised} PDE.

The time integration methods discussed here all bear a strong similarity to the finite difference method discussed in \cref{sec:sd-finite-diff-meth}, except that the independent variable discretised is time instead of space.
This is appropriate since time is one-dimensional, hence no complex geometry is possible and so typically there is no need for the more advanced finite element method.

Some key attributes of a time integration scheme are \cite{Atkinson2009}:
\begin{itemize}
\item \textbf{Accuracy (order)} -- An estimate of how rapidly the approximate solution converges to the exact solution as the time step size is reduced.

\item \textbf{Stability} -- Roughly speaking a scheme is stable if the approximate solution does not ``blow up'' (\ie become catastrophically inaccurate, infinite, oscillate in non-physical ways, \ldots) for sufficiently large time steps, see \cref{sec:A-stability} for a more rigorous description.

\item \textbf{Conservation properties} -- Some differential equations have properties which should ideally be conserved in the discretised system.
For example the magnetisation length and energy properties of the Landau-Lifshitz-Gilbert equation discussed in \cref{sec:prop-cont-llg}.
Schemes which conserve such properties are often referred to as ``geometric'' integrators.

\item \textbf{Self-starting} -- A scheme is self starting if it only requires a single initial value.
This is desirable because methods of estimating additional initial values make the final scheme more complex and may introduce additional error.
\end{itemize}

These attributes are discussed more rigorously in the rest of \thisref{sec:time-discretisation}.

\subsection{Explicit and implicit time integration schemes}
\label{sec:explicit-vs-implicit-schemes}

Explicit time discretisation schemes calculate the value at the next time step in terms of the value(s) at the present and/or previous time steps.
The simplest such scheme is the \emph{(forward) Euler method}
\begin{equation}
  \label{eq:44}
  \yv_{n+1} = \yv_n + \dtn \fv(t_n, \yv_n),
\end{equation}
where $\dtn = t_{n+1} - t_n$ is the $n$-th time step size and $\yv_i$ is the approximation to $\yv(t_i)$.
Clearly, given $\fv(t,y)$ and an initial value for $\yv(t_0)$, we can calculate $\yv(t_n)$ for any $n$.

In contrast to explicit schemes, implicit schemes use the value at the next time step in its own calculation.
Hence at each step a (linear or non-linear) system of equations must be solved.
The simplest implicit scheme is the first order \emph{backwards difference formula} (BDF1, also known as backwards Euler)
\begin{equation}
  \label{eq:bdf1-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{t_{n+1}, \yv_{n+1}}.
\end{equation}

At first glance it seems that the additional solution of a system of equations required for one step of an implicit scheme mean would that explicit schemes are more efficient, and it is true that one step of an implicit method requires more computational effort than one step of an explicit method.
However all explicit time integration schemes suffer from issues of limited stability for some problems, and so are forced to use time step sizes much smaller than would be required for reasons of accuracy (see \cref{sec:A-stability} for details).
In these cases implicit schemes can be much more efficient; problems where this is the case are often called ``stiff'' problems.
The semi-discretisation of a PDE often results in such a stiff problem.
In \cref{cha:stiffn-llg-equat} we investigate stiffness in micromagnetics problems.


% Micromagnetics solvers for non-stiff systems commonly use the RK4 (fourth order Runge-Katta) method \cite{Suess2002}.
% Micromagnetics models commonly use BDF schemes of various order \cite{Suess2002} or the implicit midpoint rule \cite{DAquino2005} for the modelling of stiff systems.


\subsection{Some implicit time integration schemes}
\label{sec:some-implicit-time-integrators}

For the remainder of this thesis we will focus on three time integration schemes with good stability and accuracy properties that are widely employed in practice.
All three are implicit schemes for reasons of stability, as mentioned in \cref{sec:explicit-vs-implicit-schemes}.

\emph{Trapezoid rule} (TR) is the average of the forward and backward Euler methods from \cref{sec:explicit-vs-implicit-schemes} and is defined as \cite[260]{GreshoSani}:
\begin{equation}
  \yv_{n+1} = y_n + \dtn (\ffv{t_{n+1}, \yv_{n+1}} + \ffv{t_n, \yv_n})/2.
  \label{eq:tr-definition}
\end{equation}

The \emph{second order backwards difference formula} (BDF2) is \cite[715]{GreshoSani}:
\begin{equation}
  \frac{\yv_{n+1} - \yv_n}{\dtn} = \frac{\dtn}{2\dtn + \dtx{n-1}} \frac{\yv_n - \yv_{n-1}}{\dtx{n-1}}
  + \frac{\dtn + \dtx{n-1}}{2\dtn + \dtx{n-1}} \fv(t_{n+1}, \yv_{n+1}).
\end{equation}

The method that this thesis focuses on most of all (for reasons that will become clear through the remainder of \thisref{sec:time-discretisation}) is the \emph{implicit midpoint rule} (IMR).
It is given by \cite[263]{GreshoSani}:
\begin{equation}
  \label{eq:imr-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}}.
\end{equation}
Note that for cases where $\fv$ is linear in both $t$ and $\yv$, IMR is equivalent to TR, and in general the properties of the two methods are similar.

IMR can be easily implemented in terms of BDF1 by the following simple algorithm: take a step of BDF1 (the simplest possible implicit method) using $\dtn^\bdfo = \dtn^\imr/2$ to find $\yv_\midp$ at $t_\midp$.
Then calculate $\yv_{n+1}$ using a rearrangement of $\yv_\midp = \frac{\yv_n + \yv_{n+1}}{2}$ \cite{Malidi2005}:
\begin{equation}
    \yv_{n+1} = 2\yv_\midp - \yv_n,
\end{equation}
and set $t_{n+1} = t_\midp + \dtn^\imr/2$.

TR and IMR are both self starting while BDF2 requires an additional start-up value.
This can be generated, for example, by taking a single step of IMR or TR before beginning the use of BDF2.


\subsection{Local truncation error and order}
\label{sec:deriv-local-trunc}

The \emph{local truncation error} (LTE) of a time integration scheme is the error due to a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$, $\yv_{n-1} = \yv(t_{n-1})$, etc. into the approximation for the next time-step, then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$, \ie
\begin{equation}
  \label{eq:60}
  \lte = \yv(t_{n+1}) - \hat{\yv}_{n+1},
\end{equation}
where $\hat{\yv}_{n+1}$ is the approximation to $\yv$ at time $t_{n+1}$ given when the exact solution is used for all history values (\ie using $\yv_{n} = \yv(t_n)$, $\yv_{n-1} = \yv(t_{n-1})$, $\ldots$).
For example the local truncation error of IMR is
\begin{equation}
  \lte^\imr =  \yv(t_{n+1}) - \yv(t_n) - \dtn \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}^\imr}{2}}.
  \label{eq:trunc-start}
\end{equation}
If the local truncation error of a time integration scheme is such that
\begin{equation}
  \lte \leq c \dtn^{p+1}
\end{equation}
then we say that the scheme is of order $p$.

It is important to distinguish the local truncation error from the \emph{global error}
\begin{equation}
  \label{eq:global-temporal-error}
    E_n = \yv(t_{n+1}) - \yv_{n+1}.
\end{equation}
The difference is that the global error includes any error accumulated over previous time steps (\ie the exact values $\yv(t_n)$ in \cref{eq:60} are replaced by their approximations).
Note that because of this accumulation the global error will typically be larger than the LTE.
The global error of IMR expressed in the same form as \cref{eq:trunc-start} is
\begin{equation}
  E_n^\imr =  \yv(t_{n+1}) - \yv_n^\imr - \dtn \ffv{ \thf, \frac{\yv_n^\imr + \yv_{n+1}^\imr}{2} }.
\end{equation}

The TR and BDF2 methods are both second order.
Their local truncation errors are \cite[261]{GreshoSani}
\begin{equation}
  \label{eq:tr-lte}
  \lte^\tr = -\frac{\dtn^3 \yv_n'''}{12}
  + \order{\dtn^4},
\end{equation}
and \cite[715]{GreshoSani}
\begin{equation}
  \label{eq:bdf2-lte}
  \lte^\bdf = -\frac{(\dtn + \dtx{n-1})^2}{\dtn(2\dtn + \dtx{n-1})}
  \frac{\dtn^3 \yv_n'''}{6}
  + \order{\dtn^4}.
\end{equation}
The derivations of \cref{eq:tr-lte,eq:bdf2-lte} are simple and can be found in most text books on the subject.

We now give a derivation of the local truncation error of IMR, which is somewhat complex because of the approximation $\yv \approx \frac{\yv_n + \yv_{n+1}}{2}$.
We first write the Taylor expansion $\yv(t_{n+1})$ and $\yv(t_{n})$ about $\thf$.
We use the midpoint rather than one of the end points (a typical choice in such calculations) because it results in simpler expressions.
Assuming that $\yv(t)$ is ``sufficiently smooth'' for the required derivatives to exist, its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf[']
  + \frac{\dtn^2}{8} \yvhf['']
  + \frac{\dtn^3}{48} \yvhf[''']
  + \order{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
Similarly, the expansion at $t_n$ about $\thf$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf[']
  + \frac{\dtn^2}{8} \yvhf['']
  - \frac{\dtn^3}{48} \yvhf[''']
  + \order{\dtn^4}.
  \label{eq:taylorn}
\end{equation}
Substituting \cref{eq:taylornp1,eq:taylorn} into \cref{eq:trunc-start} gives\footnote{If we had chosen to Taylor expand about $t_n$ there would be an additional term in $\yv_n''$ in \cref{eq:trunc-mid}.}
\begin{equation}
  \lte^\imr = \underbrace{\frac{\dtn^3}{24} \yvhf[''']}_{\text{I}}
  + \underbrace{\dtn\bigs{ \yvhf['] - \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}^\imr}{2}} }}_{\text{II}}
  + \order{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to \cref{eq:trunc-mid}: the first term (I) is fairly standard for second order time integration schemes (\cf the LTEs for TR and BDF2).
The second term (II) originates from the use of an approximation to $\yv(\thf)$ in the evaluation of $\fv$ (\ie the Runge-Kutta nature of IMR).
The rest of the derivation requires applying Taylor expansions to II and is carried out in \cref{sec:full-imr-lte-calculation}.
The final result shows that IMR is of second order:
\begin{equation}
  \lte^\imr = \frac{\dtn^3}{24} \left[\yvhf['''] - 3 \dfdyhf \cdot \yvhf[''] \right]
  + \order{\dtn^4},
  \label{eq:trunc-final}
\end{equation}
where $\dfdyhf$ is the Jacobian matrix of $\fv$ with respect to $\yv$ evaluated at $t=\thf$.
An additional condition required in the derivation is that the magnitude of the eigenvalues of $\frac{\dtn}{2}\dfdyhf$ are less than one.
This condition is easily satisfied in the asymptotic case for most $\fv$, the case where it does not hold is discussed in more detail in \cref{sec:order-reduction}.

For a true comparison with the other local truncation errors we would need to shift this approximation to $t_n$.
However a simple comparison can be given easily: for $\fv$ linear in $\yv$, $t$ TR and IMR are the same, hence term I must be the same as TR in this case,.
A second error term is then added to $\lte^\imr$ corresponding to II for non-linear $\fv$.
Hence TR has the smallest LTE of the three methods discussed for most real-world ODEs.
The relative magnitude of the additional term in $\lte^\imr$ is highly problem dependent, see \cref{sec:order-reduction} for details, so the comparison of the LTEs of IMR and BDF2 is more complex.

Note however that for the accuracy of calculations the \emph{accumulated} local truncation error is what matters, so these comparisons are not the whole story.

\subsection{A-stability}
\label{sec:A-stability}

To discuss the stability of time integrators the following initial value ODE is widely used:
\begin{equation}
  \begin{aligned}
    \ffv{\yv} &= \lambda \yv, \\
    \yv_0 &= 1.
    \label{eq:ode-test-f}
  \end{aligned}
\end{equation}
Its analytical solution is
\begin{equation}
  \yv(t) = \exp(\lambda t).
\end{equation}

A-stability\footnote{The A does not stand for anything, it is just ``A'' \cite[40]{HairerWanner}. In particular it does \emph{not} mean ``absolute stability''.} is the property that a method is has no stability restrictions on the time step size when used to integrate \cref{eq:ode-test-f} for all $\lambda$ with $\realp(\lambda) \leq 0$.
% When $\realp(\lambda) > 0$ the ODE itself is unstable (, and so stability is not expected in general.

The IMR, TR, BDF1 and BDF2 methods are all A-stable \cite[pgs. 43, 251]{HairerWanner}.
Linear explicit methods are never A-stable \cite{Nevanlinna1974} (``linear'' methods in this reference include all common time integration methods: Runge-Kutta, multistep, predictor-corrector, \ldots), which is the main reason for our focus on implicit methods.
Additionally there are no A-stable linear multistep methods of order greater than two \cite[261]{GreshoSani}, which is why we have chosen the  methods in \cref{sec:some-implicit-time-integrators} for discussion.
It should be noted that A-stable implicit Runge-Kutta methods of higher order do exist, see \eg \cite[73]{HairerWanner}, but they require the solution of significantly larger systems of equations at each time step.

\subsection{Spurious numerical damping}
\label{sec:numerical-damping}

Some time integration methods create additional, non-physical damping in approximate solutions, even when none exists in the true solution.
This can be problematic for the modelling of highly oscillatory ODEs, such as the LLG with low damping parameter $\dampc$.

In the solution of the ODE \cref{eq:ode-test-f} with $\lambda = i\omega$ (and $\omega \in \real$), $\abs{y}$ should not decrease over time.
If $\abs{y_n}$ does decrease over time in the approximate solution given by a time integration scheme, then we say that the scheme causes spurious damping.

For example, for IMR:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega (y_n + y_{n+1})/2, \\
    &= \frac{1 + i\dtn \omega/2}{1 - i\dtn \omega/2} y_n,
  \end{aligned}
\end{equation}
therefore
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}}^2 &=  \frac{\abs{1 + i\dtn \omega/2}^2}{\abs{1 - i\dtn \omega/2}^2} \abs{y_n}^2, \\
    \abs{y_{n+1}}^2 &=  \frac{(1 + i\dtn \omega/2)(1 - i\dtn \omega/2)}
    {(1 - i\dtn \omega/2)(1 + i\dtn \omega/2)} \abs{y_n}^2, \\
    &=  \abs{y_n}^2,
  \end{aligned}
\end{equation}
hence IMR does not cause spurious damping.

Since $f$ is linear in $y$ for this example TR and IMR are identical
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn(i\omega y_n + i\omega y_{n+1})/2, \\
    &= y_n + \dtn i \omega (y_n + y_{n+1})/2,
  \end{aligned}
\end{equation}
and the same property applies for TR.

For BDF1, however, we have:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega y_{n+1}, \\
    &= \frac{1}{1 - i \dtn \omega} y_n.
  \end{aligned}
\end{equation}
Hence,
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}} &= \sqrt{ \frac{1}{(1 - i \dtn \omega)(1 + i \dtn \omega)}} \abs{y_n}, \\
    &= \frac{1}{\sqrt{1 + \dtn^2 \omega^2}} \abs{y_n}, \\
  \end{aligned}
\end{equation}
and the magnitude of $y$ decreases over time, \ie the solution is damped.
The analysis for the second order BDF2 case is more complex due to the fact that it is a multistep method, but the result is the same: the solution suffers from non-physical damping \cite[265]{GreshoSani}.


\subsection{B-convergence and order reduction}
\label{sec:order-reduction}

It is known that certain implicit Runge-Kutta methods are susceptible to reduced accuracy when used to solve certain extremely stiff problems \cite[156]{Atkinson1994} \cite[225]{HairerWanner}.
In the case of the implicit midpoint rule this corresponds to cases when term II of the LTE \cref{eq:trunc-mid} is large.
This occurs when the error in $\ffv{t, \yv}$ due to a small error in $\yv$ is large.

The concept of ``B-convergence'' is used to analyse this effect.
Roughly speaking, a Runge-Kutta method is B-convergent of order $r$ if the global error is $\order{\dtn^r}$ for ``infinitely stiff'' ODEs.
The implicit midpoint rule is B-convergent of order 1 \cite[231]{HairerWanner},\footnote{In this reference IMR is referred to as ``the second order Gauss method'', BDF1 is Radau IIA, TR is Lobatto IIIA \cite[72-76]{HairerWanner}.} which is one order less than its global error on typical ODEs.
We call this effect ``order reduction''.
In contrast, the TR and BDF methods do not suffer from order reduction \cite[159]{Atkinson1994}.
This is because all evaluations of $\fv$ are at integer time points (\ie $t_{n+1}, t_{n}, t_{n-1}, \ldots$ rather than $\thf$) and so there is no term in the LTE containing $\pd{\fv}{\yv}$.

This order reduction effect is a potential downside for IMR, but only when applied to specific ODEs (and no such effect has been reported in micromagnetics simulations we are aware).
Additionally it should be accounted for by a good adaptive time step selection algorithm (see \cref{sec:adaptivity} for details).

A simple test ODE demonstrates this phenomenon \cite[157]{Atkinson2009}:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-test-order-reduction}
    f(t, y) &= -\lambda (y - g(t)) + g'(t), \\
    y_0 &= g(0),
  \end{aligned}
\end{equation}
for some function $g(t)$ and parameter $\lambda \geq 0$.
The exact solution (which can be seen to be correct by substitution) is
\begin{equation}
  y(t) = g(t).
\end{equation}

Note that $\pd{f}{y} = \lambda$, so we can directly control the magnitude of term (II) of the IMR truncation error \eqref{eq:trunc-mid}.

We now derive the LTE for IMR in this example when $\abs{\lambda\dtn} \gg 1$.
From \cref{eq:trunc-implicit-form} we have that
\begin{equation}
  \begin{aligned}
    (1 + \frac{\dtn \lambda}{2})\lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}. \\
  \end{aligned}
\end{equation}
Then using $\abs{\lambda\dtn} \gg 1$:
\begin{equation}
  \begin{aligned}
    \frac{\dtn \lambda}{2} \lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}, \\
    \lte^\imr &= \frac{\dtn^2}{12\lambda} \left[g'''(\thf) - 3 \lambda g''(\thf) \right] + \order{\dtn^4}. \\
  \end{aligned}
\end{equation}
If additionally $\abs{\lambda\dtn} \gg \abs{g'''(t)}$, which will typically be true for very large $\lambda$ and $g \neq g(\lambda)$:
\begin{equation}
  \lte^\imr = \frac{-\dtn^2}{4} g''(\thf) + \order{\dtn^3}.
  \label{eq:reduced-order-imr-truncation-error}
\end{equation}
So we can see that the order has been reduced by one power of $\dtn$ compared with the standard LTE \cref{eq:trunc-final} for cases when $\abs{\lambda\dtn} < 1$.


\subsection{Adaptivity}
\label{sec:adaptivity}

Adaptive time integration methods automatically select time step sizes in response to an estimate of the local truncation error.
This can improve efficiency: for example on problems where a high frequency part of the solution is gradually damped out the time step can increase by orders of magnitude at later times without loss of accuracy.
Adaptive integration also greatly simplifies the use of the numerical method: the user only has to choose the allowed LTE and the algorithm will handle the rest.

Most local truncation error estimators for implicit integrators (e.g. TR, BDF2) compute an explicit estimate of the solution $\yv^E_{n+1} \sim \yv(t_{n+1})$ (sometimes known as a predictor step) to the same order of accuracy as the implicit method.
The explicit method is selected such that it uses only the same derivatives as are required for the implicit method, so no additional $\fv$ evaluations are needed.
Algebraic rearrangements of the LTE expressions for the explicit and the implicit methods are then used to compute an approximation to the actual LTE \cite[707-716]{GreshoSani}.
This is known as Milne's device.

No such estimation is possible for IMR because of the specific form of the local truncation error \eqref{eq:trunc-mid}.
The details of this issue and the first (to our knowledge) algorithm for adaptive time integration with IMR are described in \cref{sec:adaptive-imr}.

It is also possible to construct adaptive time step algorithms based on error estimates specific to the equation being solved.
Such estimates have the obvious limitation that they are only (directly) applicable to the equations for which they were derived.

One example of such an algorithm for the LLG equation is to compare the effective damping with the expected damping and use the difference as an error estimator, as proposed by Albuquerque \etal \cite{Albuquerque2001}.
A major disadvantage of this method is that the error estimator is unable to distinguish between spatial errors and temporal errors.
Another possible method is to use the time error estimates for the LLG equation, with limited effective field terms and a specific finite element discretisation, derived for both IMR and BDF2 by Banas \cite{Banas-thesis}.
However the derivation is fairly complex, and may be difficult to extend to include exchange field or FEM/BEM magnetostatics.


\subsection{Magnetisation length conservation}
\label{sec:ensuring-constant-mv}

We now move on to the discussion of some properties specific to the LLG equation.
As discussed in \cref{sec:prop-cont-llg}, the length of the magnetisation, $\abs{\mv}$, at each point in space is constant over time.
However, in the approximations given by numerical methods this property is often lost, in which case it must be enforced separately.
Note that this is not related to the spurious damping discussed in \cref{sec:numerical-damping}: in that section the key property was the conservation of the amplitude of \emph{oscillations}.

A simple method of dealing with the constraint is to re-normalise each value of $\mv$ after some number of time steps, or when the error in $\abs{\mv}$ exceeds some tolerance \cite{Fidler2000}.
However this approach fundamentally changes the system of equations being solved in a non-linear and unpredictable manner \cite{Lewis2003}.
Renormalisation after a tolerance is used in \magpar \cite{magpar-source-code}\footnote{The function of interest is in the file \texttt{src/util/renormvec.c}, it is called by \texttt{CheckIterationLLG} (which is the standard LLG solver iteration function) with tolerance \texttt{renormtol}.} with a default tolerance of $\E{-2}$, while renormalisation after every step is used in \oommf's 4th order Runge-Kutta scheme \cite{fangor-donahue-priv-comm}.

Another way to avoid this problem is to use a spherical polar coordinate system $(r,\theta,\phi)$ for the magnetisation.
In these coordinates \cref{eq:LLG} can be expressed in terms of only the angles $(\theta,\phi)$ representing the direction of $\mv$ and the length constraint is automatically enforced since
\begin{equation}
  \label{eq:40}
  \pd{\abs{\mv}}{t} = \pd{r}{t} \equiv 0.
\end{equation}
However problems occur with this approach when the polar angle, $\theta$, approaches zero because $\pd{m_{\phi}}{t} \propto \frac{1}{\sin(\theta)}$ \cite{Fukushima2005} and so the derivative becomes singular.
Also the calculation of the sum of the effective field vectors is much more complex in spherical polar coordinates (involving trigonometric identities), resulting in more complex non-linear/linear systems when using implicit time integration schemes.

A third approach is to use Lagrange multipliers to constrain the solution such that $\abs{\mv}$ is constant \cite{Szambolics2008a}.
The main problem with this method is that an additional degree of freedom is needed for every value of $\mv$ in space.

% ??ds first draft
Two more approaches that are currently used in mainstream models are: the use a modified LLG equation with a self-correcting term; and renormalisation by projection ??ds onto sphere?
These approaches are discussed in more detail \cref{sec:sc-llg} and \cref{sec:renorm-to-sphere} respectively.

Geometric time integration schemes aim to solve the issue of length conservation by constructing a scheme that naturally preserves the value of $\abs{\mv}$.
Some examples of such schemes are based on IMR \cite{DAquino2005}, or a semi-implicit extension of IMR using extrapolations of the effective field \cite{Spargo2003,Serpico2001}.
Alternatively, geometric integration methods based on Cayley transforms can be used \cite{Lewis2003,Bottauscio2011}.

% ??ds first draft
\subsubsection{The modified self-correcting LLG}
\label{sec:sc-llg}

An interesting technique to maintain the condition $\abs{\mv} = 1$ was introduced by ??ds and
The idea is that instead of solving the standard LLG equation we solve the modified ``self-correcting'' equation
\begin{equation}
  \label{eq:sc-ll}
  \dmdt = -\frac{1}{(1 + \dampc^2)}\mxh  -\frac{\dampc}{(1 + \dampc^2)} \mxmxh
  + \scc \mv \bigb{1 - \abs{\mv}^2},
\end{equation}
where the final term has been added, and $\scc$ is some constant which must be chosen.
The motivation behind this is that if, after a step, $\abs{\mv} > 1$ then during the next step there will be a an additional term in $\dmdt$ of $-\scc \mv \abs{1 - \abs{\mv}^2}$.
As this additional term is in the direction $-\mv$ it modifies the magnetisation length in the direction of $\abs{\mv} = 1$.
Similarly if $\abs{\mv} < 1$ there is a small additional term in the direction of $+\mv$.

The potential advantage of this method over the standard renormalisation is that the time integration scheme ``knows'' about the renormalisation, and so could potentially provide more accurate error estimates ??ds avoids the need to restart? (but I've never needed to restart anyway...)

A potential downside is that the resulting system is ``more'' non-linear than the standard LL/LLG.
This could mean that additional steps are needed for the non-linear solver to converge and could have a fairly large impact on computation times.

Another downside is that this is more complex to implement than the standard renormalisation method: it requires the discretisation of an additional term, as well as the derivation of its Jacobian contribution.

A third issue is the selection of an appropriate value for the additional parameter $\scc$ must be chosen.
Too large and it may cause non-physical oscillations, reduced non-linear solver convergence and other issuse.
Too small and the benefits of renormalisation are lost.
There is no research on appropriate values for $\scc$. % ??ds show some in appendix?



% %??ds move derivation to LL/LLG section and only reference it from here?
% Note that \cref{eq:sc-ll} uses the Landau-Lifshitz form of the LLG.
% We can derive the equivalent self correcting term for the LLG form as follows.
% We begin with the ansatz (\ie a guess)
% \begin{equation}
%   \dmdt = - \mv \times \hv + \dampc \mv \times \dmdt + \scc \mv \bigb{1 - \abs{\mv}^2},
%   \label{eq:sc-llg}
% \end{equation}
% then, following the standard, derivation for converting from the LLG form to the LL form \cite[181]{Aharoni1996}, we take $\mv \times$ on both sides of \cref{eq:sc-llg} to find
% \begin{equation}
%   \mv \times \dmdt = - \mv \times \bigb{\mv \times \hv}
%   + \dampc \mv \times \bigb{\mv \times \dmdt}
%   + \scc \bigb{\mv \times \mv} \bigb{1 - \abs{\mv}^2}.
%   \label{eq:106}
% \end{equation}
% The final term in \cref{eq:106} is zero by the definition of the cross product.
% Using the double cross product identity $ \av \times \bigb{\bv \times \cv} = \bv (\av \cdot \cv) - \cv (\av \cdot \bv)$ and the fact that $\dmdt \cdot \mv = 0$ the second term becomes
% \begin{equation}
%   \label{eq:107}
%   \dampc \mv \times \bigb{\mv \times \dmdt}
%   = \dampc \mv (\mv \cdot \dmdt) - \dampc\dmdt (\mv \cdot \mv)
%   = -\dampc \dmdt.
% \end{equation}
% Hence by combining \cref{eq:105,eq:106,eq:107} we obtain
% \begin{equation}
%   \dmdt = - \mv \times \hv
%   - \dampc \mv \times \bigb{\mv \times \hv} -\dampc^2 \dmdt
%   + \scc \mv \bigb{1 - \abs{\mv}^2},
% \end{equation}
% which is exactly \cref{eq:sc-ll} so \cref{eq:sc-llg} and \cref{eq:sc-ll} are equivalent.

% ??ds but this is only for the case when m is conserved! Which is obvious since we are adding a zero term to both equations...


This method is used by \nmag \cite{fangor-donahue-priv-comm} \cite{Fischbacher2009}.

% ??ds first draft
\subsubsection{??ds projection to sphere}
\label{sec:renorm-to-sphere}

This method is used by \oommf's forward Euler scheme \cite{fangor-donahue-priv-comm}.


\subsubsection{Conservation of $\abs{\mv}$ by IMR in a macrospin model}
\label{sec:proof-magn-length-ode-imr-llg}

In \thisref{sec:proof-magn-length-ode-imr-llg} we show that the discrete form of the Landau-Lifshitz-Gilbert equation that results from the use of the implicit midpoint rule maintains the magnetisation length conservation property of the continuous form shown in \cref{sec:prop-cont-llg}.
These results are applicable to strong form equations, \ie finite difference and macrospin discretisations but not finite elements.
For the purposes of this section we assume that the discretised non-linear Landau-Lifshitz-Gilbert equation is satisfied exactly, \ie the linear and/or non-linear solver used gives exactly correct results.

As can be seen from \cref{eq:imr-definition} the IMR is defined by the following substitutions:
\begin{equation}
\begin{aligned}
  \label{eq:55}
  \pd{\mv}{t} &\rightarrow \frac{\mv_{n+1} - \mv_n}{\dtn}, \\
  \mv &\rightarrow \frac{\mv_{n+1} + \mv_n}{2}, \\
  t &\rightarrow \frac{t_{n+1} + t_n}{2}.
\end{aligned}
\end{equation}
So the IMR-discretised version of the LLG is
\begin{equation}
  \frac{\mv_{n+1} - \mv_n}{\dtn} = - \frac{\mv_{n+1} + \mv_n}{2} \times
  \bigb{\hv \bigs{\frac{\mv_{n+1} + \mv_n}{2}} - \dampc \frac{\mv_{n+1} - \mv_n}{\dtn}}.
  \label{eqn:disc-llg}
\end{equation}

The conservation properties of the IMR discretisation come from the cancellation of cross terms in the inner product $\ip{\lop[\mv]}{\dmdt}$ for any symmetrical linear operator $\lop$.
More precisely:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-linop}
    \ip{\lop \left[ \frac{\mv_{n+1} + \mv_n}{2} \right]}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}} + \ip{\mv_{n+1}}{\lop \mv_{n}} \\
    & \qquad\qquad - \ip{\mv_{n}}{\lop \mv_{n+1}} - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big] \\
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}}
    - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big].
  \end{aligned}
\end{equation}
This means that if $\lop \mv$ is orthogonal to $\dmdt$ in the continuous or semi-discretised equations then it will also be orthogonal in the IMR-discretised version.
In particular, setting $\lop$ as the identity operator will allow us to show that $\mv$ and $\dmdt$ are orthogonal in \thisref{sec:proof-magn-length-ode-imr-llg} and setting $\lop = \hop$ will allow us to show some nice properties of the energy in \cref{sec:proof-energy-prop}.

We now examine the change in magnetisation length using the same technique as in \cref{sec:prop-cont-llg}: take the dot product of \cref{eqn:disc-llg} with $\frac{\mv_{n+1} + \mv_n}{2}$ on both sides and use the triple product identity \cref{eq:dot-cross-id} to get
\begin{equation}
  \label{eq:63}
  \frac{\mv_{n+1} + \mv_n}{2} \cdot \frac{\mv_{n+1} - \mv_n}{\dtn} = 0.
\end{equation}
Now using~\cref{eqn:imr-linop} with $\lop$ as the identity operator and the dot product as the inner product gives us
\begin{equation}
  \frac{\ip{\mv_{n+1}}{\mv_{n+1}} - \ip{\mv_n}{\mv_n} }{2 \dtn} = 0.
\end{equation}
Therefore
\begin{equation}
  \abs{\mv_{n+1}} = \abs{\mv_n},
\end{equation}
\ie the magnetisation length does not change between time steps.

Note that for other time integration schemes the property \cref{eqn:imr-linop} typically does not hold.
For example, in the case of BDF1 we have
\begin{equation}
  \begin{aligned}
    \ip{\lop \bigs{\mv_{n+1}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_{n+1}}{\lop \mv_n} }, \\
    &\neq \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_n}{\lop \mv_n} },
  \end{aligned}
\end{equation}
\ie orthogonality properties from the continuous form do not carry over to the discrete form.

Notice that the only requirement in the derivation of \cref{eq:63} is to use the midpoint approximation of magnetisation in the LLG itself.
The derivation does not make any assumption about how the effective field is approximated.
This explains why various semi-implicit modifications to IMR which use explicit calculations of effective fields also conserve the magnetisation length.

% ??ds explain what various semi-implicit modifications to IMR are  -- Andrew


\subsection{Energy conservation/decay}
\label{sec:energy-cons}

Another geometrical property of the Landau-Lifshitz-Gilbert equation, also discussed in \cref{sec:prop-cont-llg}, is the conservation or monotonic decay of energy depending on the value of the damping constant $\dampc$ (and the applied field $\happ$).

IMR also conserves energy when $\dampc = 0$ and it ensures that the energy is a decreasing function of time when $\dampc > 0$ and $\happ$ is constant \cite{DAquino2005}, a proof of this property is given in the next section.
The other geometric integration schemes mentioned in \cref{sec:ensuring-constant-mv}, and in particular the semi-implicit modifications of IMR, do not have this property.


\subsubsection{Proof of energy property for the IMR discretised macrospin model}
\label{sec:proof-energy-prop}

\newcommand{\happerror}{\mathcal{E}_\text{ap}}

In \thisref{sec:proof-energy-prop} we show that the change in the energy of the magnetic system over time when evolved using the IMR discretised LLG equation obeys certain relationships which are similar to those of the continuous LLG equation.

Before starting the derivation we introduce some alternative notation for the effective field when considered as an operator, $\hopb{\mv}$:
\begin{equation}
  \begin{aligned}
    \label{eq:hop}
    \hv(\xv, t) &= \hopb{\mv(\xv)} + \happ(\xv, t). \\
    \hopb{\mv(\xv)} &= \lap \mv(\xv) + \hms[\mv(\xv)] + \hca[\mv(\xv)], \\
  \end{aligned}
\end{equation}

We also expand the midpoint value of the applied field, $\mphapp$, into
\begin{equation}
  \begin{aligned}
    \mphapp &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2}
    + \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4}, \\
    &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror,
    \label{eq:happ-midpoint}
  \end{aligned}
\end{equation}
where
\begin{equation}
  \happerror = \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4},
\end{equation}
is the error in this midpoint-like approximation of $\happ$.

Now we are ready to derive the energy property in the same manner as \cref{sec:proof-magn-length-ode-imr-llg}.
We begin by taking the $\ltwo$ inner product of the IMR discretised LLG equation, \cref{eqn:disc-llg}, with the sum of the IMR discretised effective field and damping terms:
\begin{equation}
  \begin{aligned}
  & \mphop + \happ\bigb{\mpt} - \dampc \mpdmdt, \\
  &= \mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror - \dampc \mpdmdt.
  \end{aligned}
\end{equation}
The result of this inner product is
\begin{equation}
  \begin{aligned}
    &\ltip{\mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror}{\mpdmdt} \\
    & \quad - \dampc \ltip{\mpdmdt}{\mpdmdt} = 0,
    \label{eq:54}
  \end{aligned}
\end{equation}
where the RHS is zero due to the triple product identity.

It can be shown (see \eg \cref{sec:linear-symm-field-operators}) that $\hop$ is a linear symmetric operator on $\mv$.
So from \cref{eqn:imr-linop} with $\lop = \hop$ we have the property
\begin{equation}
  \ltip{\hopb{\frac{\mv_{n+1} + \mv_n}{2}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
  = \frac{1}{2\dtn} \bigs{ \ltip{\mv_{n+1}}{\hop \mv_{n+1}} - \ltip{\mv_{n}}{\hop \mv_{n}}}.
  \label{eq:orth-energy}
\end{equation}
Additionally the energy at time step $n$ due to the effective field operator $\hop$ can be written as\footnote{This essentially follows from the definitions of the non-dimensional energies and the effective fields given in \cref{sec:land-lifsh-gilb-normalisation}, details are given in \cref{sec:energy-field-relation}.}
\begin{equation}
  \label{eq:energy-hop}
  \ehop_{,n} = - \frac{1}{2}\ltip{\mv_n}{\hopb{\mv_n}}.
\end{equation}
So using \cref{eq:energy-hop,eq:orth-energy} we can write the $\hop$ term of \cref{eq:54} as
\begin{equation}
  \begin{aligned}
    \ltip{\mphop}{\mpdmdt} &= -\frac{\ehop_{,n+1} - \ehop_{,n}}{\dtn}.
  \end{aligned}
  \label{eq:50}
\end{equation}

Next we examine the applied field term of \cref{eq:54}.
Omitting the $L^2$ from the inner products and temporarily writing $\happ(t_i)$ as $\hv_i$ for brevity we have
\begin{equation}
  \begin{aligned}
    \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}
    &= \ip{\hv_{n+1} + \hv_n}{\mpdmdt} \\
    & \qquad - \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}, \\
    &= -\frac{\eapp_{, n+1} - \eapp_{, n}}{\dtn}  + A,
    \label{eq:76}
  \end{aligned}
\end{equation}
where
\begin{equation}
  \begin{aligned}
    A &= \frac{1}{2\dtn}\Big[ 2\ip{\hv_n}{\mv_{n+1}} - 2\ip{\hv_{n+1}}{\mv_n} \\
    & \qquad - \ip{\hv_{n+1}}{\mv_{n+1}} +\ip{\hv_{n+1}}{\mv_n}
    - \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= \frac{1}{2\dtn}\Big[ - \ip{\hv_{n+1}}{\mv_{n+1}} - \ip{\hv_{n+1}}{\mv_n}
    + \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= - \ip{ \frac{\hv_{n+1} - \hv_n}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }. \\
    % &= - \ltip{ \frac{\happ(t_{n+1}) - \happ(t_n)}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }.
    \label{eq:52}
  \end{aligned}
\end{equation}
% ??ds Check this derivation, factors of 2?  -- Andrew
Finally, we insert the results of \cref{eq:50,eq:76,eq:52} into \cref{eq:54} to find that the change in the total energy is
\begin{equation}
  \begin{aligned}
    \frac{\e_{n+1} - \e_n}{\dtn} = &-\dampc \ltnorm{\mpdmdt}^2
    - \ltip{\frac{\happ(t_{n+1}) -\happ(t_n)}{\dtn}}{\frac{\mv_{n+1} + \mv_{n}}{2}} \\
    &\quad+ \ltip{\happerror}{\mpdmdt}.
    \label{eqn:imr-llg-energy}
  \end{aligned}
\end{equation}
The first term of this expression is exactly the discrete representation of the energy loss due to damping.
The second term is the discrete representation of the energy change due to changes in the applied field.
The third term is the error due to changes in the applied field which are not accurately captured by the approximations used in IMR.
In particular note that the energy is conserved when $\dampc = 0$ and the applied field is constant.
This relationship is illustrated in \cref{fig:commutation-imr-energy}.

When $\happ$ is piecewise linear, for example when an applied field is instantaneously switched on, time steps can be selected such that each non-linear change happens exactly at the start/end of a step.
So again we have $\happerror = 0$ and the same properties as for linear applied fields.

It should be noted that none of the above is applicable to semi-implicit modifications of IMR since in that case $\mphop$ is replaced by an expression of the form $a\hopb{\mv_n} + b\hopb{\mv_{n-1}}$.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 10cm, auto, >=latex']
    \node[block] (start) {Continuous LLG};
    \node[block, right of=start] (discrete) {IMR discretised LLG};
    \node[block, below of=start, node distance = 3cm] (energy) {Continuous energy};
    \node[block, right of=energy] (denergy) {IMR discretised energy};

    \path [->] (start) edge node {Discretise} (discrete);
    \path [->] (start) edge node {Derive energy} (energy);
    \path [->] (energy) edge node {Discretise} (denergy);
    \path [->] (discrete) edge node {Derive energy} (denergy);
  \end{tikzpicture}
  \caption{Commutation relationship between the IMR discretisation and the LLG energy derivation.
In contrast to other time integration methods the result is independent of the order of operations.}
\label{fig:commutation-imr-energy}
\end{figure}


\subsection{Conclusions}

In the previous sections we have described a number of important properties of time integration schemes.

We also highlighted a number of interesting geometric integration properties of the implicit midpoint rule.
There are a number of reasons to believe that the geometric integration properties will translate into an improvement in the accuracy and robustness of the overall solver:
\begin{itemize}
\item Errors in the energy dissipation \cite{Albuquerque2001} and magnetisation length \cite{Chantrell2001} have been successfully used as error estimators for local truncation error.
Hence the removal of energy or magnetisation length errors should reduce the total local truncation error.
\item It is well known that geometric integration schemes can result in much smaller long-timescale error build-up than schemes that do not preserve such quantities \cite[77]{Iserles2009}.
\item The non-linear modification to the Landau-Lifshitz-Gilbert equation caused by renormalisation of the magnetisation length (as commonly used to maintain correct magnetisation length in non-conservative time integrators) may cause significant step changes in the magnetostatic field \cite{Lewis2003}.
This also modifies the balance between the various energy terms, which is similar to the methods that lead to the ``flying ice cube'' problem \cite{Harvey1998} in molecular dynamics.\footnote{In such problems the rescaling of particle velocities (to maintain constant temperature despite numerical error accumulation) can result in large amounts of kinetic energy being transferred from the motion of internal degrees of freedom to motion of the centre of mass.}
Avoiding renormalisation eliminates the possibility of such errors occurring.
\end{itemize}

Additionally, the same geometric properties enable a proof of convergence for IMR when applied to a FEM semi-discretised LLG equation with a slight modification that will be discussed in \cref{sec:nodal-integration} \cite{Bartels2006}.

Note that no assumption of fixed $\dtn$ was made, and that there is no dependence on previous values of $\dtn$.
Hence the geometric properties are expected to still hold when the step sizes are varied.


\section{Linearisation}
\label{sec:linearisation}

When implicit time integration methods are applied to a non-linear differential equation (such as the LLG equation) a non-linear system of equations must be solved.
Explicit time integration methods sidestep the issue of non-linearity by avoiding the need to solve any system.

In the context of implicit time integration of a semi-discretised PDE we write
\begin{equation}
  \yvdis_n = \set{\yv_{n,i}}_{i=0}^{i=N},
\end{equation}
for the list of spatially discrete values of $\yv$ at time step $n$.
For example, for an LLG problem consisting of a single macrospin $\yvdis_n = \mv_n$.
For an LLG problem semi-discretised by the finite element method $\yvdis_n$ is the list of all the discrete values that define $\mv_n$ along with any values needed for the magnetostatic calculations.


\subsection{Functional iteration}
\label{sec:picard}

Functional iteration is the process of repeatedly applying a function until the result converges to a fixed point.
This process can be applied to find the solution of a non-linear system by constructing a function such that its fixed point solves the system.

As an example we can use functional iteration to solve the non-linear system resulting from the discretisation of a non-linear ODE, written as $\dydt = \fv(t,y)$, by the implicit midpoint rule.
We first solve for an auxiliary variable $\wvdis$ using the iteration
\begin{equation}
  \begin{aligned}
    \nlit{\wvdis}{0} &= \yvdis_n, \\
    \nlit{\wvdis}{i+1} &= \frac{\dtn}{2} \fv(t_n + \frac{\dtn}{2}, \nlit{\wvdis}{i}) + \yvdis_n.
    \label{eq:98}
  \end{aligned}
\end{equation}
This variable can be thought of as the approximation for $y$ used in the evaluation of $f$, \ie $\wv = (\yvdis_{n+1} + \yvdis_n)/2$.
Once the iteration \cref{eq:98} has converged (as determined, for example, by $\abs{\nlit{\wv}{i+1} - \nlit{\wv}{i}} < \epsilon$), the value of $\yvdis_{n+1}$ is calculated using
\begin{equation}
  \yvdis_{n+1} = \yvdis_n + \dtn \fv(t_n + \frac{\dtn}{2}, \wvdis).
\end{equation}

The main advantage of functional iteration is that convergence proofs can be easily constructed via the Banach fixed point theorem (albeit under some fairly restrictive assumptions on time step size) \cite[125]{Iserles2009}.

Additionally, the cost of each iteration can be very cheap if the function $\fv(t, \yv)$ can be written explicitly (\ie the ODE is not implicit in $\dydt$), since it only requires the application of a function rather than a linear solve required by the Newton-Raphson method.
This is not the case for the LLG, but the linear systems resulting from functional iteration are simpler than those resulting from other methods, which can simplify the construction of an efficient linear solver \cite{Bartels2006}.

However, the functional iteration only converges for time steps of a limited size.
This limit is related to the spatial discretisation, resulting in similar problems to explicit methods.
This makes the method useful for applying implicit methods to non-stiff problems, but less useful for stiff ones \cite{Iserles2009}.


\subsection{Newton-Raphson method}
\label{sec:newt-raph}

A non-linear problem can be written in residual form as
\begin{equation}
  \text{Find $\yvdis_{n+1}$ such that } \resi(t_{n+1}, \yvdis_{n+1}) = 0.
\end{equation}

The motivation for the Newton-Raphson method comes from a simple Taylor expansion of the residual, $\resi$.
If the exact root of the residual is $\nexty_{n+1}$ and we have an initial guess for this root $\nlit{\yvdis_{n+1}}{0}$ then we can obtain a correction to this initial guess from:
\begin{equation}
  \begin{aligned}
    0 &= \resi(t_{n+1},\nexty_{n+1}), \\
    &= \resi(t_{n+1}, \nlit{\yvdis_{n+1}}{0} + \corr), \\
    &= \resi(t_{n+1}, \nlit{\yvdis_{n+1}}{0}) + \jac(t_{n+1}, \nlit{\yvdis_{n+1}}{0}) \cdot \corr + \order{\corr^2},
    \label{eq:99}
  \end{aligned}
\end{equation}
where $\jac(t_{n+1}, \nlit{\yvdis_{n+1}}{0})$ is the Jacobian matrix of the residual with respect to $\yvdis_{n+1}$ evaluated at $\nlit{\yvdis_{n+1}}{0}$.
From \cref{eq:99} we have
\begin{equation}
  \begin{aligned}
    \label{eq:49}
    \nexty_{n+1} &= \nlit{\yvdis_{n+1}}{0} + \corr, \\
    &= \nlit{\yvdis_{n+1}}{0} + \jac^{-1}(t_{n+1}, \nlit{\yvdis_{n+1}}{0}) \cdot \resi(\nlit{\yvdis_{n+1}}{0}) + \order{\corr^2}.
  \end{aligned}
\end{equation}
This suggests the iteration
\begin{equation}
  \nlit{\yvdis_{n+1}}{i+1} = \nlit{\yvdis_{n+1}}{i} - \jac^{-1}(t_{n+1}, \nlit{\yvdis_{n+1}}{i}) \cdot \resi(\nlit{\yvdis_{n+1}}{i}),
\end{equation}
in which, if the residual is sufficiently smooth and the required correction is ``small'', the error is \emph{squared} after each iteration (\ie convergence is quadratic).

The assumption that we can discard terms in $\order{\corr^2}$ is fairly strong: it means that we need a good initial guess so that the initial correction, $\corr$, is small.
Fortunately in time integration problems the value at the previous time step provides a good initial guess: $\nlit{\yvdis_{n+1}}{0} = \yvdis_n$.
It is also possible to construct an even better initial guess by taking a single cheap explicit ``predictor'' step, as used in Milne's device for adaptive time integration (and if such an adaptive scheme is in use then this improved initial guess is ``free'').

The iteration is terminated when
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \ntol,
\label{eq:newton-convergence-test}
\end{equation}
for some user defined tolerance $\ntol$.
Alternatively the absolute tolerance condition \cref{eq:newton-convergence-test} can be replaced by, or combined with, a relative tolerance condition
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \norm{\resi(\nlit{\yvdis}{0})}_{\infty} \nrtol.
  \label{eq:100}
\end{equation}
If the residual is normalised, \ie has initial magnitude of $\order{1}$, then \cref{eq:newton-convergence-test,eq:100} are roughly equivalent.

% For micromagnetics problems we find that this reliably converges to a tolerence of $\sim 10^{-10}$ in around 2-3 steps!

As discussed in \cref{sec:picard} the main downside of the Newton-Raphson method is that it always requires the solution of a system of linear equations to obtain each correction $\corr$, and that these systems are more complex than those produced by function iteration.
The efficient solution of these systems is the subject of \cref{sec:solution-lin-sys}.

Another potential downside is that the Newton-Raphson method is only guaranteed to converge if the initial guess ``good enough''.
However for non-linear systems resulting from time integration we always have a very good initial guess, so this is not often a problem.


% \subsection{Advanced LLG-specific linearisation methods}
% \label{sec:advanced-lin}


% Linearisation based on choice of test functions of much interest in recent times, initially for only exchange effective field \cite{Alouges2008}.
% Has recently been extended to handle general effective field contributions \cite{Banas2012}.

% Advantages:
% \begin{itemize}
% \item Only need to solve one linear system per step (rather than $\sim 2$ for Newton's method).
% \item Jacobian matrix is constant.
% \end{itemize}

% Disadvantages:
% \begin{itemize}
% \item Method is limited to first order: so need step sizes orders of magnitude smaller.
% \item Have to use specialised time integration scheme (no geometric, adaptive, ... schemes).
% \item Specific to the LLG equation: standard code libraries may not work, not as widely understood as standard FEM, extension to related equations (stochastic LLG, LLGS, LLB) is non-trivial.
% \end{itemize}

% The limitation to first order has been worked around recently, however the higher order schemes lose either stability or linearity \cite{Kritsikis2014}.
% Since the advantage of the scheme over standard methods was this unique combination of properties, this appears (to me at least) to not be very useful.

% Still a promising line of research though!



\section{Solution of sparse linear systems}
\label{sec:solution-lin-sys}

If we are using an implicit time integration scheme with the Newton-Raphson method for linearisation, then we are left with the problem of solving a sequence of sparse linear systems.
Each of these linear systems can be stated as the problem: Given a sparse $\nrow \times \nrow$ matrix $\mat$ and a vector $\bv$, find $\xv$ such that
\begin{equation}
  \label{eq:linear-system}
  \mat \xv = \bv.
\end{equation}
This is a very widely studied problem, see for example \cite{Saad2000}, but efficient techniques for very large $\nrow$ (which corresponds to a large spatial problem and/or good spatial resolution) are complex and problem dependent.
There are two general classes of linear solvers: direct solvers and iterative solvers.
Direct solvers are very general, but are typically computationally expensive in both memory and time.
Effective iterative solvers are usually problem specific but can be much more efficient and can even achieve optimal scaling (\ie of computational complexity $\order{\nrow}$ in time and memory).


\subsection{Direct methods}
\label{sec:direct-methods}

Direct methods for the solution of linear systems seek exact solutions using methods based on Gaussian elimination.
The direct method most commonly used in the solution of systems resulting from discretised PDEs is LU decomposition.

% ??ds Citation for this?  -- Andrew
The main advantage of direct methods is their robustness: for reasonably non-singular systems and given enough time a direct solver will be able to solve the system.
However as matrix sizes become large direct solvers become increasingly demanding in both time and memory.
The problem is that the factors of a typical sparse matrix can be significantly more dense than the original matrix, \ie there is some amount of fill-in.
This means that the time and memory requirements for the solution rarely scale as $\order{\nrow}$ except for very specific matrices.
The exact level of fill-in depends on the sparsity pattern of the initial matrix, but the final computational complexity is typically between $\order{\nrow^{3/2}}$ and $\order{\nrow^2}$ \cite{??ds: need citation for this for corrections, Iserles?}.

For three dimensional PDE problems there are significantly more non-zero entries in each row than in two dimensions due to the increased connectivity of the discrete points.
Hence the performance of a direct solver on three dimensional problems is much worse than for comparable problems in two dimensions, even if $\nrow$ remains the same.

\subsection{Krylov solvers}
\label{sec:krylov-solvers}

The first class of iterative methods for the solution of linear systems that we discuss are the Krylov solvers \cite[151]{Saad2000}.
These methods are based on finding an approximation to the solution, $\xv$, in the sequence of Krylov subspaces of the matrix $\mat$:
\begin{equation}
  \krylov_m(\mat, \yv) = \spanop \set{\yv, \mat \yv, \mat^2\yv, \ldots, \mat^m\yv}.
  \label{eq:96}
\end{equation}
Note that computing a basis for the next Krylov subspace $\krylov_{m+1}$ only requires a matrix vector product with the matrix $\mat$ applied to a basis vector of $\krylov_m$ (although in practice the simple basis in \cref{eq:96} is modified to improve the properties of the method).
Hence constructing the subspace is computationally cheap for a sparse $\mat$.
At each iteration of a Krylov method a new approximation to the solution in the space $\krylov_{m+1}$ is found by minimising some easy to calculate form of the error in the direction given by the new Krylov basis vector.

The two Krylov solvers used in this thesis are the \emph{method of conjugate gradients} (CG) and the \emph{generalised minimum residual method} (GMRES).
The basic procedure for these methods is:
1) Find a new search direction in the $m$-th Krylov subspace which is ``orthogonal'' to all previous directions.
2) Find the point in that direction which in some sense gives the ``optimally accurate'' solution and add this vector to the solution.
The details of what is meant by ``orthogonal'' and ``optimal'' depends on the method.

The key difference between the two methods is that CG is only applicable to symmetric positive definite matrices, whereas GMRES is applicable to general matrices.
However, CG is the more efficient of the two: it takes only $\order{\nrow}$ computation time for each step of the solve, whereas GMRES takes $\order{\nrow k}$ time for the $k$-th step (and similarly for the memory requirements).
This is because CG uses the Lanczos algorithm (which can be represented as a 3-term recurrence relationship) for the orthogonalisation of the search directions, while GMRES uses Arnoldi's method \cite[153]{Saad2000}.
This limitation of GMRES can be avoided by restarting the method after some number of steps with the current approximation as the new initial guess, or by using an alternative Krylov solver such as BiCGStab \cite[172]{HowardElmanDavidSilvester2006}.
However, these methods lose some of the convergence guarantees of GMRES and CG, hence we avoid using them in this thesis.

The size of the Krylov subspace needed for an effective approximation (\ie the number of iterations needed) is strongly dependent on the properties of the matrix, in particular its eigenvalues and eigenvectors.
The maximum number of iterations to find a solution using CG or GMRES can be shown to be $\nrow$, if numerical error is ignored.
However, for the solver to be efficient the number of iterations should be far smaller than $\nrow$, and preferably constant with respect to $\nrow$.

In order to achieve optimal performance we often instead solve a modified system of the form
\begin{equation}
  \begin{aligned}
    \Dm \xv &= \cv, \quad \Dm = \precond^{-1} \mat, \quad \cv = \precond^{-1} \bv,
  \end{aligned}
\end{equation}
where $\precond$ is a \emph{preconditioner} which is chosen such that the the matrix $\precond^{-1} \mat$ has the required eigenvalue and eigenvector properties for rapid convergence.
Since the preconditioner must be applied at every step of the Krylov solve it should be computationally cheap to compute $\precond^{-1}\xv$.
The construction of a preconditioner which results in iteration counts independent of the matrix size while also being computationally cheap to set up and apply is a difficult and problem dependent task.
In the next section we briefly discuss some approaches for such preconditioners.

Note that the preconditioner, $\precond$, must be the same at each step in order for the analysis of most Krylov methods to hold.
This limitation can be avoided by the use of the flexible GMRES method (FGMRES) \cite{Saad1993}, which has performance and memory usage very similar to standard GMRES.


\subsection{Preconditioning of Krylov solvers}
\label{sec:preconditioners}

As mentioned above, the preconditioning of Krylov solvers is a critical component in obtaining good performance.
However the construction of a good preconditioner is typically problem specific.

There are a number of ``general purpose'' preconditioners which have some beneficial effect on the convergence of the Krylov solver for most linear systems.
However they typically do not result in low numbers of iterations independent of the number of rows in the matrix.
One widely used general purpose preconditioner is the incomplete LU decomposition (ILU) \cite[287]{Saad2000}.
This method uses an LU decomposition, but with the number of additional non-zero values (the fill-in) limited in such a way that the matrix remains sparse and operations remain cheap.
The simplest way to limit the fill-in is to allow only $n$ ``generations'' of fill-in, \ie with $n=0$ no fill-in is allowed, with $n=1$ only the original matrix elements are allowed to generate new elements, etc.
We denote the ILU method with $n$ levels of fill-in by ILU($n$).

Another useful class of preconditioners are based on (algebraic) \emph{multigrid} methods which are discussed in \cref{sec:multigrid-methods}.
Multigrid-based preconditioners are highly effective for the preconditioning of linear systems arising from Poisson problems (\ie $\lap \phim(\xv) = f(\xv)$), and other similar problems \cite{Henson2002}.

An interesting preconditioning approach for linear systems arising from PDEs containing multiple physical effects is \emph{block preconditioning}.
This involves the division of the matrix $\mat$ into blocks corresponding to the different physical degrees of freedom.
A preconditioner can then be constructed by manipulating this matrix on the block level, typically based on the physics of the problem.
For example, by removing or eliminating off-diagonal blocks the preconditioner can be solved by inverting the diagonal blocks and using block back/forward substitution.
The inverse of the diagonal blocks can be approximated using known effective preconditioners for the equivalent single-physics problem.
In \cref{sec:solution-strategies} we introduce such a preconditioning approach for the solution of the coupled LLG-magnetostatics system.



\subsection{Multigrid methods}
\label{sec:multigrid-methods}

There are a number of simple iterative methods, called relaxation methods, which are fairly ineffective when applied directly to most linear systems but are very cheap to apply.
Examples of these methods are Jacobi, Gauss-Seidel and SOR methods \cite[103]{Saad2000}.

Multigrid methods are a class of linear solvers which combine these relaxation methods with a hierarchical algorithm.
They can be highly efficient and scalable solvers for the discrete systems arising from certain classes of PDEs \cite{multigrid-tut}.
Multigrid methods are based on two observations: firstly that relaxation methods are very effective at reducing the high-frequency (in space) error of a solution, but fail on the low frequency parts.
The second observation is that many of the low frequency errors can be converted to high frequency errors by projecting them onto a coarser grid.
So by solving the problem on a sequence of meshes all frequencies of error except for the very lowest can be treated effectively using only cheap relaxation methods.
The lowest frequency errors can be well represented on a very coarse grid (\ie a very small linear system), and so can be cheaply resolved using a direct solve.

Typical multigrid solvers are based on something similar to the following algorithm:

\begin{algorithm}[H]
Start with initial guess for the solution on the finest mesh\;
Apply a few iterations of a relaxation method\;
Project the solution onto a coarser mesh\;
Apply a few iterations of a relaxation method\;
Project the solution onto a coarser mesh\;
\qquad $\vdots$\\
On the coarsest mesh solve the resulting problem directly\;
Project the solution onto a finer mesh\;
Apply a few iterations of a relaxation method\;
\qquad $\vdots$\\
End with the corrected solution on the finest mesh\;
\end{algorithm}

The algorithm above is called a V-cycle because a diagrammatic representation of the mesh sizes used draws a ``V'' shape.
There are a number of similar but more complex cycles possible.
The details of the cycle(s), along with the relaxation method used, the projection between meshes, the selection of coarse meshes etc. all depend on the particular variant of the multigrid method used.

It turns out that for certain problems multigrid solvers can achieve computation times which scale as $\order{\nrow \log \nrow}$, \ie their performance does not severely degrade as the number of nodes in the mesh is increased.
The memory usage of multigrid methods is around $\order{\nrow}$, depending on the method used to obtain the coarse meshes.
In particular, as noted in the previous section, multigrid methods are very effective on systems involving the matrix corresponding to a discrete Laplace operator.
This makes them useful for a number of problems because the Laplace operator is an important part of many PDEs.

Standard multigrid methods make use of the geometric properties of the mesh underlying the PDE, which makes them difficult to apply to most unstructured meshes.
However the same techniques can be applied without any knowledge of the underlying geometry using \emph{algebraic multigrid} (AMG) methods.
These methods use information purely from the matrix itself to construct ``coarse'' versions of the matrix based, for example, on the strength of the coupling between matrix elements.



\section{Compatibility of methods}
\label{sec:comp-meth}

While in theory any of the methods discussed above can be combined with any other, there are certain combinations which are more effective than others.
In \thisref{sec:comp-meth} we discuss which combinations of methods are particularly compatible or incompatible.

The use of integral methods for magnetostatic field calculations with implicit time integration is problematic: the integral formulation couples magnetisation at every point to every other point, meaning that the Jacobian representing the derivatives of each magnetisation with respect to each other magnetisation is a full matrix (and hence solution times are extremely slow).
Explicit time integration methods avoid this issue because no system solve is required.

For problems with very simple geometry (\ie thin films, cuboid shapes) the finite difference method with FFT for magnetostatic calculations is sufficient.
Additionally: for simple geometry there is often no need for well refined meshes, so the problem can be only mildly stiff and explicit time integration methods can be efficient.
The most well known example of such a model is \oommf \cite{oommf-website}.
This combination of methods is very simple to implement and so is often used for GPU based code \cite{Vansteenkiste2011}.

For problems with more complex geometry the finite element method must be used for spatial discretisation to obtain good accuracy.
Magnetostatic calculations on the resulting non-uniform mesh are typically carried out using FEM/BEM (\eg in \nmag \cite{Fischbacher2007}, \magpar \cite{Scholz2003} and \femme) although a non-uniform FFT approach has been used in at least one model \cite{Chang2011}.
The use of refined meshes to fully resolve the geometry can often result in stiff problems, requiring the use of implicit time integration schemes for good efficiency (see \cref{cha:stiffn-llg-equat} or \cite{Shepherd2014}).
In all ``real world'' models that we know of using implicit time integration the Newton-Raphson method is used for linearisation.
Typically some form of preconditioned Krylov solver is then used to solve the resulting linear systems.


\subsection{Solvers and geometric integration}

It should be noted that the geometric properties of IMR derived in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} are only true up to the accuracy with which we solve the discrete LLG.
This is the tolerance of the linearisation method used if the problem is non-linear or the tolerance of the iterative solver if it is linear.

Additionally the derivations in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} assume that all fields are calculated at the midpoint without any additional approximations, \ie that we are using a fully implicit (monolithic) method.
In contrast to this, some models use an explicit approximation to the magnetostatic field in order to avoid a dense contribution due to the long-range magnetostatic interactions\footnote{At least a dense sub-block arises from all accurate and commonly used magnetostatic calculation methods, in particular: FEM/BEM, FFT and FMM. See \cref{sec:magn-field-calc} for more details.} in the non-linear solve.
Unfortunately, in this case the energy properties will be lost due to the replacement of $\hopb{\mpm}$ by an alternative approximation (in a similar way to the semi-implicit modification of IMR).
However, the magnetisation length property will be retained because it only relies on $\mv$ itself being calculated implicitly, and not the effective field.

One way to allow a completely implicit treatment of the magnetostatic field without a dense contribution was explored by d'Aquino \etal \cite{DAquino2005}, and is also used by \nmag \cite{??ds private communication? nmag paper?} ??ds and probabaly by femme and magpar but this needs checking.
They used a quasi-Newton method where the dense contribution was simply left out of the Jacobian matrix.
This greatly slows down the convergence of the non-linear solve: in the example given in the paper 10-14 iterations were needed (compared to $\sim2$ with a full Newton-Raphson method).
However, in the general case it is not clear that this quasi-Newton method will converge at all!

In \cref{sec:solution-strategies} we describe these issues in more detail and demonstrate a novel efficient and robust solver allowing fully implicit magnetostatic calculations.


\section{Conclusions}
\label{sec:num-meth-conclusions}

In \thisref{sec:numer-meth-micr} we have described a number of numerical methods for each component of a dynamic micromagnetic solver.
For the rest of this thesis we focus on the methods appropriate for modelling general geometries: the FEM for spatial discretisation, with implicit time integration methods to avoid issues with stiffness, and FEM/BEM magnetostatic calculations.
We use a Newton-Raphson linearisation method because of the faster convergence and the lack of time step limitations.
We aim to solve the resulting linear systems with preconditioned Krylov solvers for high computational efficiency.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
