\chapter{Numerical Methods for Dynamic Micromagnetic Modelling}
\label{sec:numer-meth-micr}

In this Chapter we give an overview of numerical methods that have been or could be used in dynamic micromagnetic calculations.
These calculations involve solving some form of the LLG equation \eqref{eq:LLG} with fields determined by energy derivatives as discussed in \autoref{sec:cont-micromag}.
These systems of partial differential equations (PDEs) can only be solved analytically in a few extremely simple cases,\footnote{For example the Stoner-Wolfarth theory for the rotation of a single grain.\cite{Stoner1948a}}\cite{Aharoni1996} so numerical solution methods are almost always required.

Many numerical methods for solving non-linear PDEs, such as \eqref{eq:LLG}, can be thought of as a combination of various component methods, each of which handles a different part of the conversion from a purely mathematical description into a complete algorithm which can be performed by a computer\footnote{Technically proofs of convergence etc. should be carried out for every different combination of time and space discretisation\cite[382]{Iserles2009}; in practice suprises seem to be rare, at least within micromagnetics.}.

The essential components of such a numerical method are:
\begin{itemize}
\item Spatial discretisation: convert space derivatives into algebraic relationships between points in space, \eg Finite elements, finite differences, macrospin models. 
\item Time discretisation (a.k.a. time integration):  convert time derivatives into algebraic relationships between points in time, \eg Runge-Kutta methods, backwards difference formulae (BDF), implicit midpoint rule (IMR).
\end{itemize}
For some choices of time and space discretisations additional component methods are needed:
\begin{itemize}
\item Linearisation: Convert a system of non-linear algebraic equations into a sequence of systems of linear algebraic equations, \eg Newton-Raphson method, Picard/fixed-point iteration. 
\item Linear solver: solve a system of linear algebraic equations, \eg LU decomposition, Krylov solvers, multigrid methods. 
\end{itemize}

Additionally in micromagnetic modelling the calculation of the magnetostatic field is required.
If the integral form given in \eqref{eq:Hmsint} is used then an additional integral evaluation method is needed to handle it.
Alternatively if the equivalent potential formulation \eqref{eq:Hms}, \eqref{eq:cont-phi-bound} is used then more PDEs are added to the system of PDEs describing the magnetisation dynamics.

It should be noted that not all methods fit into this framework of independent methods for each part of the problem.
In particular one example which uses a combined space/time discretisation to obtain some interesting properties is discussed (briefly) in \autoref{sec:advanced-lin}.
In general however, the modularity of both understanding and code given by thinking of a PDE solver as a combination of components is extremely powerful.


\section{Spatial Discretisation}
\label{sec:spat-discr}

\subsection{Macrospins}
\label{sec:sd-macrospins}

In a granular material (a material consisting of magnetic grains separated by a non-magnetic material) the simplest way to handle spatial derivatives in the problem is to assume that within each grain the exchange coupling is so strong that they are zero, \ie that each grain rotates as a single \emph{macrospin}.
We assign a single value of $\Mv$ to each macrospin and proceed to calculate energy, effective field and/or magnetisation of each macrospin as required.
One caveat is that the magnetostatic self field is not automatically accounted for since there is no modelling of intra-grain effects.
Hence it must be calculated and included separately to the magnetostatic interactions between grains.
When included in this way the magnetostatic self field is often called the \emph{shape anisotropy} since it is dependent on the shape of the grain and acts in a similar way to the magnetocrystalline anisotropy. 
The same approach may be used with any system in which there are a number of ``small'',\footnote{All dimensions of the bodies must be much smaller than the exchange length so that all magnetisation within the body is approximately parallel.} separate magnetic bodies with approximately uniform magnetisation inside the body.

The obvious downside of a macrospin approach is that it only applies to a fairly specific geometry, although the case of a granular media has been of much interest for magnetic data storage. 
Additionally, if there are non-uniformities in magnetisation within the regions where it has been assumed constant the model may be inaccurate.

However it is often simpler to construct a macrospin model than to use the methods described in \autoref{sec:sd-finite-diff-meth} and~\ref{sec:sd-finite-elem-meth}.
Also the assumption that each grain has uniform magnetisation will reduce the number of calculations needed.

Closely related to macrospin models are atomistic models\cite{Evans2014}.
In these models the assumption of a continuous magnetisation $\mv(\xv, t)$ is dropped in favor of an assumption that each atom is the location of a single macrospin.
As such they allow modelling of some systems, such as antiferromagnetic materials, that are inaccessible using micromagnetics.
However the required spatial resolution comes with a large increase in computational cost.


\subsection{Finite Difference Methods}
\label{sec:sd-finite-diff-meth}

Another method of spatial discretisation is the finite difference method: a single magnetisation vector is assigned to each point on (or ``cell'' in) a simple square/cubic grid which covers the entire domain.
Spatial derivatives are then approximated by expressions of the form
\begin{equation}
  \evalatb{\pd{f}{x}}{x'} \approx \frac{f(x' + h) - f(x')}{h},
\end{equation}
where $h$ is the distance between points in the grid.

The finite difference method works well for very simple geometries when the grid can be lined up with all geometric features.
For example when we are interested in how the magnetisation evolves over time in a non-granular cuboid-shaped thin film of magnetic material a finite difference method will be sufficient (\eg in the $\mu$mag standard problems\cite{mumag-website}).

However when the geometry involves curves, diagonals, hexagonal grains, bit patterned media or any other more complex geometric system other methods are better suited.


\subsection{Finite Element Methods}
\label{sec:sd-finite-elem-meth}

A more complex method of spatial discretisation is the finite element method \cite{HowardElmanDavidSilvester2006}.
Here the magnetic body is divided up into a finite number of polygonal \emph{elements}, which can vary in both size and shape.
Within each of these elements each magnetisation component is approximated by a simple polynomial function of space, typically a linear polynomial.
Spatial derivatives are calculated by simply differentiating these polynomial functions.
More details of the finite element method are given in \autoref{sec:intr-finite-ele-diff}.

The main advantage of the finite element method is that it can accurately approximate any geometrical feature by an appropriate arrangement of the polygonal elements.
An additional advantage is that the size of elements (and thus the accuracy of the approximation) can be varied arbitrarily as needed to give better accuracy in more complex or important regions. 
The choice of element size can be done automatically using \emph{adaptive mesh refinement}: after each calculation an error estimate is calculated (known as \emph{a posteriori} error estimation).
If the error is determined to be too high anywhere the mesh is refined near that region and the calculation is repeated.
Hence, given the desired error and a method to estimate the error, a mesh giving an efficient and accurate approximation can be automatically generated.\cite{Schrefl1999}

The major downside of finite element models is that the underlying mathematics is more complex than that required for other methods, especially finite difference methods.
Also the set up time and memory usage can be greater because of the additional ``bookkeeping'' required to keep track of the more complex meshes.


\section{Time Integration}
\label{sec:time-discretisation}

Time integration is for....

When discussing time integration schemes it is helpful to use a (vector) initial value ordinary differential equation:
\begin{equation}
  \begin{aligned}
    \frac{d \yv}{dt} &= \ffv{t, \yv}, \\
    \yv(0) &= \yv_0,
    \label{eq:45}
  \end{aligned}
\end{equation}
where $\fv(t,\yv)$ is a known function and $\yv_0$ is the (known) initial condition.
After the application of one of the spatial discretisations described above the PDE describing the Landau-Lifshitz-Gilbert equation is reduced to this problem, sometimes called a \emph{semi-discretised} PDE.

The time discretisation methods discussed here all bear a strong similarity to the finite difference method discussed in \autoref{sec:sd-finite-diff-meth}, except that the variable discretised is time instead of space.
This is appropriate since there is no analogue of geometric features in time, hence typically no need for the more advanced finite element method.

Some key attributes of a time discretisation scheme are: \cite{Atkinson2009}
\begin{itemize}

\item \textbf{Accuracy (order)} -- An estimate of how rapidly the approximate solution converges as the time step size is reduced.

\item \textbf{Stability} -- Roughly speaking a scheme is stable if the approximate solution does not ``blow up'' (\ie become catastophically inaccurate, infinite, oscillate in non-physical ways, \ldots) when time steps are larger than maximum stable step (see \autoref{sec:A-stability} for a more rigorous description).

\item \textbf{Preservation of geometrical properties} -- Some differential equations have properties which should ideally be conserved in the discretised system.
For example the properties of the Landau-Lifshitz-Gilbert equation discussed in \autoref{sec:prop-cont-llg}.

\item \textbf{Self-starting} -- A scheme is self starting if it only requires a single initial value.
This is desirable because the methods of estimating additional initial values make the final scheme more complex and may introduce additional errors.
\end{itemize}

These attributes are discussed more rigorously in the rest of this Section.

\subsection{Explicit and implicit time integration schemes}
\label{sec:explicit-vs-implicit-schemes}

Explicit time discretisation schemes give the value at some future time in terms of the value at the present time and/or previous times.
The simplest such scheme is the (forward) Euler method
\begin{equation}
  \label{eq:44}
  \yv_{n+1} = \yv_n + \dtn f(t_n, \yv_n),
\end{equation}
where $\dtn$ is the $n$-th time-step. 
Clearly, given $f(t,y)$ and an initial value for $y(t_0)$ we can solve for $y(t_n)$ for any $n$. However the stability and convergence behaviour of this simple scheme is less than impressive. Typically more advanced explicit schemes are used which give increased stability \cite{Atkinson2009}.
Also the so called ``CFL condition'' (Courant-Freidrich-Lewy condition) can force the use of lower time-steps in explicit solvers if a finite element/difference spatial discretisation with small elements/cells is used.

Implicit time discretisation schemes allow much longer time-steps to be used without loss of stability.
However an implicit scheme gives the value at the next time-step in terms of current/previous times and in terms of the \emph{value at the next time-step}.
Hence at each step a (linear or non-linear) system of equations must be solved, however the increase in maximum time-step size offsets this increase in calculation time per step in many cases.

The backwards differences schemes are a simple and commonly used example. The first order formula is
\begin{equation}
  \label{eq:bdf1}
  \yv_{n+1} = \yv_n + \dtn \ffv{t_{n+1}, \yv_{n+1}}.
\end{equation}

Micromagnetics solvers for non-stiff systems commonly use the RK4 (fourth order Runge-Katta) method \cite{Suess2002}.
Micromagnetics models commonly use BDF schemes of various order \cite{Suess2002} or the implicit midpoint rule \cite{DAquino2005} for the modelling of stiff systems.


??ds move?
Other micromagnetic models use a combination of implicit and explicit schemes: everything except for the magnetostatic field is discretised as normal using an implicit scheme, the magnetostatic field is updated (using an explicit calculation) after each time step. 
This method gains a larger time-step from the implicit method but the system of equations to be solved remains sparse.
However when using this method the determination (see \autoref{sec:adaptivity}) of the time-step must be done specially (\ie not using normal adaptivity) since the system of equations contains no information on the magnetostatic field \cite{Schrefl1997}.


\subsection{Some implicit time integration schemes}
\label{sec:some-implicit-time-integrators}

For the rest of this section we will use as examples three time integration schemes with good stability and accuracy properties that are widely employed in practice.
All three are implicit schemes, for reasons of stability as explained in \autoref{sec:explicit-vs-implicit-schemes}.

Trapezoid rule (TR) is the average of the forward and backward Euler methods from \autoref{sec:explicit-vs-implicit-schemes}:
\begin{equation}
  \yv_{n+1} = \dtn (\ffv{t_{n+1}, \yv_{n+1}} + \ffv{t_n, \yv_n})/2.
\end{equation}

The second order backwards difference formula (BDF2) is:
\begin{equation}
  \frac{\yv_{n+1} - \yv_n}{\dtn} = \frac{\dtn}{2\dtn + \dtx{n-1}} \frac{\yv_n - \yv_{n-1}}{\dtx{n-1}} 
  + \frac{\dtn + \dtx{n-1}}{2\dtn + \dtx{n-1}} \fv(t_{n+1}, \yv_{n+1}).
\end{equation}

The method that this thesis focuses on (for reasons that will become clear through the rest of this section) is the implicit midpoint rule (IMR).
It is given by
\begin{equation}
  \label{eq:imr-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}}.
\end{equation}
Note that for ODEs such that $\fv$ is linear in both $t$ and $\yv$ this is equvalent to TR, and in general the properties of the two methods are similar.

IMR is trivial to implement in any existing implicit solver: take a BDF1 step as usual then update via
\begin{equation}
  \begin{aligned}
    t_{n+1} &\rightarrow t_{n+1} + \dtn, \\
    \mv_{n+1} &\rightarrow 2\mv_{n+1} - \mv_n
  \end{aligned}
\end{equation}
This is exactly equivalent to IMR with a step size of $2\dtn$ \cite{Malidi2005}.

TR and IMR are both self starting.
BDF2 requires an additional startup value.
This can be generated, for example, by taking a step of IMR, TR or BDF1 before beginning the use of BDF2. 



\subsection{Local truncation error and order}
\label{sec:deriv-local-trunc}

The local truncation error (LTE) of a time integration scheme is the error due to a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$, $\yv_{n-1} = \yv(t_{n-1})$, etc. into the approximation for the next time-step then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$. 
\ie
\begin{equation}
  \label{eq:60}
  \lte = \yv(t_{n+1}) - \hat{\yv}_{n+1},
\end{equation}
where the history values ($\yv_{n}, \yv_{n-1}, \ldots$) used to calculate $\hat{\yv}_{n+1}$ are exact.
For example the local truncation error of IMR is
\begin{equation}
  \lte^\imr =  \yv(t_{n+1}) - \yv(t_n) - \dtn \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}^\imr}{2}}.
  \label{eq:trunc-start}
\end{equation}

If the local truncation error of a time integration scheme is such that 
\begin{equation}
  \lte \leq c \dtn^p
\end{equation}
then we say that it is of order $p$.


It is important to distinguish the local truncation error from the global (temporal) error
\begin{equation}
  \label{eq:global-temporal-error}
  \begin{aligned}
    E_n &= \yv(t_{n+1}) - \yv_{n+1}, \\
    E_n^\imr &=  \yv(t_{n+1}) - \yv_n^\imr - \dtn \ffv{ \thf, \frac{\yv_n^\imr + \yv_{n+1}^\imr}{2} }.
  \end{aligned}
\end{equation}
The difference is that the global error includes any error accumulated over previous time steps (the exact values $\yv(t_n)$ in \eqref{eq:60} are replaced by their approximations).

We now give a derivation of the local truncation error of IMR. 
The derivations for TR and BDF2 are much simpler and can be found in most text books on the subject.
We first Taylor expand $\yv(t_{n+1})$ and $\yv(t_{n})$ about $\thf$.
We use the midpoint rather than one of the end points (as typically used in these calculations) because it results in simpler calculations.
Assuming that $\yv(t)$ is ``sufficiently smooth'' for the required derivatives to exist its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf['] 
  + \frac{\dtn^2}{8} \yvhf['']
  + \frac{\dtn^3}{48} \yvhf[''']
  \porder{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
Similarly the expansion at $t_n$ about $\thf$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf['] 
  + \frac{\dtn^2}{8} \yvhf[''] 
  - \frac{\dtn^3}{48} \yvhf['''] 
  \porder{\dtn^4}.
  \label{eq:taylorn}
\end{equation}

Substituting equations~\eqref{eq:taylornp1} and \eqref{eq:taylorn} into equation~\eqref{eq:trunc-start} gives\footnote{If we had chosen to Taylor expand about $t_n$ there would be an additional term in $\yv_n''$ here.}
\begin{equation}
  \lte^\imr = \underbrace{\frac{\dtn^3}{24} \yvhf[''']}_{\text{I}}
  + \underbrace{\dtn\bigs{ \yvhf['] - \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}}{2}} }}_{\text{II}}
  \porder{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to this error: the first term (I) is fairly standard in second order time integrators.
The second term (II) originates from the use of an approximation to $\yv(\thf)$ in the evaluation of $\fv$ (\ie the Runge-Kutta nature of IMR).
The rest of the derivation requires applying Taylor expansions to II and is carried out in \autoref{sec:full-imr-lte-calculation}.
The final result shows that IMR is second order:
\begin{equation}
  \lte^\imr = \frac{\dtn^3}{24} \left[\yvhf['''] - 3 \dfdyhf \cdot \yvhf[''] \right]
  \porder{\dtn^4},
  \label{eq:trunc-final}
\end{equation}
where $\dfdyhf = \pd{\fv}{\yv}\evalat{t=\thf}$ is the Jacobian of $\fv$ with respect to $\yv$ evaluated at $t=\thf$.
An additional condition required in the derivation is that ??ds the eigenvalues of $\dtn\dfdyhf$ are less than one, the implications of this are discussed in \autoref{sec:order-reduction}.

The TR\cite[261]{GreshoSani} and BDF2\cite[715]{GreshoSani} methods are also second order.
Their local truncation errors are similar to term I of \eqref{eq:trunc-mid}:
\begin{equation}
  \label{eq:tr-lte}
  \lte^\tr = \yv_{n+1} - \yv(t_{n+1}) = -\frac{\dtn^3 \yv_n'''}{12}
  + \order{\dtn^4},
\end{equation}

\begin{equation}
  \label{eq:bdf2-lte}
  \lte^\bdf = \yv_{n+1} - \yv(t_{n+1}) = \frac{(\dtn + \dtx{n-1})^2}{\dtn(2\dtn + \dtx{n-1})}
  \frac{\dtn^3 \yv_n'''}{6}
  + \order{\dtn^4}.
\end{equation}


\subsection{A-stability}
\label{sec:A-stability}

To discuss the stability of time integrators the following test equation is widely used
\begin{equation}
  \ffv{\yv} = \lambda \yv,
  \label{eq:ode-test-f}
\end{equation}
\ie
\begin{equation}
  \yv(t) = \exp(\lambda t).
\end{equation}

A-stablility\footnote{The A does not stand for anything, it is just ``A''\cite[40]{HairerWanner}, in particular it does \emph{not} mean ``absolute stability''.} is the property that a method is has no stability restrictions on the time step size when used to integrate \eqref{eq:ode-test-f} for all $\lambda$ with $\realp(\lambda) \leq 0$.
When $\realp(\lambda) > 0$ the ODE itself is unstable and so stability is not expected in general.
A-stability is a good way to classify the suitability of a time integrator for solving ``stiff'' ODEs.
Such ODEs often emerge from the semi-discretisation of a PDE.

The IMR, TR, BDF1 and BDF2 methods are all A-stable \cite[pgs. 43, 251]{HairerWanner}.
Linear explicit methods are never A-stable \cite{Nevanlinna1974} (``linear'' methods in this reference include all common time integration methods: Runge-Kutta, multistep, predictor-corrector, \ldots)\footnote{These limitations could theoretically be circumvented by using significantly more complex methods, but such questions are far beyond the scope of this thesis.}, which is the main reason for our focus on implicit methods.
Additionally there are no A-stable linear multistep methods\footnote{A-stable implicit Runge-Kutta methods of higher order do exist, see \eg \cite[73]{HairerWanner}.
However moving to higher order methods significantly increases the size of the system to be solved at each step. Hence they are prohibatively expensive for the time integration of semi-discretised PDEs.
Also second order is in some sense ``good enough'' for LLG integration since the spatial accuracy is only second order and contains higher order derivatives than in time.
??ds talk to Milan/Matthias about this: it seems that with good preconditioning we can have \eg a 4th order method (Gauss order 4, higher order version of IMR) for only ~4x the cost per step, a gain of 25x in (computation time)/accuracy with steps of ~0.1} of order greater than 2 \cite[261]{GreshoSani}, which is why we have chosen the three widely used second order methods for discussion.

\subsection{Spurious numerical damping}
\label{sec:numerical-damping}

Some time integration methods create additional, non-physical damping in approximate solutions, even when none exists in the exact solution.
This is problematic for the modelling of highly oscillatory ODEs, such as the LLG with low damping.

We say that a time integration method causes spurious damping if when solving the test ODE \eqref{eq:ode-test-f} with $\lambda = i\omega$ the magnitude of $y$ decreases over time.

For example, for the implicit midpoint rule:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega (y_n + y_{n+1})/2, \\
    &= \frac{1 + i\dtn \omega/2}{1 - i\dtn \omega/2} y_n,
  \end{aligned}
\end{equation}
and so
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}}^2 &=  \frac{\abs{1 + i\dtn \omega/2}^2}{\abs{1 - i\dtn \omega/2}^2} \abs{y_n}^2, \\
    \abs{y_{n+1}}^2 &=  \frac{(1 + i\dtn \omega/2)(1 - i\dtn \omega/2)}
    {(1 - i\dtn \omega/2)(1 + i\dtn \omega/2)} \abs{y_n}^2, \\
    &=  \abs{y_n}^2.
  \end{aligned}
\end{equation}
So IMR does not cause spurious damping.

Since $f$ is linear in $y$ for this example ODE TR and IMR are identical
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn(i\omega y_n + i\omega y_{n+1})/2, \\
    &= y_n + \dtn i \omega (y_n + y_{n+1})/2.
  \end{aligned} 
\end{equation}
Hence the same property applies for TR.

For BDF1
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega y_{n+1}, \\
    &= \frac{1}{1 - i \dtn \omega} y_n.
  \end{aligned}
\end{equation}
Hence
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}} &= \sqrt{ \frac{1}{(1 - i \dtn \omega)(1 + i \dtn \omega)}} \abs{y_n}, \\
    &= \frac{1}{\sqrt{1 + \dtn^2 \omega^2}} \abs{y_n}, \\
  \end{aligned}
\end{equation}
and so the magnitude of $y$ decreases over time--the solution is damped.
The analysis for the second order BDF2 case is more complex due to the fact that it is a multistep method, but the result is the same: the solution suffers from non-physical damping \cite[265]{GreshoSani}.


\subsection{B-convergence and order reduction}
\label{sec:order-reduction}

It is known that certain implicit Runge-Kutta methods are susceptible to reduced accuracy when used to solve certain extremely stiff problems \cite[156]{Atkinson1994}\cite[225]{HairerWanner}.
In the case of the implicit midpoint rule this corresponds to the case when the error due to term II of \eqref{eq:trunc-mid} is large. This occurs when $\pd{\fv}{\yv}\evalat{\thf{}}$ is large, \ie when the error in $\ffv{t, \yv}$ due to a small error in $\yv$ is large.

The concept of ``B-convergence'' is used to analyse this effect.
Roughly speaking a Runge-Kutta method is B-convergent of order $r$ if the global error is  $\order{\dtx{\text{max}}^r}$ for all sufficiently smooth ODEs without any assumption about the size of $\pd{\fv}{\yv}$.
The implicit midpoint rule is B-convergent of order 1 \cite[231]{HairerWanner}\footnote{In this reference IMR is refered to as the second order Gauss method, BDF1 is Radau IIA, TR is Lobatto IIIA \cite[72-76]{HairerWanner}.}, one order less than for typical ODEs.
In contrast, the TR and BDF methods do not suffer from order reduction \cite[159]{Atkinson1994}.
This is because all evaluations of $\fv$ are at integer time points (\ie $t_{n+1}, t_{n}, t_{n-1}, \ldots$ rather than $\thf$).

A simple test ODE gives a useful example of this phenomenon \cite[157]{Atkinson2009}
\begin{equation}
  \label{eqn:imr-test-order-reduction}
  \begin{aligned}
    f(t, y) &= -\lambda (y - g(t)) + g'(t), \\
    y(t) &= g(t), \\
  \end{aligned}
\end{equation}
for some function $g(t)$ and parameter $\lambda \geq 0$.
Note that $\pd{f}{y} = \lambda$, so we can directly control the second term of the IMR truncation error.

We now derive the LTE for IMR in this example when $\abs{\lambda\dtn} >> 1$.
From \autoref{sec:full-imr-lte-calculation}, \eqref{eq:trunc-implicit-form} we have that
\begin{equation}
  \begin{aligned}
    (1 + \frac{\dtn \lambda}{2})\lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} \porder{\dtn^4}, \\ 
  \end{aligned}
\end{equation}
Then using $\abs{\lambda\dtn} >> 1$:
\begin{equation}
  \begin{aligned}
    \frac{\dtn \lambda}{2} \lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} \porder{\dtn^4}, \\ 
    \lte^\imr &= \frac{\dtn^2}{12\lambda} \left[g'''(\thf) - 3 \lambda g''(\thf) \right] \porder{\dtn^4}.
  \end{aligned}
\end{equation}
So we can see that the order has been reduced by a factor of $1/\dtn$.
If addtionally $\abs{\lambda\dtn} >> \abs{g'''(t)}$, which will typically be true for very large $\lambda$ and $g \neq g(\lambda)$:
\begin{equation}
  \lte^\imr = \frac{-\dtn^2}{4} g''(\thf).
  \label{eq:reduced-order-imr-truncation-error}
\end{equation}


\subsection{Adaptivity}
\label{sec:adaptivity}

Adaptive time integration methods automatically select time step sizes in response to an estimate of the local truncation error.
This can improve efficiency depending on the equation being solved, for example on problems where a high frequency part of the solution is gradually damped out the time step can increase by orders of magnitude at later times without loss of accuracy.
Adaptive integration also greatly simplifies the use of the simulation program--the user only has to choose the allowed error and the program will handle the rest.

Most local truncation error estimators for implicit integrators (e.g. TR, BDF2) use a Milne-device based method \cite[707-716]{GreshoSani}.
This means that they compute an explicit estimate $\yv^E_{n+1} \sim \yv(t_{n+1})$  (sometimes known as a predictor step) to the same order of accuracy as the implicit method and using the same input values and derivatives.
They then use algebraic rearrangements of the local truncation error expressions of the predictor step and the implicit step to give an approximation of the true local truncation error.

No such estimation is possible for IMR because of the specific form of the local truncation error.
The details of this issue and the first (to the author's knowledge) algorithm for adaptive time integration with IMR are described in \autoref{sec:adaptive-imr}.

It is also possible to create less general adaptivity methods based on error estimates specific to the equation being solved.
For example in cases where the damping is not exactly correct the effective damping can be used as an error estimator, as proposed by Albuquerque \etal \cite{Albuquerque2001}.
The disadvantage of this method (aside from the obvious requirement that the damping be inexact) is that, depending on the discretisation used, the error estimator may be unable to distinguish between insufficient space refinement and insufficient time refinement.
In particular this is the case when the solution is not completely divided into separate effective field calculations and pointwise time integration, such as in a typical Galerkin finite element model.

Additionally time error estimates for the LLG with limited effective field terms and a specific finite element discretisation were proven and used to perform adaptive time integration with both IMR and BDF2 by Banas \cite{Banas-thesis}.
??ds do these work, why does no one use them?
These estimates have the obvious limitation that they are only (directly) applicable to the case for which they were derived.
It is also a fairly complex derivation (at least for non-numerical analysts), and difficult to test because of its limited applicability.


\subsection{Magnetisation length conservation}
\label{sec:ensuring-constant-mv}

As discussed in \autoref{sec:prop-cont-llg} the length of the magnetisation at a point does not change over time.
However in the approximations given by numerical methods this is often lost, and must be enforced separately.

A simple method of dealing with the constraint is to re-normalise $\Mv$ after some number of time steps or when the error in $|\Mv|$ exceeds some tolerance \cite{Fidler2000}.
However this approach fundamentally changes the system of equations being solved in a non-linear and unpredictable manner \cite{Lewis2003}.

??Ds rewrite:
If we have a system with only a single (macro)spin (\ie a single value of $\Mv$ represents the magnetisation of the entire system) it is easy to avoid this problem by using a spherical polar coordinate system $(r,\theta,\phi)$.
Equation~\eqref{eq:LLG} can be expressed in terms of only the angles $(\theta,\phi)$ representing the direction of $\Mv$ and the non-convex constraint is automatically enforced since
\begin{equation}
  \label{eq:40}
  \pd{\abs{\Mv}}{t} = \pd{r}{t} \equiv 0.
\end{equation}
However to extend this to systems where $\Mv$ varies with space we have to use a separate spherical polar coordinate system at each point where $\dMdt$ is calculated.
Also a Cartesian global coordinate system is still needed to calculate the interactions between the discretised points (\ie magnetostatics, exchange coupling).
Hence we have to convert back and forth between coordinate systems during the simulation.\cite{Scholz2003} 
Finally problems can occur with this approach as the polar angle, $\theta$, approaches zero because $\pd{\Mv}{t} \propto \frac{1}{\sin(\theta)}$ \cite{Fukushima2005}.

Geometric integration schemes aim to solve this problem by constructing a time integration scheme that naturally preserves the value of $|M_s|$.
Some examples of such schemes are based on IMR \cite{DAquino2005}, or a semi-explicit extension of IMR using explicit extrapolations of the effective field \cite{Spargo2003} \cite{Serpico2001}.
Alternatively geometric integration methods based on Cayley transforms can be used \cite{Lewis2003} \cite{Bottauscio2011}.


\subsubsection{Proof of magnetisation length conservation for IMR on the ODE LLG}
\label{sec:proof-magn-length-ode-imr-llg}

In this section we show that the discrete form of the Landau-Lifshitz-Gilbert equation that results from the use of the implicit midpoint rule maintains the magnetisation length conservation property of the continuous form shown in \autoref{sec:prop-cont-llg}.
These results are applicable to cases where, after any spatial discretisation, each spatial point is time evolved by simply integrating the LLG with the values for that point, \ie finite difference and macrospin discretisations (but not finite elements).
For the purposes of this section we assume that the linearisation method is exact, \ie the discretised but non-linear Landau-Lifshitz-Gilbert equation is satisfied exactly.

As can be seen from \eqref{eq:imr-definition} the IMR is defined by the following substitutions:
\begin{align}
  \label{eq:55}
  \pd{\mv}{t} &\rightarrow \frac{\mv_{n+1} - \mv_n}{\dtn}, \\
  \mv &\rightarrow \frac{\mv_{n+1} + \mv_n}{2}, \\
  t &\rightarrow \frac{t_{n+1} + t_n}{2}.
\end{align}
So the IMR-discretised LLG is
\begin{equation}
  \frac{\mv_{n+1} - \mv_n}{\dtn} = - \frac{\mv_{n+1} + \mv_n}{2} \times
  \left(
    \hv \left[\frac{\mv_{n+1} + \mv_n}{2} \right]
    + \happ\left(\frac{t_{n+1} + t_n}{2}\right)
    - \dampc \frac{\mv_{n+1} - \mv_n}{\dtn}
  \right).
  \label{eqn:disc-llg}
\end{equation}

Intuitively the conservation properties of the IMR discretisation can be seen to come from the cancellation of cross terms in $\ip{\lop[\mv]}{\dmdt}$ for any symmetrical linear operator $\lop$.
More precisely:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-linop}
    \ip{\lop \left[ \frac{\mv_{n+1} + \mv_n}{2} \right]}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}} + \ip{\mv_{n+1}}{\lop \mv_{n}} \\
    & \qquad\qquad - \ip{\mv_{n}}{\lop \mv_{n+1}} - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big] \\
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}}
    - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big].
  \end{aligned}
\end{equation}
This means that any orthogonality relationships between $\lop \mv$ and $\dmdt$ in the continuous case carry over to the discrete case. ??ds more detail on what this means

We can now examine the change in magnetisation length using the same technique as in \autoref{sec:prop-cont-llg}: take the dot product with $\frac{\mv_{n+1} + \mv_n}{2}$ on both sides of~\eqref{eqn:disc-llg} and use the triple product identity \eqref{eq:dot-cross-id} to get
\begin{equation}
  \label{eq:63}
  \frac{\mv_{n+1} + \mv_n}{2} \cdot \frac{\mv_{n+1} - \mv_n}{\dtn} = 0.
\end{equation}
Now using~\eqref{eqn:imr-linop} with $\lop$ as the identity operator gives us
\begin{equation}
  \frac{\ip{\mv_{n+1}}{\mv_{n+1}} - \ip{\mv_n}{\mv_n} }{2 \dtn} = 0.
\end{equation}
Therefore
\begin{equation}
  \abs{\mv_{n+1}} = \abs{\mv_n},
\end{equation}
\ie the magnetisation length does not change between time steps.


Note that for other time integration schemes the property \eqref{eqn:imr-linop} typically does not hold.
For example in the case of BDF1 we have
\begin{equation}
  \begin{aligned}
    \ip{\lop \bigs{\mv_{n+1}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}} 
      - \ip{\mv_{n+1}}{\lop \mv_n} }, \\
    &\neq \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_n}{\lop \mv_n} },
  \end{aligned}
\end{equation}
so orthogonality properties from the continuous form do not carry over in general.

Also note that the derivation of \eqref{eq:63} only requires that the midpoint representation of magnetisation be used for terms directly in the llg, it does not require anything about the effective field calcuation.
This explains why the semi-explicit modification to IMR also conserves the magnetisation length. 


\subsection{Energy conservation/decay}
\label{sec:energy-cons}

Another geometrical property of the Landau-Lifshitz-Gilbert equation, also discussed in \autoref{sec:prop-cont-llg}, is the conservation or controlled decay of energy depending on the value of the damping constant $\dampc$.

IMR also conserves energy when the damping term is zero and it ensures that the energy is a decreasing function of time when the damping is non-zero \cite{DAquino2005}.
The other geometric integration schemes mentioned in \autoref{sec:ensuring-constant-mv}, in particular the semi-explicit modifications of IMR, do not have this property.


\subsubsection{Proof of energy property for IMR on the ODE LLG}
\label{sec:proof-energy-prop}

Before we can derive energy decay properties of the discretised LLG we need to look at the properties of the effective field when considered as an operator on $\mv$:
\begin{equation}
  \label{eq:hop}
  \hv(\xv, t)[\mv(\xv)] = \hop [\mv(\xv)] + \happ(\xv, t).
\end{equation}
It can be shown (see \autoref{sec:linear-symm-field-operators}) that $\hop[\mv]$ is a linear symmetric operator on $\mv$ provided that there is no surface anisotropy and that the magnetocrystalline anisotropy is uniaxial (extensions may be possible.. not really checked).
The applied field is not an operator at all so we treat it separately. The energy can be written using this notation as
\begin{equation}
  \label{eq:energy-hop}
  \e_n = \ehop_{,n} + \eapp_{,n} = - \ip{\mv_n}{\frac{\hop[\mv_n]}{2}} - \ip{\mv_n}{\happ(t_n)},
\end{equation}
see \autoref{sec:energy-field-relation} for details.
Also note that we can expand $\mphapp$ into a midpoint-like form
\begin{equation}
  \mphapp = \frac{\happ(t_{n+1}) + \happ(t_n)}{2} 
  + \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4}.
  \label{eq:happ-midpoint}
\end{equation}
We write the error term as
\newcommand{\happerror}{\mathcal{E}_\text{ap}}
\begin{equation}
  \happerror = \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4}.
\end{equation}

In the same manner as the previous section we derive the change in energy of the discrete LLG by taking the $\ltwo$ inner product of equation~\eqref{eqn:disc-llg} with
\begin{equation}
  \mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror - \dampc \mpdmdt,
\end{equation}
resulting in
\begin{equation}
  \begin{aligned}
    &\ltip{\mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror}{\mpdmdt} \\
    & \quad - \dampc \ltip{\mpdmdt}{\mpdmdt} = 0.
    \label{eq:54}
  \end{aligned}
\end{equation}
The $\hop$ term in \eqref{eq:54} can be simplified by using the orthogonality identity~\eqref{eqn:imr-linop} and then written as an energy using \eqref{eq:energy-hop}
\begin{equation}
  \begin{aligned}
    \ltip{\mphop}{\mpdmdt} 
    &= \frac{1}{2\dtn} \Big[\ip{\mv_{n+1}}{\hop \left[\mv_{n+1} \right]}
    - \ip{\mv_{n}}{ \hop\left[ \mv_{n} \right]} \Big], \\
    &= -\frac{\ehop_{,n+1} - \ehop_{,n}}{\dtn}.
  \end{aligned}
  \label{eq:50}
\end{equation}
Next we examine the applied field term.
Dropping the $L^2$ from the inner products and temporarily writing $\happ(t_i)$ as $\hv_i$ for brevity we have
\begin{equation}
  \begin{aligned}
    \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}
    &= \ip{\hv_{n+1} + \hv_n}{\mpdmdt} \\
    & \qquad - \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}, \\
    &= -\frac{\eapp_{, n+1} - \eapp_{, n}}{\dtn}  + A,
  \end{aligned}
\end{equation}
where
\begin{equation}
  \begin{aligned}
    A &= \frac{1}{2\dtn}\Big[ 2\ip{\hv_n}{\mv_{n+1}} - 2\ip{\hv_{n+1}}{\mv_n} \\
    & \qquad - \ip{\hv_{n+1}}{\mv_{n+1}} +\ip{\hv_{n+1}}{\mv_n} 
    - \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= \frac{1}{2\dtn}\Big[ - \ip{\hv_{n+1}}{\mv_{n+1}} - \ip{\hv_{n+1}}{\mv_n} 
    + \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= - \ip{ \frac{\hv_{n+1} - \hv_n}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }, \\
    &= - \ltip{ \frac{\happ(t_{n+1}) - \happ(t_n)}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }.
    \label{eq:52}
  \end{aligned}
\end{equation}
Finally we insert the results of equations~\eqref{eq:50} and \eqref{eq:52} into \eqref{eq:54} to find
\begin{equation}
  \frac{\e_{n+1} - \e_n}{\dtn}
  = -\dampc \ltnorm{\mpdmdt}^2
  - \ltip{\frac{\happ(t_{n+1}) -\happ(t_n)}{\dtn}}{\frac{\mv_{n+1} + \mv_{n}}{2}}
  + \happerror.
  \label{eqn:imr-llg-energy}
\end{equation}
If the applied field is linear (and so $\happerror = 0$) this is exactly the midpoint discretisation of the continuous energy balance equation~\eqref{eq:energy-decay}.
In particular the energy is conserved when $\dampc = 0$ and the applied field is constant.

Note that when $\happ$ is piecewise linear, for example going from zero field to a constant field at a certain point in time, time steps can be trivially selected such that each non-linear change happens exactly at the start/end of a step.
As long as the time between such changes in field is comparable to the time scale of the dynamics this will have no effect on the efficiency.

Also note that none of the above is applicable to the semi-explicit modification of IMR since in that case $\mphop$ is replaced by something of the form $a\hop \bigs{\mv_n} + b\hop \bigs{\mv_{n-1}}$.

??ds draw commutation diagram

\subsection{Symplecity}

??ds learn about, experiment with and disccuss symplecity?

The fixed step midpoint method is ``almost symplectic'' for the LLG equation with zero damping, , \ie the property equivalent to Hamiltonian flow for  dissipative systems is conserved up to $\order{??ds}$ \cite{DAquino2005} \cite{Austin1993}.

It is well known that most adaptive schemes are not symplectic\cite[91]{Iserles2009} because they constantly change the ``nearby Hamiltonian'' that is followed by the symplectic integrator.
Therefore the adaptive IMR is not expected to have the almost-symplectic property discussed in \autoref{??ds}.


\subsection{Conclusions}

In the previous sections we have highlighted a number of interesting properties of the implicit midpoint rule.
We have a number of reasons to believe that the geometric properties will translate into an improvement in the accuracy and robustness of the overall solver:
\begin{itemize}
\item Errors in the energy dissipation \cite{Albuquerque2001} and magnetisation length \cite{Chantrell2001} have been successfully used as error estimators for total error.
\item It is well known that geometric integration schemes typically result in much smaller long-timescale error-build-up than schemes that do not preserve such quantities \cite[77]{Iserles2009}.
\item The non-linear modification to the Landau-Lifshitz-Gilbert equation caused by renormalisation of the magnetisation length (as commonly used to maintain correct magnetisation length in non-conservative time integrators) may cause significant changes in the magnetostatic field \cite{Lewis2003}.
\item Renormalisation of the magnetisation modifies the balance between the various energy terms.
  This is similar to the methods that lead to the ``flying ice cube'' problem \cite{Harvey1998} in molecular dynamics.\footnote{In such molecular dynamics the rescaling of particle velocities (to maintain constant temperature despite numerical error accumulation) can result in large amounts of kinetic energy being transferred from the motion of internal degrees of freedom to motion of the centre of mass.}
\end{itemize}

Additionally the same geometric properties enable convergence proofs of IMR when applied to FEM with a slight modification that will be discussed in \autoref{sec:nodal-integration} \cite{Bartels2006}.



\section{Linearisation}
\label{sec:linearisation}

When implicit time integration methods are used (on a non-linear differential equation such as the LLG) a non-linear system of equations must be solved.
Explicit time integration methods essentially sidestep the non-linearity of the LLG equation by avoiding the need to solve any kind of system.

Such a problem can be stated as:
Given a non-linear operator $F$ and a list of previous magnetisation values $\mv_{\text{history}}$ (as needed for the time integration method) find
\begin{equation}
  \label{eq:non-lin-system}
  \mv_{n+1} = \min_{\mv'} \norm{\rv(\mv')}.
\end{equation}


\subsection{Functional iteration}
\label{sec:picard}

??ds fill in

Cheap iterations

Convergence proof via Banach fixed point theorem for sufficiently small time steps.

Useful for non-stiff problems (convergence requirements similar to stiffness requirements) \cite{Iserles2009}.

In practice people often terminate the Picard iteration after only one or two steps, leading to a predictor-corrector method. 
These methods have similar properties to explicit methods: cheap per step but vunerable to stability problems.

Seem to be lots of variations of this...

\subsection{Newton-Raphson method}
\label{sec:newt-raph}

The motivation for the Newton-Raphson method comes from a simple Taylor expansion of the residual.
If the root of the residual is $\nextm$ and we have an initial guess for this root $\nowm$ then we can obtain a correction to this initial guess from:
\begin{equation}
  \begin{aligned}
    \resi(\nextm) &= 0, \\
    &= \resi(\nowm + \corr), \\
    &= \resi(\nowm) + \jac(\nowm) \cdot \corr + \order{\corr^2},
  \end{aligned}
\end{equation}
where $\jac(\nowm) = \evalatb{\pd{\resi}{\mv}}{\nowm}$ is the Jacobian matrix for the residual.
So
\begin{equation}
  \begin{aligned}
    \label{eq:49}
    \nextm &= \nowm + \corr, \\
    &= \nowm + \jac^{-1}(\nowm) \cdot \resi(\nowm) + \order{\corr^2}.
  \end{aligned}
\end{equation}

This suggests the iteration
\begin{equation}
  \mv^{i+1} = \mv^i + \jac^{-1}(\mv^i) \cdot \resi(\mv^i),
\end{equation}
in which the error is \emph{squared} after each iteration, assuming that the Taylor series expansion is valid and that we can discard the higher order terms.
The iteration is terminated when
\begin{equation}
  \norm{\resi(\mv^{i+1})}_{\infty} < \ntol,
\end{equation}
for some user defined tolerance $\ntol$.

In time stepping problems the value at the previous time step provides a very good initial guess: $\nowm = \mv_n$.
For micromagnetics problems we find that this reliably converges to a tolerence of $\sim 10^{-10}$ in around 2-3 steps!

One downside of this method is that it requires the assembly and solution of a system of linear equations to obtain each correction $\corr$.
This is the subject of \autoref{sec:solution-lin-sys}.
Another, more theoretical downside is that the iteration is not guaranteed to converge, especially if the initial guess is not very good.
However for non-linear systems resulting from time stepping we always have a very good initial guess, so this does not seem to be a real issue.


\subsection{Advanced LLG-specific methods}
\label{sec:advanced-lin}

??ds clean up or remove?

Linearisation based on choice of test functions of much interest in recent times, initially for only exchange effective field\cite{Alouges2008}.
Has recently been extended to handle general effective field contributions\cite{Banas2012}.

Advantages:
\begin{itemize}
\item Only need to solve one linear system per step (rather than $\sim 2$ for Newton's method).
\item Jacobian matrix is constant.
\end{itemize}

Disadvandages:
\begin{itemize}
\item Method is limited to first order: so need step sizes orders of magnitude smaller.
\item Have to use specialised time integration scheme (no geometric, adaptive, ... schemes).
\item Specific to the LLG equation--standard code libraries may not work, not as widely understood as standard FEM, extension to related equations (stochastic LLG, LLGS, LLB) is non-trivial.
\end{itemize}

The limitation to first order has been worked around recently, however the higher order schemes lose either stability or linearity \cite{Kritsikis2014}.
Since the advantage of the scheme over standard methods was this unique combination of properties, this appears (to me at least) to not be very useful.

Still a promising line of research though!



\section{Solution of linear systems}
\label{sec:solution-lin-sys}

If we are using an implicit time integration scheme with the Newton-Raphson method for linearisation then we are left with the problem of solving a sequence of sparse linear systems.
The problem can be stated as: Given a sparse $n \times n $ matrix $\Am$ and a vector $\bv$ find $\xv$ such that
\begin{equation}
  \label{eq:linear-system}
  \Am \xv = \bv.
\end{equation}
This is a very well studied problem, see for example\cite{Saad2000}, but efficent techniques for very large $n$ (which corresponds to a large spatial problem and/or good spatial resolution) are complex and problem dependent.


\subsection{Direct methods}
\label{sec:direct-methods}

Construct exact solutions to linear systems of equations

Main example is LU decomposition.

The main advantage of direct methods is their robustness: given any non-singular linear system a well designed direct solver will be able provide an answer.
As such direct solvers are usually the first approach tried when solving a new system of equations.

However as matrix sizes become large direct solvers become increasingly demanding in both time and memory.
The problem is that the inverse of a sparse matrix is significantly more dense than the starting matrix.
This means that the time and memory requirements for the solution scale as $\order{n^2}$, where $n$ is the number of rows/columns in the matrix.


\subsection{Multigrid methods}

There are a number of ``simple'' iteration methods which are fairly ineffective when used directly to solve reasonably sized linear systems, but are very cheap to apply since they can be written in a form where each iteration is a matrix vector product.
Examples are Jacobi, Gauss-Seidel and SOR methods \cite[103]{Saad2000}.

Multigrid methods are a class of linear solvers which combine these simple iteration methods with a hierarchical algoritm, resulting in highly efficent and scalable solvers \cite{multigrid-tut}.
They are based on two observations: firstly that the simple iterative solvers are very effective at reducing the high-frequency (in space) error of a solution, but fail on the low frequency parts.
The second observation is that many of the low frequency errors can be converted to high frequency errors by solving the problem on a coarser mesh.
So by solving the problem on a sequence of meshes all frequencies of error can be treated effectively (except the very lowest, which can be removed by a direct solve on a very coarse mesh).

Typical multigrid solvers work use something similar to the following algorithm
\begin{itemize}
\item Start with initial guess on finest mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh \\
  $\vdots$
\item On the coarsest mesh solve the solution directly
\item Project the solution onto a finer mesh
\item Apply a few iterations of a simple solver \\
  $\vdots$
\item End with solution on finest mesh
\end{itemize}
The details of the ``simple'' iterative solver, the projection between meshes, the selection of coarse meshes, the exact order/number of repeats of the cycle etc. all vary between methods.

It turns out that multigrid solvers can achieve performance which does not degrade as the number of nodes in the mesh is increased, \ie they can solve the system in $\order{N}$ operations.

Their main downside is that they can be extremely fickle with regards to application to different problems, and can be difficult to optimise due to the large number of input parameters.
They are truely effective only for certain types of matrix.
In particular they are effective on the matrix resulting from discretising a Poisson problem which, is in some way the most important part of many elliptic PDEs (due to the Laplace operator containing the highest derivatives).

In fact it often turns out that multigrid methods are most effective when combined with another type of solver, as will be discussed below.


\subsection{Krylov solvers}
\label{sec:krylov-solvers}

??ds expand

Another class of iterative linear solvers are the Krylov solvers.
These methods are based on building an approximation to the solution from linear combinations of the Krylov subspace of the matrix

Based on Krylov subspace (polynomial of $\Am$ multiplied by $\xv$).

The size of the Krylov subspace needed for an effective approximation (\ie the number of iterations needed) is strongly dependent on the propeties of the matrix, in particular the eigenvalues.
So for optimal performance a ``preconditioner'' is needed, which brings the eigenvalues of the matrix close to 1.
Essentially the preconditioner needs to be a reasonable but cheap to calculate approximation to the inverse of the matrix (since $\Am^{-1}\Am = \Im$, and the identity matrix is the matrix with all eigenvalus 1).
The construction of a preconditioner which is effective independent of the mesh size is a difficult and problem dependent task. 

Some work has been done on preconditioning for the LLG by Suess et. al. \cite{Suess2002}.
Also Banas et. al.\cite{Banas2008} \cite{Banas2010} created a multigrid preconditioner for the Maxwell-LLG equation with a fixed point linearisation method (and their time step sizes are a factor of 20 smaller than a similar algorithm using a Newton-Raphson linearisation on the same problem).


\section{Magnetostatic Field Calculations}
\label{sec:magn-field-calc}

Methods for the calculation of magnetostatic fields break down into two categories: intergral and potential methods.
Integral methods are based on efficiently approximating the very large number of integrals required by \eqref{eq:Hmsint}.
Potential methods are based on the use of the potential form \eqref{eq:Hms} with standard spatial discretisation methods, but some special treatment is required for the boundary conditions at infinity \eqref{eq:cont-phi-bound}.

More recently models have been developed which extend the magnetic field calculations to include all of Maxwells equations (and so to calculate the electric field and its effects on the magnetic field as well).
Such models greatly increase the number of degrees of freedom in the problem: 3 magnetisation components, 3 magnetic field components and 3 electrical field components are needed, whereas for magnetostatic calculations only the magnetisation and possibly up to 2 potentials are needed.
Since the additional electrical field does not seem to have a substantial effect on the results for most problems of interest we will not consider these full Maxwell models further.

\subsection{Integral Methods}
\label{sec:magstat-field-calc-inte}

\begin{figure}
  \centering
  \begin{tikzpicture}[level 1/.style={sibling distance=5.4cm},
    level 2/.style={sibling distance=3.6cm}]

    \node[block] {\textbf{Magnetostatic Calculations}}
    child {node[block,text width=6cm] {Scalar Potential Formulation (with some spatial discretisation)}
      child{node[block,text width=4cm,xshift=-1cm] {Asymptotic Boundary Conditions}}
      child{node[block,text width=4.3cm] {Hybrid Finite/Boundary Element Method}}
    }
    child {node[block,yshift=-2.7cm] {Integral Formulation}
      child{node[block,text width=3.2cm] {Full Calculation}}
      child{node[block,text width=3.2cm] {Fast Fourier Transform}}
      child{node[block,text width=3.2cm] {Fast\\ Multipole\\ Method}}
    };
  \end{tikzpicture}
  \caption{Methods of magnetostatic field calculation that have been used in micromagnetic models.}
  \label{fig:types-mag-stat}
\end{figure}

Integral methods are based on the expression
\begin{equation}
  \Hms(\xv) = \frac{1}{4 \pi} \bigs{ 
    - \int_\magd \nabla' \cdot \Mv(\xv') \frac{(\xv - \xv')}{\abs{\xv -\xv'}^3} \d^3 \xv'
    + \int_\boundd \Mv(\xv') \cdot \nv(\xv') \frac{(\xv - \xv')}{\abs{\xv - \xv'}^3} \d^2 \xv' }.
  \label{eq:Hmsint2}
\end{equation}

After the application of a discretisation scheme the integrals in \eqref{eq:Hmsint2} become a sum over all nodes.
The naive way to calculate the magnetostatic fields would then be to work through the list of nodes calculating the field at each of them, so for each node a sum of the contributions from all other nodes must be calculated.
Hence this results in a computation time that scales as $\order{N^2}$ (where $N$ is the number of points used in the space discretisation), this is usually unacceptably slow.

In this section we refer to magnetic charges to mean the macrospins/cells/nodes as appropriate for the spatial discretisation, and $N$ the number of such charges.


\subsubsection{Fast fourier transform methods}

The fast Fourier transform method (FFT) is a simple and efficient method for calculating the magnetostatic field when the magnetic charges are on a regular lattice and the boundary conditions are periodic.

The calculation of $\Hms$ in equation~\eqref{eq:Hmsint} can be thought of as applying a convolution operator (\ie $\Hms = D \big[\Mv\big]$).
The matrix corresponding to this operator is only dependent on the geometry, hence it can be precomputed, Fourier transformed and stored.
Then all that is needed to calculate the magnetostatic field is to apply a Fourier transform to $\Mv$, compute the convolution and transform the result back into the time domain by applying the inverse Fourier transform.
Because of the regularity, applying the convolution in the frequency domain is very fast and hence the complexity of the calculation is limited by the complexity of a fast Fourier transform.
This results in a complexity of $\order{N \log(N)}$.
\cite{Jones1997}

The downside of this method is that points to be calculated must be on a regular lattice, similar to the finite difference method.
Hence, it is most suited for use in combination with models using a finite difference spatial discretisation.
Alternatively it may be used in less regular macrospin models by approximating the the macrospins as being on a regular lattice \cite{Jones1997}.


\subsubsection{Fast multipole method}
\label{sec:fast-mult-meth}

An alternative method of calculation of the magnetostatic field is the fast multipole method (FMM).
The fast multipole method takes advantage of the fact that distant magnetic charge has a much smaller effect on the total field at a point than a nearby magnetic charge.

For the field calculation at a specific point the full calculation is only performed for nearby magnetic charges.
Groups of more distant charges are approximated (lumped) as a single multipole placed at the centre of the group.
As the charges become more distant they contribute much less to the field due to the $\frac{1}{(\xv - \xv')^2}$ scaling in equation~\eqref{eq:Hmsint2}.
Hence for distant points the a cheaper but less accurate approximation can be used without reducing the accuracy of the final result.

The trick for quickly calculating fields at a large number of points is to pre-calculate multipole approximations for a range of accuracies over all space.
Then the calculation of a field at a single point only requires the full calculation of effects from a few nearby points and from the appropriate multipoles \cite{Beatson1997}.

One advantage of this method over the fast Fourier transform is that it allows for arbitrary geometries.
Also the complexity of the method is $\order{N}$ \cite{Chang2011}, so FMM is more efficent than FFT for sufficiently large problems (although ``sufficiently large'' may in fact be very large due to the cost of computing the required multipole approximations).


\subsection{Scalar potential methods}
\label{sec:magstat-field-calc-pote}

Potential methods are based on the formulation:
\begin{equation}
\begin{aligned}
  \label{eq:Hms2}
  \hms &= - \nabla \phim, \\
  \lap \phim &= \nabla \cdot \mv,
\end{aligned}
\end{equation}
with boundary conditions
\begin{equation}
  \begin{aligned}
    \label{eq:cont-phi-bound2}
    \phim^\inte - \phim^\exte &= 0 \quad \xv \in \boundd, \\
    \pd{\phim^\inte}{\nv} - \pd{\phim^\exte}{\nv} &= \mv \cdot \nv \quad \xv \in \boundd, \\
  \end{aligned}
\end{equation}
and
\begin{equation}
  \begin{aligned}
    \label{eq:phi-inf-bound}
    \phim \rightarrow 0 \text{ as } &\abs{\xv} \rightarrow \infty.
  \end{aligned}
\end{equation}

This is a differential equation and so inside the domain it can be solved simply by applying the finite element or finite difference discretisation methods.
However \eqref{eq:phi-inf-bound}, the boundary condition on $\phim$ at infinity, is problematic.
We obviously can not apply this condition directly using standard methods, since that would involve either an infinite number of elements or infinitely large elements.
Hence other techniques must be used, such techniques are the subject of the rest of this section.

The speed of the internal field calculation is $\order{N}$ since it is just a standard FEM or FD calculation, however applying the boundary conditions can require additional processing time.


\subsubsection{Asymptotic boundary conditions}
\label{sec:asymptot-bcs}

One way to avoid an infinite domain is to truncate the external region at some finite distance from the magnetic domain.
However the relationship between truncation distance and accuracy is problem dependent (since the size of the external field at any point depends on the problem geometry) and does not lead to good accuracy even for large truncation distances.

A more sophisticated method is to use asymptotic boundary conditions \cite{Yang1997}. The idea here is to use a truncated external region to calculate the boundary conditions on the magnetic domain that correspond to \eqref{eq:phi-inf-bound} being applied at infinity.
Additionally the fact that any solution to the Poisson equation~\eqref{eq:Hms2} can be represented as an infinite series of harmonic functions is used to improve the accuracy.
Unfortunately the accuracy of this approach is still quite low, even for large truncation distances \cite{Bottauscio2008}.


\subsubsection{The hybrid finite element method/boundary element method}
\label{sec:bound-elem-meth}

The idea of the hybrid finite element method/boundary element method (FEM/BEM) is to replace the external domain by a dipole layer placed on the surface of the magnetic domain which mimics the effect of the infinite external domain \cite{Fredkin1990}.
This removes the need to truncate or discretise the infinite external domain.
The name comes from the similarity of the use of a dipole layer with the boundary element method, and the fact that the standard finite element method is used to calculate the required potentials in the bulk.
The full details of the method applied to magnetostatic calculations is discussed in \autoref{sec:hybr-finit-elem}.

A comparison by Bottauscio\cite{Bottauscio2008} found that using the FEM/BEM method was more accurate than applying asymptotic boundary conditions for a calculation of the time evolution of the magnetisation of a sphere with zero exchange coupling.
Even with a truncation distance of four times the size of the magnetic sphere (the total domain was $4^d$ times larger then the sphere) the accuracy when using asymptotic boundary conditions was worse and did not improve between truncation distances of three and four times the magnetic sphere radius.
Even when exchange coupling was added (giving an easier test) the truncation method was worse than the FEM/BEM method.

The main downside of the FEM/BEM method is that it involves dense matrix of size $N_b \times N_b$ where where $N_b$ is the number of boundary nodes.
This matrix must be stored in memory, and multiplied by a vector for the calculation of the boundary conditions.
The speed of calculation of the boundary values in the method is limited by the dense matrix multiplication which is $\order{N_b^2}$.
Similarly the addtional memory usage is $\order{N_b^2}$, which can limit the size of problems.
Hence the speed of the FEM/BEM method depends on the geometry.

The use of hierarchical matrix techniques can reduce this to $\order{N_b \log(N_b)}$ \cite{Knittel2009}.
In 3D structures which are roughly spherical $N_b = \order{N^{2/3}}$ and so the use of a hierarchical matrix gives optimal computation speed\footnote{With hierarchical matrix techniques the speed is $\order{N^{2/3}\log(N^{2/3})}$ but $\log(x) << x^{1/2}$ for large $x$, hence $\order{N^{2/3}\log(N^{2/3})} << \order{N}$, \ie optimal computation speed scaling.} but for extremely flat structures the number of boundary nodes can be as bad as $N_b = \order{N}$.

Another downside of FEM/BEM is the increase in the complexity of the model: some components of the FEM/BEM method are fundamentally different to FEM, and much additional code and mathematical knowledge may be needed for its implementation.
As an example: singular integrals occur in the boundary element method so more advanced integration methods are needed than for FEM alone.



\section{Compatability of methods}
\label{sec:comp-meth}

While in theory any of the methods discussed above can be combined with any other, there are certain combinations which are problematic.
In this Section we discuss which combinations of methods are especially good or bad.

The use of integral methods with implicit time integration is problematic: the integral formulation couples magnetisation at every point to every other point, meaning that the Jacobian representing the derivatives of each magnetisation with respect to each other magnetisation is completely dense (and hence solution times are extremely slow)!
Explicit time integration methods avoid this issue because no system solve is required.
The most well known example of such a model is OOMMF.


For problems with very simple geometry (\ie thin films, cubeoid shapes) the finite difference method with FFT for magnetostatic calculations is sufficient.
Additionally: for simple geometry there is often no need for well refined meshes, so the problem can be only mildly stiff and explicit time integration methods can be efficient.
This combination of methods is very simple to implement and so is widely used, especially with GPU based code.

For problems with more complex geometry the finite element method must be used for spatial discretisation to obtain good accuracy. 
Magnetostatic calculations on the resulting non-uniform mesh are typically then carried out using either FMM (fastmag) or FEM/BEM (nmag, magpar, femme, \ldots).
The use of refined meshes to fully resolve the geometry can often result in stiff problems, requiring the use of implicit time integration schemes for good efficiency.
In all ``real world'' models using implicit time integration known to the author the Newton-Raphson method is used for linearisation.
Typically some form of preconditioned Krylov solver is then used to solve the resulting linear systems.

\subsection{Non-linear solvers and geometric integration}

It should be noted that the geometric properties of IMR derived in \autoref{sec:proof-magn-length-ode-imr-llg} and \autoref{sec:proof-energy-prop} are only true up to the accuracy with which we solve the LLG.
This is the tolerance of the linearisation method used. 

Additionally the derivations assume that all fields are calculated at the midpoint without any additional approximations, \ie that we are using a fully implicit method.
In contrast to this, many existing models use an explicit approximation to the magnetostatic field in order to avoid including a dense contribution for the long-range magnetostatic interactions\footnote{A dense contribution arises from all accurate magnetostatic calculation methods known to the author, in particular: FEM/BEM, FFT and FMM. See \autoref{sec:magn-field-calc} for more details.} in the non-linear solve.
Unfortunately in this case the energy properties will be lost due to the replacement of $\hop \left[ \mpm \right]$ by different approximation (in a similar way to the semi-explicit modification of IMR).
However the magnetisation length property will be retained because it only relies on $\mv$ itself being calculated implicitly.

One way to allow a completely implicit treatment of the magnetostatic field without a dense contribution was explored by d'Aquino \cite{DAquino2005}.
He used a quasi-Newton method where the dense contribution was simply left out of the Jacobian matrix.
This greatly slows down the convergence of the non-linear solve: in the example given in the paper 10-14 iterations were needed (compared to $\sim2$ with a full Newton method).
Additionally in the general case it is not clear that this modified Newton method will converge at all!

In \autoref{sec:fully-implicit-bem} we describe and demonstrate a novel efficent and robust solver allowing fully implicit magnetostatic calculations.


% \section{Conclusions}
% \label{sec:model-conclusions}

% A dynamic modelling method will be used because the ability to model time dependent processes is essential.

% For the spatial discretisation a macrospin model is not considered because its application is limited to cases with well separated grains and because it is not able to model effects inside a grain.
% Therefore a finite element spatial discretisation will be used because the ability to treat arbitrary geometries is extremely desirable (for example in the modelling of bit patterned media).
% Additionally \texttt{oomph-lib}\cite{oomph-lib-website} (a multi-physics open source finite element modelling library) gives an excellent framework in which to construct a new finite element micromagnetic model.
% This choice will also allow easy extension to the modelling of heat assisted magnetic recording using a finite element discretisation of the heat equation to model heat flow.

% The magnetostatic field will be evaluated from a scalar potential by the hybrid finite/boundary element method.
% A finite element method will be used because it can be easily integrated into the overall model.
% The hybrid method will be used to apply boundary conditions because the accuracy loss involved in efficiently approximating the external region by asymptotic boundary conditions is large and problem dependent.

% The time discretisation scheme used will be the mid-point method because of it's preservation of magnetisation vector length and ability to deal with stiffness.
% The use of preconditioners to speed up to the solution of the non-linear system created at each time-step will be investigated.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
