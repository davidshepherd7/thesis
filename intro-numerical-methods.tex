\chapter{Numerical Methods for Dynamic Micromagnetic Modelling}
\chaptermark{General numerical methods}
\label{sec:numer-meth-micr}

\section{Introduction}

In \thisref{sec:numer-meth-micr} we give an overview of numerical methods that have previously been used in dynamic micromagnetic calculations.
These calculations involve solving some form of the LLG equation \cref{eq:LLG} with effective fields determined by energy derivatives as discussed in \cref{sec:cont-micromag}.
These systems of partial differential equations (PDEs) can only be solved analytically in a few extremely simple cases \cite{Aharoni1996}, so numerical solution methods are almost always necessary.


\subsection{Components of a numerical method for solving PDEs}

Many numerical methods for solving non-linear PDEs, such as \cref{eq:LLG}, can be thought of as a combination of various component methods, each of which handles a different part of the conversion from a continuous PDE into an algorithm which can be performed by a computer\footnote{Technically proofs of convergence etc. should be carried out for every different combination of time and space discretisation \cite[382]{Iserles2009}; in practice surprises seem to be rare, at least within micromagnetics.}.

The essential components of a numerical method for solving PDE problems are:
\begin{itemize}
\item Spatial discretisation: convert space derivatives into algebraic relationships, typically between discrete points, in space, \eg finite elements, finite differences, macrospin models.
\item Time discretisation (a.k.a. time integration):  convert time derivatives into algebraic relationships in time, again typically between points, \eg Runge-Kutta methods, backwards difference formulae.
\end{itemize}
For some choices of time and space discretisations additional component methods are needed:
\begin{itemize}
\item Linearisation procedure: Solve a system of non-linear algebraic equations, often by an iterative procedure which requires the solution of a sequence of systems of linear algebraic equations, \eg Newton-Raphson method, Picard/fixed-point iteration.
\item Linear solver: solve a system of linear algebraic equations, \eg LU decomposition, Krylov solvers, multigrid methods.
\end{itemize}

Additionally in micromagnetic modelling the calculation of the magnetostatic field is required.
If the integral form given in \cref{eq:Hmsint} is used then an additional integral evaluation method is needed to handle it.
Alternatively, if the equivalent potential formulation \cref{eq:Hms,eq:cont-phi-bound} is used then the magnetostatic field calculation simply requires solving additional PDEs (although there are some issues with the application of the boundary condition at infinity, see \cref{sec:magstat-field-calc-pote}).

It should be noted that not all methods fit into this framework of applying separate methods for each part of the problem, see \cref{sec:advanced-lin} for one example.



\subsection{Desirable properties of a numerical method}
\label{sec:desir-prop-numer}

Ideally all numerical methods would run quickly on cheap hardware and with sufficient accuracy, unfortunately this is not the case in general and we are forced to make trade-offs.
In \thisref{sec:numer-meth-micr} we detail some \emph{ideal} properties of a numerical method (although satisfying all of these properties at once is almost certainly impossible).

The accuracy of a simulation is generally proportional to the ``fineness''  of the discretisations used (\ie the number of points used in the approximation), but increasing the fineness also increases the computational time.
However, careful choice of algorithms can improve the accuracy at no additional cost or improve the speed with no loss of accuracy.

\begin{itemize}
\item The time discretisation fineness should be dictated only by what is needed to accurately resolve the physics and not by numerical properties of the discretisation.
  In particular improved spatial fineness should not require improved time fineness.

\item The optimal fineness in time (and space) should be determined automatically by the algorithm, and should be allowed to vary as needed.
  This allows us to get the best ``value'' for our time when increasing the fineness of the discretisation.

\item The computation time should scales linearly as fineness in space or time increases.
  The minimum increase is linear scaling ($\order{N}$, where $N$ is the number of discrete points involved) because we always have to do \emph{something} with each value.
  Attaining this property for fine spatial discretisation requires specific time discretisations (\ie increasing the fineness should not be coupled).
  Attaining this property for spatial discretisation usually impacts the choice of linear solver and/or magnetostatic calculation: many methods do not scale this well.

\item Can handle ``interesting'' geometries without extortionate cost:
  Ideally our method should be able to effectively approximate any geometry without major computational cost.
\end{itemize}

Additionally we would like our algorithms to be ``robust'', that is they should reliably give a correct answer (up to any tolerances specified).
In other words there should be few edge cases or ``interesting'' surprises when applying the method to various physical problems.
Some important properties which improve robustness are:
\begin{itemize}
\item Automatic determination of the required fineness helps here as well: there is less possibility to choose

\item The method should obey any qualitative physical properties of the continuous equation.

\item Mathematical evidence that all parts of the method will ``work'' on any well defined problem (\eg proofs of convergence, etc.).
\end{itemize}

Other nice things to have are

Simplicity: people need to understand a method before they can implement it, and even users need a surface level understanding of a method.

Generality: methods which apply to many problems can be implemented once in a library by experts and reused by many users.
Also general applicability makes methods much easier to test because of a greater choice of example problems.


\section{Spatial Discretisation}
\label{sec:spat-discr}

\subsection{Macrospin models}
\label{sec:sd-macrospins}

In a granular magnetic material (a material consisting of magnetic grains separated by a non-magnetic material) the simplest way to handle spatial variation in the problem is to assume that within each grain the exchange effective field is so strong that the magnetisation is constant, \ie that each grain behaves as a single \emph{macrospin}.
We assign a single value of $\Mv$ to each macrospin and proceed to calculate energy, effective field and/or magnetisation of each macrospin as required.
One caveat is that the effects of the magnetostatic field of a grain on the grain itself is not automatically accounted for since there is no modelling of intra-grain effects.
Hence it must be calculated and included separately to the magnetostatic interactions between grains.
When included in this way the magnetostatic self field is often called the \emph{shape anisotropy} since it is dependent on the shape of the grain and acts in a similar way to the magnetocrystalline anisotropy.

This macrospin approach may be used with any system in which there are a number of ``small'',\footnote{All dimensions of the bodies must be much smaller than the exchange length so that all magnetisation within the body is approximately parallel.} separate magnetic bodies with approximately uniform magnetisation inside the body.

The obvious downside of a macrospin approach is that it only applies to a fairly specific geometry, although the case of a granular media has been of much interest for magnetic data storage.
Additionally, if there are nonuniformities in magnetisation within the regions where it has been assumed constant the model may be inaccurate.

However it is often simpler to construct a macrospin model than to use the more general methods described in \cref{sec:sd-finite-diff-meth, sec:sd-finite-elem-meth}.
Also the assumption that each grain has uniform magnetisation will reduce the number of calculations needed.

Closely related to macrospin models are atomistic models \cite{Evans2014}.
In these models the assumption of a continuous magnetisation $\mv(\xv, t)$ is dropped in favour of an assumption that each atom is the location of a single macrospin.
As such they allow modelling of some systems, such as anti-ferromagnetic materials, that are inaccessible using micromagnetics.
However, the required spatial resolution comes with a large increase in computational cost.
Additionally  such methods are only accurate for materials where the magnetic moment is localised to an atom, which is not always the case.


\subsection{Finite Difference Methods}
\label{sec:sd-finite-diff-meth}

Another method of spatial discretisation is the finite difference method: a single magnetisation vector is assigned to each point on (or ``cell'' in) a simple square/cubic grid which covers the entire domain.
Spatial derivatives in the PDE are then approximated by using truncated Taylor series expansions.

The finite difference method works well for very simple geometries when the grid can be lined up with all geometric features.
For example when we are interested in how the magnetisation evolves over time in a non-granular cuboid-shaped thin film of magnetic material a finite difference method will be sufficient (\eg in the \mumag standard problems \cite{mumag-website}).

However when the geometry involves curves, diagonals, hexagonal grains, bit patterned media or any other more complex geometric system other methods are better suited.


\subsection{Finite Element Methods}
\label{sec:sd-finite-elem-meth}

A more complex method of spatial discretisation is the finite element method \cite{HowardElmanDavidSilvester2006}.
Here the magnetic body is divided up into a finite number of non-overlapping \emph{elements}, which can vary in both shape and size.
Within each of these elements each magnetisation component is approximated by a simple polynomial function of space, typically a linear polynomial.
Spatial derivatives are calculated by simply differentiating these polynomial functions.
In contrast to macrospin and finite difference methods the actual equations solved by the finite element method are a \emph{weak} form of the standard PDE (or the \emph{strong} form).
More details of the weak form and the finite element method are given in \cref{sec:intr-finite-ele-diff}.

The main advantage of the finite element method is that it can cheaply and accurately (compared to the finite difference method) approximate non-trivial geometrical features by an using an appropriate mesh of elements.
An additional advantage is that the size of elements (and thus the accuracy of the approximation) can be varied arbitrarily as needed to give better accuracy in more complex or important regions.
The choice of element size can be done automatically using \emph{adaptive mesh refinement}: after each calculation an error estimate is calculated (known as \emph{a posteriori} error estimation).
If the error is determined to be too high anywhere the mesh is refined near that region and the calculation is repeated.
Hence, given the desired error and a method to estimate the error, a mesh giving an efficient and accurate approximation can be automatically generated \cite{Schrefl1999}.

The major downside of finite element models is that the underlying mathematics is more complex than that required for other methods, especially finite difference methods.
Also the set up time and memory usage can be greater because of the additional ``bookkeeping'' required to keep track of the more complex meshes.


\section{Magnetostatic Field Calculations}
\label{sec:magn-field-calc}

Methods for the calculation of magnetostatic fields belong to two categories: integral and potential methods.
Integral methods are based on efficiently approximating the very large number of integrals required by \cref{eq:Hmsint}.
Potential methods are based on the use of the potential form \cref{eq:Hms} with standard spatial discretisation methods.
However special treatment is required for the boundary condition on the potential at infinity \cref{eq:cont-phi-bound}.

In \thisref{sec:magn-field-calc} we use the term ``discrete magnetisations'' to mean the macrospins/cells/nodes as appropriate for the spatial discretisation.
We use $N$ to denote the number of such charges.

% More recently models have been developed which extend the magnetic field calculations to include all of Maxwells equations (the electric field and its effects on the magnetic field as well).
% Such models greatly increase the number of degrees of freedom in the problem: 3 magnetisation components, 3 magnetic field components and 3 electrical field components are needed, whereas for magnetostatic calculations only the magnetisation and possibly up to 2 potentials are needed.
% Since, for not seem to have a substantial effect on the results for many problems of interest \cite{??ds} we will not consider these full Maxwell models further.

\subsection{Integral Methods}
\label{sec:magstat-field-calc-inte}

% \begin{figure}
%   \centering
%   \begin{tikzpicture}[level 1/.style={sibling distance=5.4cm},
%     level 2/.style={sibling distance=3.6cm}]

%     \node[block] {\textbf{Magnetostatic Calculations}}
%     child {node[block,text width=6cm] {Scalar Potential Formulation (with some spatial discretisation)}
%       child{node[block,text width=4cm,xshift=-1cm] {Asymptotic Boundary Conditions}}
%       child{node[block,text width=4.3cm] {Hybrid Finite/Boundary Element Method}}
%     }
%     child {node[block,yshift=-2.7cm] {Integral Formulation}
%       child{node[block,text width=3.2cm] {Full Calculation}}
%       child{node[block,text width=3.2cm] {Fast Fourier Transform}}
%       child{node[block,text width=3.2cm] {Fast\\ Multipole\\ Method}}
%     };
%   \end{tikzpicture}
%   \caption{Methods of magnetostatic field calculation that have been used in micromagnetic models.}
%   \label{fig:types-mag-stat}
% \end{figure}

Integral methods are based on the expression
\begin{equation}
  \Hms(\xv) = \frac{1}{4 \pi} \bigs{
    - \int_\magd \nabla' \cdot \Mv(\xv') \frac{(\xv - \xv')}{\abs{\xv -\xv'}^3} \d^3 \xv'
    + \int_\boundd \Mv(\xv') \cdot \nv(\xv') \frac{(\xv - \xv')}{\abs{\xv - \xv'}^3} \d^2 \xv' }.
  \label{eq:Hmsint2}
\end{equation}

A naive way to evaluate the integrals in \cref{eq:Hmsint2} would be to calculate the field at each node due to each other node.
So for each node a sum of the contributions from all other nodes must be calculated.
Hence this results in a computation time that scales as $\order{N^2}$, which is usually unacceptably slow for reasonably large $N$.
Instead, more advanced techniques are used which treat the large number of integral evaluations in an efficient way.


\subsubsection{Fast Fourier transform methods}

??ds clarify about convolution?

The fast Fourier transform method (FFT) is a simple and efficient method for calculating the magnetostatic field when the discrete magnetisations are positioned on a regular lattice.

The calculation of $\Hms$ in \cref{eq:Hmsint} can be thought of as applying a convolution to the list of discretised $\mv$ values.
The matrix corresponding to this convolution is only dependent on the geometry, hence it can be precomputed and its Fourier transform precalculated.
Then all that is needed to calculate the magnetostatic field is to apply a Fourier transform to $\Mv$, compute the convolution and transform the result back into the time domain by applying the inverse Fourier transform.
Because of the regularity, applying the convolution in the frequency domain is very fast and hence the complexity of the calculation is limited by the complexity of a fast Fourier transform.
This results in an overall complexity of $\order{N \log(N)}$ \cite{Jones1997}.

The downside of this method is that points to be calculated must be on a regular lattice, similar to the finite difference method.
Hence, it is most useful in combination with models using a finite difference spatial discretisation.

Alternatively the FFT may be used as part of a method applicable to less regular meshes.
In such ``non-uniform FFT'' methods distant discrete magnetisations are approximated by magnetisations on a regular lattice so that an FFT method can be applied while the effects of nearby magnetisations are evaluated directly \cite{Jones1997}.


\subsubsection{Fast multipole method}
\label{sec:fast-mult-meth}

An alternative method of calculation of the magnetostatic field is the fast multipole method (FMM) \cite{Beatson1997}.
The fast multipole method takes advantage of the fact that distant magnetic charge has a much smaller effect on the total field at a point than a nearby magnetic charge.

For the field calculation at a specific point the full calculation is only performed for nearby discrete magnetisations.
Groups of more distant magnetisations are approximated (lumped) as a single multipole placed at the centre of the group.
As the magnetisations become more distant they contribute much less to the field due to the $\frac{1}{(\xv - \xv')^2}$ scaling in \cref{eq:Hmsint2}.
Hence for distant points less accurate but cheaper approximation can be used without reducing the accuracy of the final result.

The trick for quickly calculating fields at a large number of points is to pre-calculate multipole approximations for a range of accuracies over all space.
Then the calculation of a field at a single point only requires the full calculation of effects from a few nearby points and from the appropriate multipoles.

One advantage of this method over the fast Fourier transform is that it allows for arbitrary geometries.
Also the complexity of the method is $\order{N}$ \cite{Chang2011}, so the FMM is more efficient than the FFT for sufficiently large problems (although ``sufficiently large'' may in fact be very large due to the cost of precomputing the required multipole approximations).


\subsection{Scalar potential methods}
\label{sec:magstat-field-calc-pote}

Potential methods are based on the formulation (see \cref{sec:magnetostatic-field} for details):
\begin{equation}
  \begin{aligned}
    \label{eq:Hms2}
    \hms &= - \grad \phim, \\
    \lap \phim &= \div \mv,
  \end{aligned}
\end{equation}
with the boundary conditions
\begin{equation}
  \begin{aligned}
    \label{eq:cont-phi-bound2}
    \phim^\inte - \phim^\exte &= 0 \quad \xv \in \boundd, \\
    \pd{\phim^\inte}{\nv} - \pd{\phim^\exte}{\nv} &= \mv \cdot \nv \quad \xv \in \boundd, \\
  \end{aligned}
\end{equation}
and
\begin{equation}
  \begin{aligned}
    \label{eq:phi-inf-bound}
    \phim \rightarrow 0 \text{ as } &\abs{\xv} \rightarrow \infty.
  \end{aligned}
\end{equation}

This is a differential equation and so inside the domain, $\magd$, it can be solved simply by applying finite element or finite difference discretisation methods.
However \cref{eq:phi-inf-bound}, the boundary condition on $\phim$ at infinity, is problematic.
We obviously can not apply this condition directly using standard methods, since that would involve either an infinite number of elements or infinitely large elements (actually these are  possible but impose heavy limitations on the allowed mesh geometries see \eg \cite{Alouges2001} and \cite{Fidler2000} respectively).
Hence other techniques must be used, and such techniques are the subject of the rest of \thisref{sec:numer-meth-micr}.

The system of equations for the internal field calculations results in a well known linear algebra problem which can be solved in $\order{N}$ time a number of well studied linear solvers (more details are given in \cref{sec:solution-lin-sys,sec:solution-strategies}).
However applying the boundary conditions can require additional processing time.



\subsubsection{Asymptotic boundary conditions}
\label{sec:asymptot-bcs}

One way to avoid an infinite domain is to truncate the external region at some finite distance from the magnetic domain.
A similar but more sophisticated and accurate approach is to use asymptotic boundary conditions \cite{Yang1997}.
The idea here is to use a truncated external region to calculate the boundary conditions on the magnetic domain that correspond to \cref{eq:phi-inf-bound} being applied at infinity.
Additionally the fact that any solution to the Poisson equation~\cref{eq:Hms2} can be represented as an infinite series of harmonic functions is used to improve the accuracy.

Unfortunately the accuracy of this approach is still quite low, even for large truncation distances \cite{Bottauscio2008}.


\subsubsection{The hybrid finite element method/boundary element method}
\label{sec:bound-elem-meth}

The idea of the hybrid finite element method/boundary element method (FEM/BEM) is to replace the external domain by a dipole layer placed on the surface of the magnetic domain which mimics the effect of the infinite external domain \cite{Fredkin1990}.
This removes the need to truncate or discretise the infinite external domain.
The name comes from the similarity of the use of a dipole layer with the boundary element method, and the fact that the standard finite element method is used to calculate the required potentials in the bulk.
The full details of the method applied to magnetostatic calculations is discussed in \cref{sec:hybr-finit-elem}.

A comparison by Bottauscio \cite{Bottauscio2008} found that using the FEM/BEM method was more accurate than applying asymptotic boundary conditions (ABCs) for a calculation of the time evolution of the magnetisation of a sphere with zero exchange coupling.
Even with a truncation distance of four times the size of the magnetic sphere (the total domain was $4$ times larger then the sphere) the accuracy when using asymptotic boundary conditions was worse and did not improve between truncation distances of three and four times the magnetic sphere radius.
Even when exchange coupling was added (giving an easier test) the ABC method was worse than the FEM/BEM method.

The main downside of the FEM/BEM method is that it involves dense matrix of size $N_b \times N_b$ where where $N_b$ is the number of boundary nodes (see \cref{sec:hybr-finit-elem} for details).
This matrix must be stored in memory, and multiplied by a vector for the calculation of the boundary conditions.
The speed of calculation of the boundary values in the method is limited by the dense matrix multiplication which, the cost of which is $\order{N_b^2}$.
Similarly, the additional memory usage is $\order{N_b^2}$, which can limit the size of problems.

The use of hierarchical matrix techniques can reduce both the speed and the memory usage of FEM/BEM to $\order{N_b \log(N_b)}$ \cite{Knittel2009}.
In 3D structures which are roughly spherical $N_b = \order{N^{2/3}}$ and so the use of a hierarchical matrix gives optimal computation speed\footnote{With hierarchical matrix techniques the speed is $\order{N^{2/3}\log(N^{2/3})}$ but $\log(x) \ll x^{1/2}$ for large $x$, hence $\order{N^{2/3}\log(N^{2/3})} \ll \order{N}$, \ie optimal computation speed scaling.} but for extremely flat structures the number of boundary nodes can be as large as $N_b = \order{N}$.
Hence the speed of the FEM/BEM method depends on the geometry.

Another downside of FEM/BEM is the increase in the complexity of the model: some components of the FEM/BEM method are fundamentally different to FEM.
As such substantial additional code and mathematical knowledge may be needed for its implementation when standard FEM libraries are used as a basis.
As an example: singular integrals occur in the boundary element method so more advanced integration methods are needed than for FEM alone.

The details of the FEM/BEM method and its implementation are discussed in \cref{sec:hybr-finit-elem}.

??ds incorporate this: ?
FEM/BEM is bad for thin film problems because every single node is on the boundary, hence the dense block is very large.
Standard FEM with linear basis functions is significantly slower than finite difference approaches because it allows the magnetisation to vary through the thickness of the film, meaning that the problem is 3D instead of 2D.
(It is certainly possible to construct a finite element method which assumes constant magnetisation in one direction, but our purpose here is to test our model not to invent new ones!)


\section{Time Integration}
\label{sec:time-discretisation}

Time integration schemes are used to convert the time derivatives of a PDE into a form that can be handled computationally.
When discussing time integration schemes it is helpful to use a (vector) initial value ordinary differential equation as an example:
\begin{equation}
  \begin{aligned}
    \frac{d \yv}{dt} &= \ffv{t, \yv}, \\
    \yv(0) &= \yv_0,
    \label{eq:45}
  \end{aligned}
\end{equation}
where $\fv(t,\yv)$ is a known function and $\yv_0$ is the (known) initial condition.
After the application of one of the spatial discretisations described above a PDE is typically reduced to the form \cref{eq:45}; this is sometimes called a \emph{semi-discretised} PDE.

The time integration methods discussed here all bear a strong similarity to the finite difference method discussed in \cref{sec:sd-finite-diff-meth}, except that the independent variable discretised is time instead of space.
This is appropriate since time is one-dimensional, hence no complex geometry is possible and so typically there is no need for the more advanced finite element method.

Some key attributes of a time integration scheme are \cite{Atkinson2009}:
\begin{itemize}
\item \textbf{Accuracy (order)} -- An estimate of how rapidly the approximate solution converges as the time step size is reduced.

\item \textbf{Stability} -- Roughly speaking a scheme is stable if the approximate solution does not ``blow up'' (\ie become catastrophically inaccurate, infinite, oscillate in non-physical ways, \ldots) for sufficiently large time steps (see \cref{sec:A-stability} for a more rigorous description).

\item \textbf{Conservation properties} -- Some differential equations have properties which should ideally be conserved in the discretised system.
For example the magnetisation length and energy properties of the Landau-Lifshitz-Gilbert equation discussed in \cref{sec:prop-cont-llg}.
Schemes which conserve such properties are often referred to as ``geometric'' integrators.

\item \textbf{Self-starting} -- A scheme is self starting if it only requires a single initial value.
This is desirable because methods of estimating additional initial values make the final scheme more complex and may introduce additional errors.
\end{itemize}

These attributes are discussed more rigorously in the rest of \thisref{sec:time-discretisation}.

\subsection{Explicit and implicit time integration schemes}
\label{sec:explicit-vs-implicit-schemes}

Explicit time discretisation schemes calculate the value at the next time step in terms of the value(s) at the present and/or previous time steps.
The simplest such scheme is the \emph{(forward) Euler method}
\begin{equation}
  \label{eq:44}
  \yv_{n+1} = \yv_n + \dtn f(t_n, \yv_n),
\end{equation}
where $\dtn = t_{n+1} - t_n$ is the $n$-th time step size and $\yv_i$ is the approximation to $\yv(t_i)$.
Clearly, given $f(t,y)$ and an initial value for $y(t_0)$ we can solve for $y(t_n)$ for any $n$.

In contrast to explicit schemes, implicit schemes use the value at the next time step in its own calculation.
Hence at each step a (possibly non-linear) system of equations must be solved.
The simplest implicit scheme is the first order \emph{backwards difference formula} (BDF1, also known as backwards Euler)
\begin{equation}
  \label{eq:bdf1-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{t_{n+1}, \yv_{n+1}}.
\end{equation}

At first glance it seems that the additional solution of a system of equations required for one step of an implicit scheme mean would that explicit schemes are more efficient, and it is true that one step of an implicit method requires more computational effort than one step of an explicit method.
However all explicit time integration schemes suffer from issues of limited stability for some problems, and so are forced to use time step sizes much smaller than would be required for reasons of accuracy (see \cref{sec:A-stability} for details).
In these cases implicit schemes can be much more efficient; problems where this is the case are often called ``stiff'' problems.
The semi-discretisation of a PDE often results in a stiff problem.
In \cref{cha:stiffn-llg-equat} we investigate stiffness in micromagnetics problems.


% Micromagnetics solvers for non-stiff systems commonly use the RK4 (fourth order Runge-Katta) method \cite{Suess2002}.
% Micromagnetics models commonly use BDF schemes of various order \cite{Suess2002} or the implicit midpoint rule \cite{DAquino2005} for the modelling of stiff systems.


\subsection{Some implicit time integration schemes}
\label{sec:some-implicit-time-integrators}

For the remainder of \thisref{sec:time-discretisation} we will use three time integration schemes with good stability and accuracy properties that are widely employed in practice as examples.
All three are implicit schemes for reasons of stability, as mentioned in \cref{sec:explicit-vs-implicit-schemes}.

\emph{Trapezoid rule} (TR) is the average of the forward and backward Euler methods from \cref{sec:explicit-vs-implicit-schemes} and is defined as \cite[260]{GreshoSani}:
\begin{equation}
  \yv_{n+1} = \dtn (\ffv{t_{n+1}, \yv_{n+1}} + \ffv{t_n, \yv_n})/2.
  \label{eq:tr-definition}
\end{equation}

The \emph{second order backwards difference formula} (BDF2) is \cite[715]{GreshoSani}:
\begin{equation}
  \frac{\yv_{n+1} - \yv_n}{\dtn} = \frac{\dtn}{2\dtn + \dtx{n-1}} \frac{\yv_n - \yv_{n-1}}{\dtx{n-1}}
  + \frac{\dtn + \dtx{n-1}}{2\dtn + \dtx{n-1}} \fv(t_{n+1}, \yv_{n+1}).
\end{equation}

The method that this thesis focuses on (for reasons that will become clear through the remainder of \thisref{sec:time-discretisation}) is the \emph{implicit midpoint rule} (IMR).
It is given by \cite[263]{GreshoSani}:
\begin{equation}
  \label{eq:imr-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}}.
\end{equation}
Note that for cases where $\fv$ is linear in both $t$ and $\yv$ IMR, \cref{eq:imr-definition}, is equivalent to TR, \cref{eq:tr-definition}.
In general the properties of the two methods are similar.

IMR is trivial to implement in any existing implicit solver: take a step of BDF1 (the simplest possible implicit method), \cref{eq:bdf1-definition}, using $\dtn^\bdfo = \dtn^\imr/2$ to find $\yv_\midp$ at $t_\midp$.
Then calculate $\yv_{n+1}$ and $t_{n+1}$ using rearrangements of $\yv_\midp = \frac{\yv_n + \yv_{n+1}}{2}$ \cite{Malidi2005}:
\begin{equation}
  \begin{aligned}
    t_{n+1} &= t_\midp + \dtn^\imr/2, \\
    \yv_{n+1} &= 2\yv_\midp - \yv_n.
  \end{aligned}
\end{equation}

TR and IMR are both self starting, while BDF2 requires an additional start-up value.
This can be generated, for example, by taking a single step of IMR or TR before beginning the use of BDF2.


\subsection{Local truncation error and order}
\label{sec:deriv-local-trunc}

The \emph{local truncation error} (LTE) of a time integration scheme is the error due to a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$, $\yv_{n-1} = \yv(t_{n-1})$, etc. into the approximation for the next time-step, then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$, \ie
\begin{equation}
  \label{eq:60}
  \lte = \yv(t_{n+1}) - \hat{\yv}_{n+1},
\end{equation}
where the history values ($\yv_{n}, \yv_{n-1}, \ldots$) used to calculate $\hat{\yv}_{n+1}$ are exact.
For example the local truncation error of IMR is
\begin{equation}
  \lte^\imr =  \yv(t_{n+1}) - \yv(t_n) - \dtn \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}^\imr}{2}}.
  \label{eq:trunc-start}
\end{equation}

If the local truncation error of a time integration scheme is such that
\begin{equation}
  \lte \leq c \dtn^p
\end{equation}
then we say that the scheme is of order $p$.

It is important to distinguish the local truncation error from the \emph{global (temporal) error}
\begin{equation}
  \label{eq:global-temporal-error}
    E_n = \yv(t_{n+1}) - \yv_{n+1}.
\end{equation}
The difference is that the global error includes any error accumulated over previous time steps (\ie the exact values $\yv(t_n)$ in \cref{eq:60} are replaced by their approximations).
Note that because of this accumulation the global error will typically be larger than the LTE.
For example the global error of IMR expressed in the same form as \cref{eq:trunc-start} is
\begin{equation}
  E_n^\imr =  \yv(t_{n+1}) - \yv_n^\imr - \dtn \ffv{ \thf, \frac{\yv_n^\imr + \yv_{n+1}^\imr}{2} }.
\end{equation}

The TR and BDF2 methods are both second order.
Their local truncation errors are \cite[261]{GreshoSani}
\begin{equation}
  \label{eq:tr-lte}
  \lte^\tr = \yv_{n+1} - \yv(t_{n+1}) = -\frac{\dtn^3 \yv_n'''}{12}
  + \order{\dtn^4},
\end{equation}
and \cite[715]{GreshoSani}
\begin{equation}
  \label{eq:bdf2-lte}
  \lte^\bdf = \yv_{n+1} - \yv(t_{n+1}) = \frac{(\dtn + \dtx{n-1})^2}{\dtn(2\dtn + \dtx{n-1})}
  \frac{\dtn^3 \yv_n'''}{6}
  + \order{\dtn^4}.
\end{equation}
The derivations of \cref{eq:tr-lte,eq:bdf2-lte} are simple and can be found in most text books on the subject.

We now give a derivation of the local truncation error of IMR, which is somewhat complex because of the approximation $\yv \approx \frac{\yv_n + \yv_{n+1}}{2}$.
We first write the Taylor expansion $\yv(t_{n+1})$ and $\yv(t_{n})$ about $\thf$.
We use the midpoint rather than one of the end points (a typical choice in such calculations) because it results in simpler expressions.
Assuming that $\yv(t)$ is ``sufficiently smooth'' for the required derivatives to exist, its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf[']
  + \frac{\dtn^2}{8} \yvhf['']
  + \frac{\dtn^3}{48} \yvhf[''']
  + \order{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
Similarly, the expansion at $t_n$ about $\thf$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf[']
  + \frac{\dtn^2}{8} \yvhf['']
  - \frac{\dtn^3}{48} \yvhf[''']
  + \order{\dtn^4}.
  \label{eq:taylorn}
\end{equation}
Substituting \cref{eq:taylornp1,eq:taylorn} into \cref{eq:trunc-start} gives\footnote{If we had chosen to Taylor expand about $t_n$ there would be an additional term in $\yv_n''$ in \cref{eq:trunc-mid}.}
\begin{equation}
  \lte^\imr = \underbrace{\frac{\dtn^3}{24} \yvhf[''']}_{\text{I}}
  + \underbrace{\dtn\bigs{ \yvhf['] - \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}}{2}} }}_{\text{II}}
  + \order{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to \cref{eq:trunc-mid}: the first term (I) is fairly standard for second order time integration schemes (\cf the LTEs for TR and BDF2, \cref{eq:tr-lte,eq:bdf2-lte}).
The second term (II) originates from the use of an approximation to $\yv(\thf)$ in the evaluation of $\fv$ (\ie the Runge-Kutta nature of IMR).
The rest of the derivation requires applying Taylor expansions to II and is carried out in \cref{sec:full-imr-lte-calculation}.
The final result shows that IMR is of second order:
\begin{equation}
  \lte^\imr = \frac{\dtn^3}{24} \left[\yvhf['''] - 3 \dfdyhf \cdot \yvhf[''] \right]
  + \order{\dtn^4},
  \label{eq:trunc-final}
\end{equation}
where $\dfdyhf = \pd{\fv}{\yv}\evalat{t=\thf}$ is the Jacobian of $\fv$ with respect to $\yv$ evaluated at $t=\thf$.
An additional condition required in the derivation is that ??ds the magnitude of the eigenvalues of $\dtn\dfdyhf$ are less than one, the implications of this are discussed in \cref{sec:order-reduction}.

??ds for comparison with the other local truncation errors we need to shift to $t_n$.
A simple approximation can be given easily: for $\fv$ linear in $yv$, $t$ TR and IMR are the same, hence term I must be the same as TR in this case, a second error term is then added to $\lte^\imr$ corresponding to II for non-linear $\fv$.
Hence TR has the smallest LTE of the three methods discussed for most real-world ODEs.
The relative magnitude of the additional term in $\lte^\imr$ is highly problem dependent, see \cref{sec:order-reduction} for details, so the comparison of the LTEs of IMR and BDF2 is more complex.
However, in terms of the accuracy of calculations, the accumulated local truncation error is what matters, so these comparisons are not the whole story.

\subsection{A-stability}
\label{sec:A-stability}

To discuss the stability of time integrators the following initial value ODE is widely used
\begin{equation}
  \begin{aligned}
    \ffv{\yv} &= \lambda \yv, \\
    \yv_0 &= 1.
    \label{eq:ode-test-f}
  \end{aligned}
\end{equation}
Its analytical solution is
\begin{equation}
  \yv(t) = \exp(\lambda t).
\end{equation}

A-stability\footnote{The A does not stand for anything, it is just ``A'' \cite[40]{HairerWanner}. In particular it does \emph{not} mean ``absolute stability''.} is the property that a method is has no stability restrictions on the time step size when used to integrate \cref{eq:ode-test-f} for all $\lambda$ with $\realp(\lambda) \leq 0$.
% When $\realp(\lambda) > 0$ the ODE itself is unstable (, and so stability is not expected in general.

The IMR, TR, BDF1 and BDF2 methods are all A-stable \cite[pgs. 43, 251]{HairerWanner}.
Linear explicit methods are never A-stable \cite{Nevanlinna1974} (``linear'' methods in this reference include all common time integration methods: Runge-Kutta, multistep, predictor-corrector, \ldots)\footnote{These limitations could theoretically be circumvented by using significantly more complex methods, but such questions are far beyond the scope of this thesis.}, which is the main reason for our focus on implicit methods.
Additionally there are no A-stable linear multistep methods of order greater than two\footnote{A-stable implicit Runge-Kutta methods of higher order do exist, see \eg \cite[73]{HairerWanner}, but they require the solution of significantly larger systems of equations.} \cite[261]{GreshoSani}, which is why we have chosen the  methods in \cref{sec:some-implicit-time-integrators} for discussion.

\subsection{Spurious numerical damping}
\label{sec:numerical-damping}

Some time integration methods create additional, non-physical damping in approximate solutions, even when none exists in the exact solution.
This is problematic for the modelling of highly oscillatory ODEs, such as the LLG with low damping.

In the solution of the ODE \cref{eq:ode-test-f} with $\lambda = i\omega$, $\abs{y}$ should not decrease over time.
If $\abs{y_n}$ decreases over time in the approximate solution given by a time integration scheme, then we say that the scheme causes spurious damping.

For example, for IMR:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega (y_n + y_{n+1})/2, \\
    &= \frac{1 + i\dtn \omega/2}{1 - i\dtn \omega/2} y_n,
  \end{aligned}
\end{equation}
therefore
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}}^2 &=  \frac{\abs{1 + i\dtn \omega/2}^2}{\abs{1 - i\dtn \omega/2}^2} \abs{y_n}^2, \\
    \abs{y_{n+1}}^2 &=  \frac{(1 + i\dtn \omega/2)(1 - i\dtn \omega/2)}
    {(1 - i\dtn \omega/2)(1 + i\dtn \omega/2)} \abs{y_n}^2, \\
    &=  \abs{y_n}^2,
  \end{aligned}
\end{equation}
hence IMR does not cause spurious damping.

Since $f$ is linear in $y$ for this example TR and IMR are identical
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn(i\omega y_n + i\omega y_{n+1})/2, \\
    &= y_n + \dtn i \omega (y_n + y_{n+1})/2,
  \end{aligned}
\end{equation}
and the same property applies for TR.

For BDF1, however, we have:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega y_{n+1}, \\
    &= \frac{1}{1 - i \dtn \omega} y_n.
  \end{aligned}
\end{equation}
Hence,
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}} &= \sqrt{ \frac{1}{(1 - i \dtn \omega)(1 + i \dtn \omega)}} \abs{y_n}, \\
    &= \frac{1}{\sqrt{1 + \dtn^2 \omega^2}} \abs{y_n}, \\
  \end{aligned}
\end{equation}
and the magnitude of $y$ decreases over time, \ie the solution is damped.
The analysis for the second order BDF2 case is more complex due to the fact that it is a multistep method, but the result is the same: the solution suffers from non-physical damping \cite[265]{GreshoSani}.


\subsection{B-convergence and order reduction}
\label{sec:order-reduction}

It is known that certain implicit Runge-Kutta methods are susceptible to reduced accuracy when used to solve certain extremely stiff problems \cite[156]{Atkinson1994} \cite[225]{HairerWanner}.
In the case of the implicit midpoint rule this corresponds to cases when term II of the LTE \cref{eq:trunc-mid} is large.
This occurs when $\pd{\fv}{\yv}\evalat{\thf{}}$ is large, \ie when the error in $\ffv{t, \yv}$ due to a small error in $\yv$ is large.

The concept of ``B-convergence'' is used to analyse this effect.
Roughly speaking, a Runge-Kutta method is B-convergent of order $r$ if the global error is $\order{\dtx{\text{max}}^r}$ for ``infinitely stiff'' ODEs.
The implicit midpoint rule is B-convergent of order 1 \cite[231]{HairerWanner},\footnote{In this reference IMR is referred to as the second order Gauss method, BDF1 is Radau IIA, TR is Lobatto IIIA \cite[72-76]{HairerWanner}.} which is one order less than its global error on typical ODEs.
We call this effect ``order reduction''.
In contrast, the TR and BDF methods do not suffer from order reduction \cite[159]{Atkinson1994}.
This is because all evaluations of $\fv$ are at integer time points (\ie $t_{n+1}, t_{n}, t_{n-1}, \ldots$ rather than $\thf$) and so there is no term in the LTE containing $\pd{\fv}{\yv}$.

This order reduction effect is a potential downside for IMR, but only when applied to specific ODEs (no such effect has been reported in micromagnetics simulations as far as the author is aware).
Additionally it should be accounted for by a good adaptive time step selection algorithm (see \cref{sec:adaptivity} for details).

A simple test ODE demonstrates this phenomenon \cite[157]{Atkinson2009}:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-test-order-reduction}
    f(t, y) &= -\lambda (y - g(t)) + g'(t), \\
    y_0 &= g(0),
  \end{aligned}
\end{equation}
for some function $g(t)$ and parameter $\lambda \geq 0$.
The exact solution (which can be seen to be correct by substitution) is
\begin{equation}
  y(t) = g(t).
\end{equation}

Note that $\pd{f}{y} = \lambda$, so we can directly control the magnitude of term (II) of the IMR truncation error \eqref{eq:trunc-mid}.

We now derive the LTE for IMR in this example when $\abs{\lambda\dtn} \gg 1$.
From \cref{eq:trunc-implicit-form} we have that
\begin{equation}
  \begin{aligned}
    (1 + \frac{\dtn \lambda}{2})\lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}. \\
  \end{aligned}
\end{equation}
Then using $\abs{\lambda\dtn} \gg 1$:
\begin{equation}
  \begin{aligned}
    \frac{\dtn \lambda}{2} \lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}, \\
    \lte^\imr &= \frac{\dtn^2}{12\lambda} \left[g'''(\thf) - 3 \lambda g''(\thf) \right] + \order{\dtn^4}. \\
  \end{aligned}
\end{equation}
If additionally $\abs{\lambda\dtn} \gg \abs{g'''(t)}$, which will typically be true for very large $\lambda$ and $g \neq g(\lambda)$:
\begin{equation}
  \lte^\imr = \frac{-\dtn^2}{4} g''(\thf) + \order{\dtn^3}.
  \label{eq:reduced-order-imr-truncation-error}
\end{equation}
So we can see that the order has been reduced by one power of $\dtn$ compared with the standard LTE \cref{eq:trunc-final} (which is the case for $\abs{\lambda\dtn} < 1$).


\subsection{Adaptivity}
\label{sec:adaptivity}

Adaptive time integration methods automatically select time step sizes in response to an estimate of the local truncation error.
This can improve efficiency depending on the equation being solved, for example on problems where a high frequency part of the solution is gradually damped out the time step can increase by orders of magnitude at later times without loss of accuracy.
Adaptive integration also greatly simplifies the use of the simulation program: the user only has to choose the allowed local temporal error and the program will handle the rest.

Most local truncation error estimators for implicit integrators (e.g. TR, BDF2) use a Milne-device based method \cite[707-716]{GreshoSani}.
This means that they compute an explicit estimate of the solution $\yv^E_{n+1} \sim \yv(t_{n+1})$  (sometimes known as a predictor step) to the same order of accuracy as the implicit method and using only derivatives that are required for the implicit method (so that no additional $\fv$ evaluations are needed).
They then use algebraic rearrangements of the local truncation error expressions of the predictor step and the implicit step to compute an approximation to the local truncation error.

No such estimation is possible for IMR because of the specific form of the local truncation error \eqref{eq:trunc-mid}.
The details of this issue and the first (to the author's knowledge) algorithm for adaptive time integration with IMR are described in \cref{sec:adaptive-imr}.

It is also possible to construct adaptive time step algorithms based on error estimates specific to the equation being solved.
Such estimates have the obvious limitation that they are only (directly) applicable to the equations for which they were derived.

One example of such an algorithm for the LLG equation (in cases where the damping is not exactly correct) the effective damping can be used as an error estimator, as proposed by Albuquerque \etal \cite{Albuquerque2001}.
A major disadvantage of this method (aside from the obvious requirement that the damping be inexact) is that the error estimator is unable to distinguish between spatial errors and temporal errors.

As another example of this less general class of adaptivity methods time error estimates for the LLG equation with limited effective field terms and a specific finite element discretisation were proven and used as part of an adaptive time integration algorithm with both IMR and BDF2 by Banas \cite{Banas-thesis}.
However the derivation is fairly complex (at least for non-numerical analysts), and so would be difficult to extend to include exchange field or FEM/BEM magnetostatics.


\subsection{Magnetisation length conservation}
\label{sec:ensuring-constant-mv}

As discussed in \cref{sec:prop-cont-llg}, the length of the magnetisation, $\abs{\mv}$, at each point in space is constant over time.
However, in the approximations given by numerical methods this property is often lost, and must be enforced separately.
Note that this is not related to the spurious damping discussed in \cref{sec:numerical-damping}: in that \thisref{sec:numerical-damping} the key property was the conservation of the \emph{amplitude} of oscillations.

A simple method of dealing with the constraint is to re-normalise each $\mv$ after some number of time steps or when the error in $\abs{\mv}$ exceeds some tolerance \cite{Fidler2000}.
However this approach fundamentally changes the system of equations being solved in a non-linear and unpredictable manner \cite{Lewis2003}.

??ds Not sure these downsides are true, seems like Mansuripur88 managed to derive all the fields in polars with no conversion to cartesians, only for macrospins though... ask Jim.
If we have a system with only a single macrospin (\ie a single value of $\mv$ represents the magnetisation of the entire system)

One way to avoid this problem is to use a spherical polar coordinate system $(r,\theta,\phi)$.
In these coordinates \cref{eq:LLG} can be expressed in terms of only the angles $(\theta,\phi)$ representing the direction of $\mv$ and the length constraint is automatically enforced since
\begin{equation}
  \label{eq:40}
  \pd{\abs{\mv}}{t} = \pd{r}{t} \equiv 0.
\end{equation}
However problems occur with this approach when the polar angle, $\theta$, approaches zero because $\pd{m_{\phi}}{t} \propto \frac{1}{\sin(\theta)}$ \cite{Fukushima2005} and so the derivative becomes singular.
Also calculating the sum of the effective field vectors is much more complex in spherical polar coordinates (involving trigonometric identities) and this will result in more complex non-linear/linear systems when using implicit time integration schemes.

Geometric time integration schemes aim to solve the issue of length conservation by constructing a scheme that naturally preserves the value of $\abs{\mv}$.
Some examples of such schemes are based on IMR \cite{DAquino2005}, or a semi-explicit extension of IMR using explicit extrapolations of the effective field \cite{Spargo2003} \cite{Serpico2001}.
Alternatively, geometric integration methods based on Cayley transforms can be used \cite{Lewis2003} \cite{Bottauscio2011}.

A third approach is to use Lagrange multipliers to constrain the solution such that $\abs{\mv}$ is constant \cite{Szambolics2008a}.
The main problem with this method is that an additional degree of freedom is needed for every $\mv$ value in space.

\subsubsection{Conservation of $\abs{\mv}$ by IMR in a macrospin model}
\label{sec:proof-magn-length-ode-imr-llg}

In \thisref{sec:proof-magn-length-ode-imr-llg} we show that the discrete form of the Landau-Lifshitz-Gilbert equation that results from the use of the implicit midpoint rule maintains the magnetisation length conservation property of the continuous form shown in \cref{sec:prop-cont-llg}.
These results are applicable to strong form equations, \ie finite difference and macrospin discretisations but not finite elements.
For the purposes of this section we assume that the discretised non-linear Landau-Lifshitz-Gilbert equation is satisfied exactly, \ie the linear and/or non-linear solver used gives exactly correct results.

As can be seen from \cref{eq:imr-definition} the IMR is defined by the following substitutions:
\begin{equation}
\begin{aligned}
  \label{eq:55}
  \pd{\mv}{t} &\rightarrow \frac{\mv_{n+1} - \mv_n}{\dtn}, \\
  \mv &\rightarrow \frac{\mv_{n+1} + \mv_n}{2}, \\
  t &\rightarrow \frac{t_{n+1} + t_n}{2}.
\end{aligned}
\end{equation}
So the IMR-discretised version of the LLG is
\begin{equation}
  \frac{\mv_{n+1} - \mv_n}{\dtn} = - \frac{\mv_{n+1} + \mv_n}{2} \times
  \bigb{\hv \bigs{\frac{\mv_{n+1} + \mv_n}{2}} - \dampc \frac{\mv_{n+1} - \mv_n}{\dtn}}.
  \label{eqn:disc-llg}
\end{equation}

??ds Check that I introduce inner products, linear operators, symmetry somewhere

Intuitively the conservation properties of the IMR discretisation can be seen to come from the cancellation of cross terms in $\ip{\lop[\mv]}{\dmdt}$ for any symmetrical linear operator $\lop$.
More precisely:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-linop}
    \ip{\lop \left[ \frac{\mv_{n+1} + \mv_n}{2} \right]}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}} + \ip{\mv_{n+1}}{\lop \mv_{n}} \\
    & \qquad\qquad - \ip{\mv_{n}}{\lop \mv_{n+1}} - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big] \\
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}}
    - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big].
  \end{aligned}
\end{equation}
This means that if $\lop \mv$ is orthogonal to $\dmdt$ in continuous equations it will also be orthogonal in the IMR discretised version.
In particular setting $\lop$ as the identity operator will allow us to show that $\mv$ and $\dmdt$ are orthogonal in \thisref{sec:proof-magn-length-ode-imr-llg} and setting $\lop = \hop$ will allow us to show some nice properties on the energy (see \cref{sec:proof-energy-prop}).

We can now examine the change in magnetisation length using the same technique as in \cref{sec:prop-cont-llg}: take the dot product of \cref{eqn:disc-llg} with $\frac{\mv_{n+1} + \mv_n}{2}$ on both sides and use the triple product identity \cref{eq:dot-cross-id} to get
\begin{equation}
  \label{eq:63}
  \frac{\mv_{n+1} + \mv_n}{2} \cdot \frac{\mv_{n+1} - \mv_n}{\dtn} = 0.
\end{equation}
Now using~\cref{eqn:imr-linop} with $\lop$ as the identity operator gives us
\begin{equation}
  \frac{\ip{\mv_{n+1}}{\mv_{n+1}} - \ip{\mv_n}{\mv_n} }{2 \dtn} = 0.
\end{equation}
Therefore
\begin{equation}
  \abs{\mv_{n+1}} = \abs{\mv_n},
\end{equation}
\ie the magnetisation length does not change between time steps.

Note that for other time integration schemes the property \cref{eqn:imr-linop} typically does not hold.
For example, in the case of BDF1 we have
\begin{equation}
  \begin{aligned}
    \ip{\lop \bigs{\mv_{n+1}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_{n+1}}{\lop \mv_n} }, \\
    &\neq \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_n}{\lop \mv_n} },
  \end{aligned}
\end{equation}
\ie orthogonality properties from the continuous form do not carry over to the discrete form in general.

Notice that the only requirement in the derivation of \cref{eq:63} is to use the midpoint approximation of magnetisation in the LLG itself.
The derivation does not make any assumption about how the effective field is approximated.
This explains why the semi-explicit modification to IMR also conserves the magnetisation length.


\subsection{Energy conservation/decay}
\label{sec:energy-cons}

Another geometrical property of the Landau-Lifshitz-Gilbert equation, also discussed in \cref{sec:prop-cont-llg}, is the conservation or monotonic decay of energy depending on the value of the damping constant $\dampc$.

IMR also conserves energy when $\dampc = 0$ and it ensures that the energy is a decreasing function of time when $\dampc > 0$ \cite{DAquino2005}, a proof of this property is given in the next section.
??ds mention commutation as well?
The other geometric integration schemes mentioned in \cref{sec:ensuring-constant-mv}, in particular the semi-explicit modifications of IMR, do not have this property.


\subsubsection{Proof of energy property for the IMR discretised macrospin model}
\label{sec:proof-energy-prop}

\newcommand{\happerror}{\mathcal{E}_\text{ap}}

In \thisref{sec:proof-energy-prop} we show that the change in the energy of the magnetic system over time when evolved using the IMR discretised LLG equation obeys certain relationships which are similar to those of the continuous LLG equation.

Before starting the derivation we introduce some alternative notation for the effective field, $\hv$, when considered as an operator, $\hopb{\mv}$:
\begin{equation}
  \begin{aligned}
    \label{eq:hop}
    \hopb{\mv(\xv)} &= \lap \mv(\xv) + \hms[\mv(\xv)] + \hca[\mv(\xv)], \\
    \hv(\xv, t)[\mv(\xv)] &= \hopb{\mv(\xv)} + \happ(\xv, t). \\
  \end{aligned}
\end{equation}

We also expand the midpoint value of the applied field, $\mphapp$, into
\begin{equation}
  \begin{aligned}
    \mphapp &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2}
    + \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4}, \\
    &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror,
    \label{eq:happ-midpoint}
  \end{aligned}
\end{equation}
where
\begin{equation}
  \happerror = \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4},
\end{equation}
is the error in this midpoint-like approximation of $\happ$.

Now we are ready to derive the energy property in the same manner as \cref{sec:proof-magn-length-ode-imr-llg}.
We begin by taking the $\ltwo$ inner product of the IMR discretised LLG equation, \cref{eqn:disc-llg}, with the sum of the IMR discretised effective field and damping terms:
\begin{equation}
  \begin{aligned}
    & \mphop + \happ(\mpt) - \dampc \mpdmdt, \\
    &= \mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror - \dampc \mpdmdt.
  \end{aligned}
\end{equation}
The result of this inner product is
\begin{equation}
  \begin{aligned}
    &\ltip{\mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror}{\mpdmdt} \\
    & \quad - \dampc \ltip{\mpdmdt}{\mpdmdt} = 0,
    \label{eq:54}
  \end{aligned}
\end{equation}
where the RHS is zero due to the triple product identity.

It can be shown (see \eg \cref{sec:linear-symm-field-operators}) that $\hop$ is a linear symmetric operator on $\mv$.
So from \cref{eqn:imr-linop} with $\lop = \hop$ we have the property
\begin{equation}
  \ltip{\hopb{\frac{\mv_{n+1} + \mv_n}{2}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
  = \frac{1}{2\dtn} \bigs{ \ltip{\mv_{n+1}}{\hop \mv_{n+1}} - \ltip{\mv_{n}}{\hop \mv_{n}}}.
  \label{eq:orth-energy}
\end{equation}
Additionally the energy at time step $n$ due to the effective field operator $\hop$ can be written as\footnote{For the magnetostatic and magnetocrystalline anisotropy energies this follows from the definitions.
  For the exchange energy the trick is to use the relationship $\mv \cdot \lap \mv = \frac{1}{2} \lap \abs{\mv}^2 - (\grad \mv)^2 = - (\grad \mv)^2$ (since $\abs{\mv}$ is constant) \cite[179]{Aharoni1996}.}
\begin{equation}
  \label{eq:energy-hop}
  \ehop_{,n} = - \frac{1}{2}\ltip{\mv_n}{\hopb{\mv_n}},
\end{equation}
where the factor of $1/2$ is to avoid double counting as discussed in \cref{sec:magnetostatic-field}.
So using \cref{eq:energy-hop,eq:orth-energy} we can write the $\hop$ term of \cref{eq:54} as
\begin{equation}
  \begin{aligned}
    \ltip{\mphop}{\mpdmdt} &= -\frac{\ehop_{,n+1} - \ehop_{,n}}{\dtn}.
  \end{aligned}
  \label{eq:50}
\end{equation}

Next we examine the applied field term of \cref{eq:54}.
Omitting the $L^2$ from the inner products and temporarily writing $\happ(t_i)$ as $\hv_i$ for brevity we have
\begin{equation}
  \begin{aligned}
    \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}
    &= \ip{\hv_{n+1} + \hv_n}{\mpdmdt} \\
    & \qquad - \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}, \\
    &= -\frac{\eapp_{, n+1} - \eapp_{, n}}{\dtn}  + A,
    \label{eq:76}
  \end{aligned}
\end{equation}
where
\begin{equation}
  \begin{aligned}
    A &= \frac{1}{2\dtn}\Big[ 2\ip{\hv_n}{\mv_{n+1}} - 2\ip{\hv_{n+1}}{\mv_n} \\
    & \qquad - \ip{\hv_{n+1}}{\mv_{n+1}} +\ip{\hv_{n+1}}{\mv_n}
    - \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= \frac{1}{2\dtn}\Big[ - \ip{\hv_{n+1}}{\mv_{n+1}} - \ip{\hv_{n+1}}{\mv_n}
    + \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= - \ip{ \frac{\hv_{n+1} - \hv_n}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }. \\
    % &= - \ltip{ \frac{\happ(t_{n+1}) - \happ(t_n)}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }.
    \label{eq:52}
  \end{aligned}
\end{equation}
Finally, we insert the results of \cref{eq:50,eq:76,eq:52} into \cref{eq:54} to find
\begin{equation}
  \begin{aligned}
    \frac{\e_{n+1} - \e_n}{\dtn} = &-\dampc \ltnorm{\mpdmdt}^2
    - \ltip{\frac{\happ(t_{n+1}) -\happ(t_n)}{\dtn}}{\frac{\mv_{n+1} + \mv_{n}}{2}} \\
    &\quad+ \ltip{\happerror}{\mpdmdt}.
    \label{eqn:imr-llg-energy}
  \end{aligned}
\end{equation}
If the applied field is linear in time (and so $\happerror = 0$) this is exactly the midpoint discretisation of the continuous energy balance \cref{eq:energy-decay}.
In particular the energy is conserved when $\dampc = 0$ and the applied field is constant.
This relationship is illustrated in \cref{fig:commutation-imr-energy}.

When $\happ$ is piecewise linear, for example when an applied field is instantaneously switched on, time steps can be selected such that each non-linear change happens exactly at the start/end of a step.
So again we have $\happerror = 0$ and the same properties as for linear applied fields.

Also note that none of the above is applicable to the semi-explicit modification of IMR since in that case $\mphop$ is replaced by an expression of the form $a\hopb{\mv_n} + b\hopb{\mv_{n-1}}$.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 10cm, auto, >=latex']
    \node[block] (start) {Continuous LLG};
    \node[block, right of=start] (discrete) {IMR discretised LLG};
    \node[block, below of=start, node distance = 3cm] (energy) {Continuous energy};
    \node[block, right of=energy] (denergy) {IMR discretised energy};

    \path [->] (start) edge node {Discretise} (discrete);
    \path [->] (start) edge node {Derive energy} (energy);
    \path [->] (energy) edge node {Discretise} (denergy);
    \path [->] (discrete) edge node {Derive energy} (denergy);
  \end{tikzpicture}
  \caption{Commutation relationship between the IMR discretisation and the LLG energy derivation.
In contrast to other time integration methods the result is independent of the order of operations.}
\label{fig:commutation-imr-energy}
\end{figure}


\subsection{Symplecity}

??ds This section will probably not end up in the final thesis, although I'm a bit worried because it may turn out to be important and I don't understand it too well.
However d'Aquino didn't do any experiments on this either, so I can probably get away with skipping it!

The implicit midpoint rule with fixed step size is ``almost symplectic'' for the LLG equation with zero damping, , \ie the property equivalent to Hamiltonian flow for  dissipative systems is conserved up to $\order{??ds}$ \cite{DAquino2005} \cite{Austin1993}.

It is well known that most adaptive schemes are not symplectic \cite[91]{Iserles2009} because they constantly change the ``nearby Hamiltonian'' that is followed by the symplectic integrator.
Therefore the adaptive IMR is not expected to have the almost-symplectic property discussed in \cref{??ds}.

??ds short discussion, future work?

\subsection{Conclusions}

In the previous sections we have highlighted a number of interesting properties of the implicit midpoint rule.
We have a number of reasons to believe that the geometric properties will translate into an improvement in the accuracy and robustness of the overall solver:
\begin{itemize}
\item Errors in the energy dissipation \cite{Albuquerque2001} and magnetisation length \cite{Chantrell2001} have been successfully used as error estimators for local truncation error.
Hence the removal of energy or magnetisation length errors should reduce the total local truncation error.
\item It is well known that geometric integration schemes typically result in much smaller long-timescale error build-up than schemes that do not preserve such quantities \cite[77]{Iserles2009}.
\item The non-linear modification to the Landau-Lifshitz-Gilbert equation caused by renormalisation of the magnetisation length (as commonly used to maintain correct magnetisation length in non-conservative time integrators) may cause significant step changes in the magnetostatic field \cite{Lewis2003}.
Avoiding renormalisation eliminates the possibility of such errors occurring.
\item Renormalisation of the magnetisation modifies the balance between the various energy terms.
This is similar to the methods that lead to the ``flying ice cube'' problem \cite{Harvey1998} in molecular dynamics.\footnote{In such problems the rescaling of particle velocities (to maintain constant temperature despite numerical error accumulation) can result in large amounts of kinetic energy being transferred from the motion of internal degrees of freedom to motion of the centre of mass.}
Again, by avoiding renormalisation we eliminate the possibility of such errors.
\end{itemize}

Additionally, the same geometric properties enable a proof of convergence for IMR when applied to a FEM semi-discretised LLG equation with a slight modification that will be discussed in \cref{sec:nodal-integration} \cite{Bartels2006}.


\section{Linearisation}
\label{sec:linearisation}

When implicit time integration methods are applied to a non-linear differential equation (such as the LLG equation) a non-linear system of equations must be solved.
Explicit time integration methods sidestep the issue of non-linearity by avoiding the need to solve any system.


\subsection{Functional iteration}
\label{sec:picard}

\newcommand{\wconv}{w}

Functional iteration is the process of repeatedly applying a function until the result converges to a fixed point.
To use functional iteration to solve a non-linear system a function is constructed such that its fixed point solves the system.

As an example we can use functional iteration to solve the non-linear system resulting from the IMR discretisation of and ODE which can be written as $\dydt = f(t,y)$.
We first solve for an auxiliary variable $\wconv$, which can be thought of as the approximation for $y$ used in the evaluation of $f$, \ie $\wconv = (y_{n+1} + y_n)/2$, using the iteration
\begin{equation}
  \begin{aligned}
    \nlit{w}{0} &= y_n, \\
    \nlit{w}{i+1} &= \frac{\dtn}{2} f(t_n + \frac{\dtn}{2}, \nlit{w}{i}) + y_n.
  \end{aligned}
\end{equation}
Once the iteration has converged (as determined, for example, by $\abs{\nlit{w}{i+1} - \nlit{w}{i}} < \ntol$) the value of $y_{n+1}$ is calculated as
\begin{equation}
  y_{n+1} = y_n + \dtn f(t_n + \frac{\dtn}{2}, \wconv).
\end{equation}

The main advantage of functional iteration is that convergence proofs can be easily constructed via the Banach fixed point theorem (under some fairly restrictive assumptions on time step size).

Additionally the cost of each iteration can be very cheap if the function $\fv(t, \yv)$ can be written explicitly (\ie the ODE is not implicit in $\dydt$), since it only requires the application of a function rather than a linear solve required by the Newton-Raphson method.
This is not the case for the LLG \cite{Bartels2006}, but still the linear systems resulting from functional iteration are simpler than those resulting from other methods, which simplifies the construction of an efficient linear solver.

However functional iteration only converges for time steps of a limited size.
This limit is related to the fineness of the spatial discretisation, resulting in similar problems to explicit methods.
This makes the method useful for applying implicit methods to non-stiff problems, but less useful for stiff ones \cite{Iserles2009}.


\subsection{Newton-Raphson method}
\label{sec:newt-raph}

??ds not quite right, lots of red from Matthias
A non-linear problem:
\begin{equation}
  \begin{aligned}
    \text{Find $\yv$ such that } \fv(t, \yv, \ldots) = \gv(t, \yv, \ldots).
  \end{aligned}
\end{equation}
can also be stated as an equivalent minimisation problem:
\begin{equation}
  \label{eq:non-lin-system}
  \begin{aligned}
    \text{min}_{\yv} \resi(t, \yv, \ldots).
  \end{aligned}
\end{equation}

In the context of implicit time integration of a semi-discretised PDE we write
\begin{equation}
  \yvdis_n = \set{\yv_{n,i}}_{i=0}^{i=N},
\end{equation}
for the list of spatially discrete values of $\yv$ at time step $n$.
For example, for an LLG problem consisting of a single macrospin $\yvdis_n = \mv_n$.
For a finite element semi-discretised LLG problem $\yvdis_n$ is the list of all the discrete values that define $\mv_n$ along with any values needed for the magnetostatic calculations.
Then the non-linear problem can be written as
\begin{equation}
  \text{Find $\yvdis_{n+1}$ such that } \resi(t, \yvdis_{n+1}, \yvdis_n, \yvdis_{n-1}, \ldots) = 0,
\end{equation}
where the number of previous values of $\yvdis$ required depends on the time integration scheme.

The motivation for the Newton-Raphson method comes from a simple Taylor expansion of the residual, $\resi$.
If the exact root of the residual is $\nexty$ and we have an initial guess for this root $\nlit{\yvdis}{0}$ then we can obtain a correction to this initial guess from:
\begin{equation}
  \begin{aligned}
    \resi(\nexty) &= 0, \\
    &= \resi(\nlit{\yvdis}{0} + \pm), \\
    &= \resi(\nlit{\yvdis}{0}) + \jac(\nlit{\yvdis}{0}) \cdot \corr + \order{\corr^2},
  \end{aligned}
\end{equation}
where $\jac(\nlit{\yvdis}{0}) = \evalatb{\pd{\resi}{\yvdis}}{\nlit{\yvdis}{0}}$ is the Jacobian matrix for the residual.
So
\begin{equation}
  \begin{aligned}
    \label{eq:49}
    \nexty &= \nlit{\yvdis}{0} + \corr, \\
    &= \nlit{\yvdis}{0} + \jac^{-1}(\nlit{\yvdis}{0}) \cdot \resi(\nlit{\yvdis}{0}) + \order{\corr^2}.
  \end{aligned}
\end{equation}
This suggests the iteration
\begin{equation}
  \nlit{\yvdis}{i+1} = \nlit{\yvdis}{i} - \jac^{-1}(\nlit{\yvdis}{i}) \cdot \resi(\nlit{\yvdis}{i}),
\end{equation}
in which, if the Taylor expansion is valid and that we can discard the higher order terms, the error is \emph{squared} after each iteration (\ie convergence is quadratic).

The assumption that we can discard terms in $\order{\corr^2}$ is fairly strong: it means that we need a good initial guess so that the initial correction, $\corr$, is sufficiently small.
Fortunately in time integration problems the value at the previous time step provides a good initial guess: $\nlit{\yvdis}{0} = \yvdis_n$.
It is also possible to construct an even better initial guess by taking a single cheap explicit ``predictor'' step, as used in Milne's device for adaptive time integration (and if such an adaptive scheme is in use then this improved initial guess is ``free'').

The iteration is terminated when
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \ntol,
\label{eq:newton-convergence-test}
\end{equation}
for some user defined tolerance $\ntol$.
Alternatively the absolute tolerance condition \cref{eq:newton-convergence-test} can be replaced by or combined with a relative tolerance condition
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \norm{\resi(\nlit{\yvdis}{0})}_{\infty} \nrtol.
\end{equation}
If the residual expression is well normalised, and so has initial magnitude of $\order{1}$, then the two are (roughly) equivalent.

% For micromagnetics problems we find that this reliably converges to a tolerence of $\sim 10^{-10}$ in around 2-3 steps!

As discussed in \cref{sec:picard} the main downside of the Newton-Raphson method is that it always requires the solution of a system of linear equations to obtain each correction $\corr$, and that these systems are more complex than those produced by function iteration.
The efficient solution of these systems is the subject of \cref{sec:solution-lin-sys}.

Another potential downside is that the iteration is only guaranteed to converge if the initial guess ``good enough''.
However for non-linear systems resulting from time stepping we always have a very good initial guess, so this is not often a problem.


\subsection{Advanced LLG-specific linearisation methods}
\label{sec:advanced-lin}

??ds clean up or remove?

Linearisation based on choice of test functions of much interest in recent times, initially for only exchange effective field \cite{Alouges2008}.
Has recently been extended to handle general effective field contributions \cite{Banas2012}.

Advantages:
\begin{itemize}
\item Only need to solve one linear system per step (rather than $\sim 2$ for Newton's method).
\item Jacobian matrix is constant.
\end{itemize}

Disadvantages:
\begin{itemize}
\item Method is limited to first order: so need step sizes orders of magnitude smaller.
\item Have to use specialised time integration scheme (no geometric, adaptive, ... schemes).
\item Specific to the LLG equation: standard code libraries may not work, not as widely understood as standard FEM, extension to related equations (stochastic LLG, LLGS, LLB) is non-trivial.
\end{itemize}

The limitation to first order has been worked around recently, however the higher order schemes lose either stability or linearity \cite{Kritsikis2014}.
Since the advantage of the scheme over standard methods was this unique combination of properties, this appears (to me at least) to not be very useful.

Still a promising line of research though!



\section{Solution of linear systems}
\label{sec:solution-lin-sys}

??ds whole section needs more work

If we are using an implicit time integration scheme with the Newton-Raphson method for linearisation then we are left with the problem of solving a sequence of sparse linear systems.
The problem can be stated as: Given a sparse $n \times n $ matrix $\Am$ and a vector $\bv$, find $\xv$ such that
\begin{equation}
  \label{eq:linear-system}
  \Am \xv = \bv.
\end{equation}
This is a very widely studied problem, see for example \cite{Saad2000}, but efficient techniques for very large $n$ (which corresponds to a large spatial problem and/or good spatial resolution) are complex and problem dependent.


\subsection{Direct methods}
\label{sec:direct-methods}

??ds expand

Construct exact solutions to linear systems of equations

Main example is LU decomposition.

The main advantage of direct methods is their robustness: given a well-scaled, non-singular linear system a well designed direct solver will be able provide an answer.
As such direct solvers are usually the first approach tried when solving a new system of equations.

??ds talk about 3d vs 2d: in 3d there is much more coupling (more overlap of test functions): so larger bandwidth and much slower direct solves

However as matrix sizes become large direct solvers become increasingly demanding in both time and memory.
The problem is that the factors of a typical sparse matrix are significantly more dense than the starting matrix, \ie there is some amount of fill-in.
This means that the time and memory requirements for the solution can never scale as $\order{n}$.
The exact level of fill-in depends on the sparsity pattern of the initial matrix, but is typically somewhere in the range $\order{n^{3/2}}$-$\order{n^2}$.


\subsection{Multigrid methods}

??ds need to talk about how it's actually the correction we solve for on the coarser grids...

There are a number of ``simple'' iteration methods which are fairly ineffective when used directly to solve reasonably sized linear systems, but are very cheap to apply since they can be written in a form where each iteration is a matrix vector product.
??ds conditioning is the problem not size (Milan)
Examples are Jacobi, Gauss-Seidel and SOR methods \cite[103]{Saad2000}.

Multigrid methods are a class of linear solvers which combine these simple iteration methods with a hierarchical algorithm, resulting in highly efficient and scalable solvers for the solution of discrete elliptical PDEs \cite{multigrid-tut}.
They are based on two observations: firstly that the simple iterative solvers are very effective at reducing the high-frequency (in space) error of a solution, but fail on the low frequency parts.
The second observation is that many of the low frequency errors can be converted to high frequency errors by projecting them onto a coarser grid and solving for a correction to the solution.
So by solving the problem on a sequence of meshes all frequencies of error can be treated effectively (except the very lowest, which can be removed by a direct solve on a very coarse mesh).

Typical multigrid solvers work use something similar to the following algorithm
\begin{itemize}
\item Start with initial guess on finest mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh \\
  $\vdots$
\item On the coarsest mesh solve the solution directly
\item Project the solution onto a finer mesh
\item Apply a few iterations of a simple solver \\
  $\vdots$
\item End with solution on finest mesh
\end{itemize}
The details of the ``simple'' iterative solver, the projection between meshes, the selection of coarse meshes, the exact order/number of repeats of the cycle etc. all vary between methods.

??ds Milan wants an image here

It turns out that multigrid solvers can achieve computation times which scale as $\order{N \log N}$, \ie their performance does not severely degrade as the number of nodes in the mesh is increased.
??ds memory requirements?

Multigrid methods are very effective on the matrix resulting from discretising a Poisson problem.
Fortunately the Laplace operator (\ie the core of a Poisson problem) is in some way the ``most important'' part of many elliptic PDEs since it contains the highest derivatives.

In fact it often turns out that multigrid methods are most effective when combined with another type of solver, as will be discussed below.


\subsection{Krylov solvers}
\label{sec:krylov-solvers}

??ds expand

Another class of iterative linear solvers are the Krylov solvers.
These methods are based on building an approximation to the solution from linear combinations of the Krylov subspace basis set of the matrix $\Am$.

Based on Krylov subspace (polynomial of $\Am$ multiplied by $\xv$).

The size of the Krylov subspace needed for an effective approximation (\ie the number of iterations needed) is strongly dependent on the properties of the matrix, in particular the eigenvalues and eigenvectors.
So for optimal performance a ``preconditioner'' is needed, which brings the eigenvalues of the matrix close to 1.
Essentially the preconditioner needs to be a reasonable but cheap to calculate approximation to the inverse of the matrix (since $\Am^{-1}\Am = \Idm$, and the identity matrix is the matrix with all eigenvalues 1).
The construction of a preconditioner which is effective independent of the mesh size is a difficult and problem dependent task.

% Some work has been done on preconditioning for the LLG by Suess et. al. \cite{Suess2002}.
% Also Banas et. al. \cite{Banas2008} \cite{Banas2010} created a multigrid preconditioner for the Maxwell-LLG equation with a fixed point linearisation method (and their time step sizes are a factor of 20 smaller than a similar algorithm using a Newton-Raphson linearisation on the same problem).

Difference between CG, GMRES, BiCGStab:

For symmetrical matrices, $\Am$, the construction of orthogonal Krylov subspace basis vectors (??ds) can be done using a only the last 3 ??ds values of $\Am \bv_i$, this results in CG ??ds also choice of minimisation direction?
For non-symmetrical matrices ALL of the previous values must be stored: GMRES ??ds different direction too.
This means that the cost per iteration of GMRES grows with the number of iterations, can avoid this by using restarted GMRES (throw away history after x steps) at the cost of convergence guarantees.

Other way to avoid this cost is to use BiCGStab ??ds


\subsection{Preconditioners}
\label{sec:preconditioners}

??ds make sure you mention that preconditioner must be same operation at each step


\subsection{Linear solvers in micromagnetics}
\label{sec:linear-solv-micr}


??ds discuss linear solvers used: GMRES from CVODE mostly,

mention BEM block?
Aquino: unpreconditioned GMRES.
Suess: ILU preconditioned BiCGstab, used as prec for GMRES solve with BEM block, most people cite this paper.



\section{Compatibility of methods}
\label{sec:comp-meth}

While in theory any of the methods discussed above can be combined with any other, there are certain combinations which are problematic.
In \thisref{sec:comp-meth} we discuss which combinations of methods are particularly compatible or incompatible.

The use of integral methods for magnetostatic field calculations with implicit time integration is problematic: the integral formulation couples magnetisation at every point to every other point, meaning that the Jacobian representing the derivatives of each magnetisation with respect to each other magnetisation is a full matrix (and hence solution times are extremely slow)!
Explicit time integration methods avoid this issue because no system solve is required.

For problems with very simple geometry (\ie thin films, cuboid shapes) the finite difference method with FFT for magnetostatic calculations is sufficient.
Additionally: for simple geometry there is often no need for well refined meshes, so the problem can be only mildly stiff and explicit time integration methods can be efficient.
This combination of methods is very simple to implement and so is often used for GPU based code \cite{Vansteenkiste2011}.
The most well known example of such a model is \oommf \cite{oommf-website}.

For problems with more complex geometry the finite element method must be used for spatial discretisation to obtain good accuracy.
Magnetostatic calculations on the resulting non-uniform mesh are typically carried out using FEM/BEM (\nmag \cite{Fischbacher2007}, \magpar \cite{Scholz2003}, \femme, \ldots) although fastmag uses a non-uniform FFT approach \cite{Chang2011}.
The use of refined meshes to fully resolve the geometry can often result in stiff problems, requiring the use of implicit time integration schemes for good efficiency (see \cref{cha:stiffn-llg-equat} or \cite{Shepherd2014}).
In all ``real world'' models using implicit time integration known to the author the Newton-Raphson method is used for linearisation.
Typically some form of preconditioned Krylov solver is then used to solve the resulting linear systems.


\subsection{Solvers and geometric integration}

It should be noted that the geometric properties of IMR derived in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} are only true up to the accuracy with which we solve the discrete LLG.
This is the tolerance of the linearisation method used if the problem is non-linear or the tolerance of the iterative solver if it is linear.

Additionally the derivations in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} assume that all fields are calculated at the midpoint without any additional approximations, \ie that we are using a fully implicit method.
In contrast to this, many existing models use an explicit approximation to the magnetostatic field in order to avoid including a dense contribution due to the long-range magnetostatic interactions\footnote{At least a dense sub-block arises from all accurate magnetostatic calculation methods known to the author, in particular: FEM/BEM, FFT and FMM. See \cref{sec:magn-field-calc} for more details.} in the non-linear solve.
Unfortunately, in this case the energy properties will be lost due to the replacement of $\hopb{\mpm}$ by an alternative approximation (in a similar way to the semi-explicit modification of IMR).
However, the magnetisation length property will be retained because it only relies on $\mv$ itself being calculated implicitly, and not the effective field.

% ??ds goes in somewhere?
% However when using this method the adaptive time step calculation must be done specially (\ie not using normal adaptivity) since the system of equations contains no information on the magnetostatic field \cite{Schrefl1997}.
% Also stability is no longer guaranteeded because an ad-hoc modification to the time integration method.

One way to allow a completely implicit treatment of the magnetostatic field without a dense contribution was explored by d'Aquino \cite{DAquino2005}.
He used a quasi-Newton method where the dense contribution was simply left out of the Jacobian matrix.
This greatly slows down the convergence of the non-linear solve: in the example given in the paper 10-14 iterations were needed (compared to $\sim2$ with a full Newton method).
Additionally, in the general case it is not clear that this modified Newton method will converge at all!

In \cref{sec:fully-implicit-bem} we describe and demonstrate a novel efficient and robust solver allowing fully implicit magnetostatic calculations.


\section{Conclusions}
\label{sec:num-meth-conclusions}

In this \thisref{sec:numer-meth-micr} we have examined a number of numerical methods for each part of overall PDE solver.
Now we need to decide which methods to use.

For the space discretisation we favour FEM for it's ability to easily handle non-trivial geometries.

For the magnetostatic calculations we have chosen FEM/BEM: in general the potential methods intergrate better with FEM, and FEM/BEM is the most accurate of these.
Additionally the non-optimality of the BEM matrix can be handled to a large extent by using a hierarchical matrix representation (and such methods are well developed).

For time integration explicit methods are out of the running because they do not scale well with spatial refinement.
The BDF methods are also discarded because of the oscillatory nature of LLG dynamics and the spurious damping they cause.
This leaves TR and IMR: IMR has superior geometric integration properties and can be proven to converge but does not have an adaptive time integration algorithm.
In our research we have addressed this shortcoming, the details are given in \cref{sec:adaptive-imr}, and so IMR is now the superior choice.

For the non-linear solver the (full) Newton-Raphson method is generally superior because it does not limit the time step size like picard iteration and it has significantly faster convergence than a quasi-Newton method.
However the convergence proofs attainable with a fixed point iteration may be useful in some extreme cases, such as when the solution undergoes ``blow-up'' \cite{something by numerical analyst guys}.
Additionally a fixed point iteration may be useful if the geometric integration properties of IMR are required for a non-stiff LL problem.

For the linear solver we aim to use a preconditioned Krylov method using a multigrid based preconditioner for $\order{N}$ scaling.
However an efficient mesh-independent preconditioner for the final linear system has not yet been found.
In our research we have made some progress towards this goal, see \cref{sec:solution-strategies} for details.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
