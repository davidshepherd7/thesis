\chapter{Numerical Methods for Dynamic Micromagnetic Modelling}
\label{sec:numer-meth-micr}

In this Chapter we give an overview of numerical methods that have previously been used in dynamic micromagnetic calculations.
These calculations involve solving some form of the LLG equation \cref{eq:LLG} with effective fields determined by energy derivatives as discussed in \cref{sec:cont-micromag}.
These systems of partial differential equations (PDEs) can only be solved analytically in a few extremely simple cases\footnote{For example the Stoner-Wolfarth theory for the rotation of a single grain \cite{Stoner1948a}.} \cite{Aharoni1996}, so numerical solution methods are almost always necessary.

Many numerical methods for solving non-linear PDEs, such as \cref{eq:LLG}, can be thought of as a combination of various component methods, each of which handles a different part of the conversion from a continuous PDE into an algorithm which can be performed by a computer\footnote{Technically proofs of convergence etc. should be carried out for every different combination of time and space discretisation \cite[382]{Iserles2009}; in practice surprises seem to be rare, at least within micromagnetics.}.

The essential components of a numerical method for solving PDE problems are:
\begin{itemize}
\item Spatial discretisation: convert space derivatives into algebraic relationships between discrete points in space, \eg finite elements, finite differences, macrospin models. 
\item Time discretisation (a.k.a. time integration):  convert time derivatives into algebraic relationships between points in time, \eg Runge-Kutta methods, backwards difference formulae.
\end{itemize}
For some choices of time and space discretisations additional component methods are needed:
\begin{itemize}
\item Linearisation procedure: Solve a system of non-linear algebraic equations, often by an iterative procedure which requires the solution of a sequence of systems of linear algebraic equations, \eg Newton-Raphson method, Picard/fixed-point iteration. 
\item Linear solver: solve a system of linear algebraic equations, \eg LU decomposition, Krylov solvers, multigrid methods. 
\end{itemize}

Additionally in micromagnetic modelling the calculation of the magnetostatic field is required.
If the integral form given in \cref{eq:Hmsint} is used then an additional integral evaluation method is needed to handle it.
Alternatively, if the equivalent potential formulation \cref{eq:Hms,eq:cont-phi-bound} is used then the magnetostatic field calculation simply requires solving additional PDEs.

It should be noted that not all methods fit into this framework of applying separate methods for each part of the problem, see \cref{sec:advanced-lin} for one example.


\section{Spatial Discretisation}
\label{sec:spat-discr}

\subsection{Macrospin models}
\label{sec:sd-macrospins}

In a granular magnetic material (a material consisting of magnetic grains separated by a non-magnetic material) the simplest way to handle spatial variation in the problem is to assume that within each grain the exchange effective field is so strong that the magnetisation is constant, \ie that each grain behaves as a single \emph{macrospin}.
We assign a single value of $\Mv$ to each macrospin and proceed to calculate energy, effective field and/or magnetisation of each macrospin as required.
One caveat is that the effects of the magnetostatic field of a grain on the grain itself is not automatically accounted for since there is no modelling of intra-grain effects.
Hence it must be calculated and included separately to the magnetostatic interactions between grains.
When included in this way the magnetostatic self field is often called the \emph{shape anisotropy} since it is dependent on the shape of the grain and acts in a similar way to the magnetocrystalline anisotropy. 

This macrospin approach may be used with any system in which there are a number of ``small'',\footnote{All dimensions of the bodies must be much smaller than the exchange length so that all magnetisation within the body is approximately parallel.} separate magnetic bodies with approximately uniform magnetisation inside the body.

The obvious downside of a macrospin approach is that it only applies to a fairly specific geometry, although the case of a granular media has been of much interest for magnetic data storage. 
Additionally, if there are nonuniformities in magnetisation within the regions where it has been assumed constant the model may be inaccurate.

However it is often simpler to construct a macrospin model than to use the more general methods described in \crefs{sec:sd-finite-diff-meth, sec:sd-finite-elem-meth}.
Also the assumption that each grain has uniform magnetisation will reduce the number of calculations needed.

Closely related to macrospin models are atomistic models \cite{Evans2014}.
In these models the assumption of a continuous magnetisation $\mv(\xv, t)$ is dropped in favour of an assumption that each atom is the location of a single macrospin.
As such they allow modelling of some systems, such as anti-ferromagnetic materials, that are inaccessible using micromagnetics.
However, the required spatial resolution comes with a large increase in computational cost.


\subsection{Finite Difference Methods}
\label{sec:sd-finite-diff-meth}

Another method of spatial discretisation is the finite difference method: a single magnetisation vector is assigned to each point on (or ``cell'' in) a simple square/cubic grid which covers the entire domain.
Spatial derivatives in the PDE are then approximated by using truncated Taylor series expansions.

The finite difference method works well for very simple geometries when the grid can be lined up with all geometric features.
For example when we are interested in how the magnetisation evolves over time in a non-granular cuboid-shaped thin film of magnetic material a finite difference method will be sufficient (\eg in the $\mu$mag standard problems \cite{mumag-website}).

However when the geometry involves curves, diagonals, hexagonal grains, bit patterned media or any other more complex geometric system other methods are better suited.


\subsection{Finite Element Methods}
\label{sec:sd-finite-elem-meth}

A more complex method of spatial discretisation is the finite element method \cite{HowardElmanDavidSilvester2006}.
Here the magnetic body is divided up into a finite number of non-overlapping polygonal \emph{elements}, which can vary in both size and shape.
Within each of these elements each magnetisation component is approximated by a simple polynomial function of space, typically a linear polynomial.
Spatial derivatives are calculated by simply differentiating these polynomial functions.
More details of the finite element method are given in \cref{sec:intr-finite-ele-diff}.

The main advantage of the finite element method is that it can accurately approximate any geometrical feature by an appropriate arrangement of the polygonal elements.
An additional advantage is that the size of elements (and thus the accuracy of the approximation) can be varied arbitrarily as needed to give better accuracy in more complex or important regions. 
The choice of element size can be done automatically using \emph{adaptive mesh refinement}: after each calculation an error estimate is calculated (known as \emph{a posteriori} error estimation).
If the error is determined to be too high anywhere the mesh is refined near that region and the calculation is repeated.
Hence, given the desired error and a method to estimate the error, a mesh giving an efficient and accurate approximation can be automatically generated \cite{Schrefl1999}.

The major downside of finite element models is that the underlying mathematics is more complex than that required for other methods, especially finite difference methods.
Also the set up time and memory usage can be greater because of the additional ``bookkeeping'' required to keep track of the more complex meshes.


\section{Magnetostatic Field Calculations}
\label{sec:magn-field-calc}

Methods for the calculation of magnetostatic fields belong to two categories: integral and potential methods.
Integral methods are based on efficiently approximating the very large number of integrals required by \cref{eq:Hmsint}.
Potential methods are based on the use of the potential form \cref{eq:Hms} with standard spatial discretisation methods.
However special treatment is required for the boundary condition on the potential at infinity \cref{eq:cont-phi-bound}.

In this section we use the term ``magnetic charges'' to mean the macrospins/cells/nodes as appropriate for the spatial discretisation.
We use $N$ the number of such charges.

% More recently models have been developed which extend the magnetic field calculations to include all of Maxwells equations (the electric field and its effects on the magnetic field as well).
% Such models greatly increase the number of degrees of freedom in the problem: 3 magnetisation components, 3 magnetic field components and 3 electrical field components are needed, whereas for magnetostatic calculations only the magnetisation and possibly up to 2 potentials are needed.
% Since, for not seem to have a substantial effect on the results for many problems of interest \cite{??ds} we will not consider these full Maxwell models further.

\subsection{Integral Methods}
\label{sec:magstat-field-calc-inte}

% \begin{figure}
%   \centering
%   \begin{tikzpicture}[level 1/.style={sibling distance=5.4cm},
%     level 2/.style={sibling distance=3.6cm}]

%     \node[block] {\textbf{Magnetostatic Calculations}}
%     child {node[block,text width=6cm] {Scalar Potential Formulation (with some spatial discretisation)}
%       child{node[block,text width=4cm,xshift=-1cm] {Asymptotic Boundary Conditions}}
%       child{node[block,text width=4.3cm] {Hybrid Finite/Boundary Element Method}}
%     }
%     child {node[block,yshift=-2.7cm] {Integral Formulation}
%       child{node[block,text width=3.2cm] {Full Calculation}}
%       child{node[block,text width=3.2cm] {Fast Fourier Transform}}
%       child{node[block,text width=3.2cm] {Fast\\ Multipole\\ Method}}
%     };
%   \end{tikzpicture}
%   \caption{Methods of magnetostatic field calculation that have been used in micromagnetic models.}
%   \label{fig:types-mag-stat}
% \end{figure}

Integral methods are based on the expression
\begin{equation}
  \Hms(\xv) = \frac{1}{4 \pi} \bigs{ 
    - \int_\magd \nabla' \cdot \Mv(\xv') \frac{(\xv - \xv')}{\abs{\xv -\xv'}^3} \d^3 \xv'
    + \int_\boundd \Mv(\xv') \cdot \nv(\xv') \frac{(\xv - \xv')}{\abs{\xv - \xv'}^3} \d^2 \xv' }.
  \label{eq:Hmsint2}
\end{equation}

After the application of a discretisation scheme the integrals in \cref{eq:Hmsint2} become a sum over all nodes.
The naive way to calculate the magnetostatic fields would then be to work through the list of nodes calculating the field at each of them, so for each node a sum of the contributions from all other nodes must be calculated.
Hence this results in a computation time that scales as $\order{N^2}$, which is usually unacceptably slow for reasonably large $N$.


\subsubsection{Fast Fourier transform methods}

The fast Fourier transform method (FFT) is a simple and efficient method for calculating the magnetostatic field when the magnetic charges are positioned on a regular lattice.

The calculation of $\Hms$ in \cref{eq:Hmsint} can be thought of as applying a convolution operator (\ie $\Hms = D \big[\Mv\big]$).
The matrix corresponding to this operator is only dependent on the geometry, hence it can be precomputed and its Fourier transform precalculated.
Then all that is needed to calculate the magnetostatic field is to apply a Fourier transform to $\Mv$, compute the convolution and transform the result back into the time domain by applying the inverse Fourier transform.
Because of the regularity, applying the convolution in the frequency domain is very fast and hence the complexity of the calculation is limited by the complexity of a fast Fourier transform.
This results in an overall complexity of $\order{N \log(N)}$ \cite{Jones1997}.

The downside of this method is that points to be calculated must be on a regular lattice, similar to the finite difference method.
Hence, it is most useful in combination with models using a finite difference spatial discretisation.

Alternatively the FFT may be used as part of a method applicable to less regular meshes.
In such ``non-uniform FFT'' methods distant charges are approximated by charges on a regular lattice so that an FFT method can be applied while the effects of nearby charges are evaluated directly \cite{Jones1997}.


\subsubsection{Fast multipole method}
\label{sec:fast-mult-meth}

An alternative method of calculation of the magnetostatic field is the fast multipole method (FMM) \cite{Beatson1997}.
The fast multipole method takes advantage of the fact that distant magnetic charge has a much smaller effect on the total field at a point than a nearby magnetic charge.

For the field calculation at a specific point the full calculation is only performed for nearby magnetic charges.
Groups of more distant charges are approximated (lumped) as a single multipole placed at the centre of the group.
As the charges become more distant they contribute much less to the field due to the $\frac{1}{(\xv - \xv')^2}$ scaling in \cref{eq:Hmsint2}.
Hence for distant points less accurate but cheaper approximation can be used without reducing the accuracy of the final result.

The trick for quickly calculating fields at a large number of points is to pre-calculate multipole approximations for a range of accuracies over all space.
Then the calculation of a field at a single point only requires the full calculation of effects from a few nearby points and from the appropriate multipoles.

One advantage of this method over the fast Fourier transform is that it allows for arbitrary geometries.
Also the complexity of the method is $\order{N}$ \cite{Chang2011}, so FMM is more efficient than FFT for sufficiently large problems (although ``sufficiently large'' may in fact be very large due to the cost of precomputing the required multipole approximations).


\subsection{Scalar potential methods}
\label{sec:magstat-field-calc-pote}

Potential methods are based on the formulation:
\begin{equation}
  \begin{aligned}
    \label{eq:Hms2}
    \hms &= - \nabla \phim, \\
    \lap \phim &= \nabla \cdot \mv,
  \end{aligned}
\end{equation}
with boundary conditions
\begin{equation}
  \begin{aligned}
    \label{eq:cont-phi-bound2}
    \phim^\inte - \phim^\exte &= 0 \quad \xv \in \boundd, \\
    \pd{\phim^\inte}{\nv} - \pd{\phim^\exte}{\nv} &= \mv \cdot \nv \quad \xv \in \boundd, \\
  \end{aligned}
\end{equation}
and
\begin{equation}
  \begin{aligned}
    \label{eq:phi-inf-bound}
    \phim \rightarrow 0 \text{ as } &\abs{\xv} \rightarrow \infty.
  \end{aligned}
\end{equation}

??ds Milan wants an image, not sure what of...

This is a differential equation and so inside the domain, $\magd$, it can be solved simply by applying the finite element or finite difference discretisation methods.
However \cref{eq:phi-inf-bound}, the boundary condition on $\phim$ at infinity, is problematic.
We obviously can not apply this condition directly using standard methods, since that would involve either an infinite number of elements or infinitely large elements.
Hence other techniques must be used, such techniques are the subject of the rest of this section.

The speed of the internal field calculation is $\order{N}$ since it is just a standard FEM or FD calculation, however applying the boundary conditions can require additional processing time.


\subsubsection{Asymptotic boundary conditions}
\label{sec:asymptot-bcs}

One way to avoid an infinite domain is to truncate the external region at some finite distance from the magnetic domain.
A similar but more sophisticated and accurate approach is to use asymptotic boundary conditions \cite{Yang1997}.
The idea here is to use a truncated external region to calculate the boundary conditions on the magnetic domain that correspond to \cref{eq:phi-inf-bound} being applied at infinity.
Additionally the fact that any solution to the Poisson equation~\cref{eq:Hms2} can be represented as an infinite series of harmonic functions is used to improve the accuracy.

Unfortunately the accuracy of this approach is still quite low, even for large truncation distances \cite{Bottauscio2008}.


\subsubsection{The hybrid finite element method/boundary element method}
\label{sec:bound-elem-meth}

The idea of the hybrid finite element method/boundary element method (FEM/BEM) is to replace the external domain by a dipole layer placed on the surface of the magnetic domain which mimics the effect of the infinite external domain \cite{Fredkin1990}.
This removes the need to truncate or discretise the infinite external domain.
The name comes from the similarity of the use of a dipole layer with the boundary element method, and the fact that the standard finite element method is used to calculate the required potentials in the bulk.
The full details of the method applied to magnetostatic calculations is discussed in \cref{sec:hybr-finit-elem}.

A comparison by Bottauscio \cite{Bottauscio2008} found that using the FEM/BEM method was more accurate than applying asymptotic boundary conditions (ABCs) for a calculation of the time evolution of the magnetisation of a sphere with zero exchange coupling.
Even with a truncation distance of four times the size of the magnetic sphere (the total domain was $4^d$ times larger then the sphere) the accuracy when using asymptotic boundary conditions was worse and did not improve between truncation distances of three and four times the magnetic sphere radius.
Even when exchange coupling was added (giving an easier test) the ABC method was worse than the FEM/BEM method.

The main downside of the FEM/BEM method is that it involves dense matrix of size $N_b \times N_b$ where where $N_b$ is the number of boundary nodes.
This matrix must be stored in memory, and multiplied by a vector for the calculation of the boundary conditions.
The speed of calculation of the boundary values in the method is limited by the dense matrix multiplication which is $\order{N_b^2}$.
Similarly, the additional memory usage is $\order{N_b^2}$, which can limit the size of problems.
Hence the speed of the FEM/BEM method depends on the geometry.

The use of hierarchical matrix techniques can reduce this to $\order{N_b \log(N_b)}$ \cite{Knittel2009}.
In 3D structures which are roughly spherical $N_b = \order{N^{2/3}}$ and so the use of a hierarchical matrix gives optimal computation speed\footnote{With hierarchical matrix techniques the speed is $\order{N^{2/3}\log(N^{2/3})}$ but $\log(x) \ll x^{1/2}$ for large $x$, hence $\order{N^{2/3}\log(N^{2/3})} \ll \order{N}$, \ie optimal computation speed scaling.} but for extremely flat structures the number of boundary nodes can be as bad as $N_b = \order{N}$.

Another downside of FEM/BEM is the increase in the complexity of the model: some components of the FEM/BEM method are fundamentally different to FEM.
As such substantial additional code and mathematical knowledge may be needed for its implementation when standard FEM libraries are used as a basis.
As an example: singular integrals occur in the boundary element method so more advanced integration methods are needed than for FEM alone.

The details of the FEM/BEM method and it's implementation are discussed in \cref{sec:hybr-finit-elem}.


\section{Time Integration}
\label{sec:time-discretisation}

Time integration schemes are used to convert the time derivatives of a PDE into a form that can be handled computationally. 
When discussing time integration schemes it is helpful to use a (vector) initial value ordinary differential equation as an example:
\begin{equation}
  \begin{aligned}
    \frac{d \yv}{dt} &= \ffv{t, \yv}, \\
    \yv(0) &= \yv_0,
    \label{eq:45}
  \end{aligned}
\end{equation}
where $\fv(t,\yv)$ is a known function and $\yv_0$ is the (known) initial condition.
After the application of one of the spatial discretisations described above a PDE is typically reduced to the form \cref{eq:45}, this sometimes is called a \emph{semi-discretised} PDE.

The time integration methods discussed here all bear a strong similarity to the finite difference method discussed in \cref{sec:sd-finite-diff-meth}, except that the independent variable discretised is time instead of space.
This is appropriate since time is one-dimensional, hence no complex geometry is possible and so typically there is no need for the more advanced finite element method.

Some key attributes of a time integration scheme are \cite{Atkinson2009}:
\begin{itemize}
\item \textbf{Accuracy (order)} -- An estimate of how rapidly the approximate solution converges as the time step size is reduced.

\item \textbf{Stability} -- Roughly speaking a scheme is stable if the approximate solution does not ``blow up'' (\ie become catastrophically inaccurate, infinite, oscillate in non-physical ways, \ldots) for sufficiently large time steps (see \cref{sec:A-stability} for a more rigorous description).

\item \textbf{Conservation properties} -- Some differential equations have properties which should ideally be conserved in the discretised system.
For example, certain important properties of the Landau-Lifshitz-Gilbert equation discussed in \cref{sec:prop-cont-llg}.
Schemes which conserve such properties are often referred to as ``geometric'' integrators.

\item \textbf{Self-starting} -- A scheme is self starting if it only requires a single initial value.
This is desirable because methods of estimating additional initial values make the final scheme more complex and may introduce additional errors.
\end{itemize}

These attributes are discussed more rigorously in the rest of this section.

\subsection{Explicit and implicit time integration schemes}
\label{sec:explicit-vs-implicit-schemes}

Explicit time discretisation schemes calculate the value at the next time step in terms of the value(s) at the present and/or previous time steps.
The simplest such scheme is the \emph{(forward) Euler method}
\begin{equation}
  \label{eq:44}
  \yv_{n+1} = \yv_n + \dtn f(t_n, \yv_n),
\end{equation}
where $\dtn = t_{n+1} - t_n$ is the $n$-th time step size and $\yv_i$ is the approximation to $\yv(t_i)$.
Clearly, given $f(t,y)$ and an initial value for $y(t_0)$ we can solve for $y(t_n)$ for any $n$.

In contrast to explicit schemes, implicit schemes use the value at the next time step in its own calculation.
Hence at each step a (possibly non-linear) system of equations must be solved.
The simplest implicit scheme is the \emph{backward Euler method} (BDF1)
\begin{equation}
  \label{eq:bdf1-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{t_{n+1}, \yv_{n+1}}.
\end{equation}

At first glance it seems that the additional solution of a system of equations required for a step of an implicit scheme mean would that explicit schemes are more efficient, and it is true that one step of an implicit method requires more computational effort than a step of an explicit method.
However all explicit time integration schemes suffer from issues of limited stability for some problems, and so are forced to use time step sizes much smaller than would be required for reasons of accuracy (see \cref{sec:A-stability} for details).
In these cases implicit schemes can be much more efficient, problems where this is the case are often called ``stiff'' problems.
The semi-discretisation of a PDE often results in a stiff problem.
In \cref{cha:stiffn-llg-equat} we investigate stiffness in micromagnetics problems.


% Micromagnetics solvers for non-stiff systems commonly use the RK4 (fourth order Runge-Katta) method \cite{Suess2002}.
% Micromagnetics models commonly use BDF schemes of various order \cite{Suess2002} or the implicit midpoint rule \cite{DAquino2005} for the modelling of stiff systems.


\subsection{Some implicit time integration schemes}
\label{sec:some-implicit-time-integrators}

For the remainder of this section we will use three time integration schemes with good stability and accuracy properties that are widely employed in practice as examples.
All three are implicit schemes for reasons of stability, as mentioned in \cref{sec:explicit-vs-implicit-schemes}.

\emph{Trapezoid rule} (TR) is the average of the forward and backward Euler methods from \cref{sec:explicit-vs-implicit-schemes}:
\begin{equation}
  \yv_{n+1} = \dtn (\ffv{t_{n+1}, \yv_{n+1}} + \ffv{t_n, \yv_n})/2.
  \label{eq:tr-definition}
\end{equation}

The \emph{second order backwards difference formula} (BDF2) is:
\begin{equation}
  \frac{\yv_{n+1} - \yv_n}{\dtn} = \frac{\dtn}{2\dtn + \dtx{n-1}} \frac{\yv_n - \yv_{n-1}}{\dtx{n-1}} 
  + \frac{\dtn + \dtx{n-1}}{2\dtn + \dtx{n-1}} \fv(t_{n+1}, \yv_{n+1}).
\end{equation}

The method that this thesis focuses on (for reasons that will become clear through the remainder of this section) is the \emph{implicit midpoint rule} (IMR).
It is given by:
\begin{equation}
  \label{eq:imr-definition}
  \yv_{n+1} = \yv_n + \dtn \ffv{\frac{t_{n+1} + t_n}{2}, \frac{\yv_n + \yv_{n+1}}{2}}.
\end{equation}
Note that for cases where $\fv$ is linear in both $t$ and $\yv$ IMR, \cref{eq:imr-definition}, is equivalent to TR, \cref{eq:tr-definition}.
In general the properties of the two methods are similar.

IMR is trivial to implement in any existing implicit solver: take a step of BDF1, \cref{eq:bdf1-definition}, using $\dtn^\bdfo = \dtn^\imr/2$ then update via
\begin{equation}
  \begin{aligned}
    t_{n+1} &\rightarrow t_{n+1} + \dtn^\imr/2, \\
    \mv_{n+1} &\rightarrow 2\mv_{n+1} - \mv_n.
  \end{aligned}
\end{equation}
This is exactly equivalent to IMR with a step size of $2\dtn$ \cite{Malidi2005}.

TR and IMR are both self starting, while BDF2 requires an additional start-up value.
This can be generated, for example, by taking a single step of IMR or TR before beginning the use of BDF2.


\subsection{Local truncation error and order}
\label{sec:deriv-local-trunc}

The \emph{local truncation error} (LTE) of a time integration scheme is the error due to a single integration step.
It can be calculated by substituting $\yv_n = \yv(t_n)$, $\yv_{n-1} = \yv(t_{n-1})$, etc. into the approximation for the next time-step, then subtracting the result from the exact solution at the next time-step, $\yv(t_{n+1})$, \ie
\begin{equation}
  \label{eq:60}
  \lte = \yv(t_{n+1}) - \hat{\yv}_{n+1},
\end{equation}
where the history values ($\yv_{n}, \yv_{n-1}, \ldots$) used to calculate $\hat{\yv}_{n+1}$ are exact.
For example the local truncation error of IMR is
\begin{equation}
  \lte^\imr =  \yv(t_{n+1}) - \yv(t_n) - \dtn \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}^\imr}{2}}.
  \label{eq:trunc-start}
\end{equation}

If the local truncation error of a time integration scheme is such that 
\begin{equation}
  \lte \leq c \dtn^p
\end{equation}
then we say that the scheme is of order $p$.

It is important to distinguish the local truncation error from the \emph{global (temporal) error}
\begin{equation}
  \label{eq:global-temporal-error}
    E_n = \yv(t_{n+1}) - \yv_{n+1}.
\end{equation}
The difference is that the global error includes any error accumulated over previous time steps (\ie the exact values $\yv(t_n)$ in \cref{eq:60} are replaced by their approximations).
Note that because of this accumulation the global error will typically be larger than the LTE.
For example the global error of IMR expressed in the same form as \cref{eq:trunc-start} is
\begin{equation}
  E_n^\imr =  \yv(t_{n+1}) - \yv_n^\imr - \dtn \ffv{ \thf, \frac{\yv_n^\imr + \yv_{n+1}^\imr}{2} }.
\end{equation}

We now give a derivation of the local truncation error of IMR. 
The derivations for TR and BDF2 are much simpler and can be found in most text books on the subject.
We first write the Taylor expansion $\yv(t_{n+1})$ and $\yv(t_{n})$ about $\thf$.
We use the midpoint rather than one of the end points (a typical choice in such calculations) because it results in simpler expressions.
Assuming that $\yv(t)$ is ``sufficiently smooth'' for the required derivatives to exist, its Taylor series expansion at $t_{n+1}$ about $\thf$ is given by
\begin{equation}
  \yv(t_{n+1}) = \yv(\thf + \frac{\dtn}{2}) = \yvhf + \frac{\dtn}{2} \yvhf['] 
  + \frac{\dtn^2}{8} \yvhf['']
  + \frac{\dtn^3}{48} \yvhf[''']
  + \order{\dtn^4}.
  \label{eq:taylornp1}
\end{equation}
Similarly, the expansion at $t_n$ about $\thf$ is
\begin{equation}
  \yv(t_n) = \yv(\thf - \frac{\dtn}{2}) = \yvhf - \frac{\dtn}{2} \yvhf['] 
  + \frac{\dtn^2}{8} \yvhf[''] 
  - \frac{\dtn^3}{48} \yvhf['''] 
  + \order{\dtn^4}.
  \label{eq:taylorn}
\end{equation}
Substituting \cref{eq:taylornp1,eq:taylorn} into \cref{eq:trunc-start} gives\footnote{If we had chosen to Taylor expand about $t_n$ there would be an additional term in $\yv_n''$ in \cref{eq:trunc-mid}.}
\begin{equation}
  \lte^\imr = \underbrace{\frac{\dtn^3}{24} \yvhf[''']}_{\text{I}}
  + \underbrace{\dtn\bigs{ \yvhf['] - \ffv{\thf, \frac{\yv(t_n) + \yv_{n+1}}{2}} }}_{\text{II}}
  + \order{\dtn^4}.
  \label{eq:trunc-mid}
\end{equation}

There are two parts to \cref{eq:trunc-mid}: the first term (I) is fairly standard for second order time integration schemes (see \eg \cref{cha:trunc-errors}).
The second term (II) originates from the use of an approximation to $\yv(\thf)$ in the evaluation of $\fv$ (\ie the Runge-Kutta nature of IMR).
The rest of the derivation requires applying Taylor expansions to II and is carried out in \cref{sec:full-imr-lte-calculation}.
The final result shows that IMR is of second order:
\begin{equation}
  \lte^\imr = \frac{\dtn^3}{24} \left[\yvhf['''] - 3 \dfdyhf \cdot \yvhf[''] \right]
  + \order{\dtn^4},
  \label{eq:trunc-final}
\end{equation}
where $\dfdyhf = \pd{\fv}{\yv}\evalat{t=\thf}$ is the Jacobian of $\fv$ with respect to $\yv$ evaluated at $t=\thf$.
An additional condition required in the derivation is that ??ds the magnitude of the eigenvalues of $\dtn\dfdyhf$ are less than one, the implications of this are discussed in \cref{sec:order-reduction}.

The TR \cite[261]{GreshoSani} and BDF2 \cite[715]{GreshoSani} methods are also second order.
Their local truncation errors are similar to term I of \cref{eq:trunc-mid}:
\begin{equation}
  \label{eq:tr-lte}
  \lte^\tr = \yv_{n+1} - \yv(t_{n+1}) = -\frac{\dtn^3 \yv_n'''}{12}
  + \order{\dtn^4},
\end{equation}
and
\begin{equation}
  \label{eq:bdf2-lte}
  \lte^\bdf = \yv_{n+1} - \yv(t_{n+1}) = \frac{(\dtn + \dtx{n-1})^2}{\dtn(2\dtn + \dtx{n-1})}
  \frac{\dtn^3 \yv_n'''}{6}
  + \order{\dtn^4}.
\end{equation}


\subsection{A-stability}
\label{sec:A-stability}

To discuss the stability of time integrators the following initial value ODE is widely used
\begin{equation}
  \begin{aligned}
    \ffv{\yv} &= \lambda \yv, \\
    \yv_0 &= 1.
    \label{eq:ode-test-f}
  \end{aligned} 
\end{equation}
It's analytical solution is
\begin{equation}
  \yv(t) = \exp(\lambda t).
\end{equation}

A-stability\footnote{The A does not stand for anything, it is just ``A'' \cite[40]{HairerWanner}. In particular it does \emph{not} mean ``absolute stability''.} is the property that a method is has no stability restrictions on the time step size when used to integrate \cref{eq:ode-test-f} for all $\lambda$ with $\realp(\lambda) \leq 0$.
When $\realp(\lambda) > 0$ the ODE itself is unstable, and so stability is not expected in general.
A-stability is a good way to classify the suitability of a time integrator for solving stiff ODEs.

The IMR, TR, BDF1 and BDF2 methods are all A-stable \cite[pgs. 43, 251]{HairerWanner}.
Linear explicit methods are never A-stable \cite{Nevanlinna1974} (``linear'' methods in this reference include all common time integration methods: Runge-Kutta, multistep, predictor-corrector, \ldots)\footnote{These limitations could theoretically be circumvented by using significantly more complex methods, but such questions are far beyond the scope of this thesis.}, which is the main reason for our focus on implicit methods.
Additionally there are no A-stable linear multistep methods\footnote{A-stable implicit Runge-Kutta methods of higher order do exist, see \eg \cite[73]{HairerWanner}.
However moving to higher order methods significantly increases the size of the system to be solved at each step. Hence they are prohibitively expensive for the time integration of semi-discretised PDEs.
Also second order is in some sense ``good enough'' for LLG integration since the spatial accuracy is only second order and contains higher order derivatives than in time.
??ds talk to Milan/Matthias about this: it seems that with good preconditioning we can have \eg a 4th order method (Gauss order 4, higher order version of IMR) for only ~4x the cost per step, a gain of 25x in (computation time)/accuracy with steps of ~0.1} of order greater than 2 \cite[261]{GreshoSani}, which is why we have chosen the  methods in \cref{sec:some-implicit-time-integrators} for discussion.

\subsection{Spurious numerical damping}
\label{sec:numerical-damping}

Some time integration methods create additional, non-physical damping in approximate solutions, even when none exists in the exact solution.
This is problematic for the modelling of highly oscillatory ODEs, such as the LLG with low damping.

In the solution of the ODE \cref{eq:ode-test-f} with $\lambda = i\omega$, $\abs{y}$ should not decrease over time.
If $\abs{y_n}$ decreases over time in the approximate solution given by a time integration scheme, then we say that the scheme causes spurious damping.

For example, for IMR:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega (y_n + y_{n+1})/2, \\
    &= \frac{1 + i\dtn \omega/2}{1 - i\dtn \omega/2} y_n,
  \end{aligned}
\end{equation}
therefore
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}}^2 &=  \frac{\abs{1 + i\dtn \omega/2}^2}{\abs{1 - i\dtn \omega/2}^2} \abs{y_n}^2, \\
    \abs{y_{n+1}}^2 &=  \frac{(1 + i\dtn \omega/2)(1 - i\dtn \omega/2)}
    {(1 - i\dtn \omega/2)(1 + i\dtn \omega/2)} \abs{y_n}^2, \\
    &=  \abs{y_n}^2,
  \end{aligned}
\end{equation}
hence IMR does not cause spurious damping.

Since $f$ is linear in $y$ for this example TR and IMR are identical
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn(i\omega y_n + i\omega y_{n+1})/2, \\
    &= y_n + \dtn i \omega (y_n + y_{n+1})/2,
  \end{aligned} 
\end{equation}
and the same property applies for TR.

For BDF1, however, we have:
\begin{equation}
  \begin{aligned}
    y_{n+1} &= y_n + \dtn i \omega y_{n+1}, \\
    &= \frac{1}{1 - i \dtn \omega} y_n.
  \end{aligned}
\end{equation}
Hence,
\begin{equation}
  \begin{aligned}
    \abs{y_{n+1}} &= \sqrt{ \frac{1}{(1 - i \dtn \omega)(1 + i \dtn \omega)}} \abs{y_n}, \\
    &= \frac{1}{\sqrt{1 + \dtn^2 \omega^2}} \abs{y_n}, \\
  \end{aligned}
\end{equation}
and the magnitude of $y$ decreases over time, \ie the solution is damped.
The analysis for the second order BDF2 case is more complex due to the fact that it is a multistep method, but the result is the same: the solution suffers from non-physical damping \cite[265]{GreshoSani}.


\subsection{B-convergence and order reduction}
\label{sec:order-reduction}

It is known that certain implicit Runge-Kutta methods are susceptible to reduced accuracy when used to solve certain extremely stiff problems \cite[156]{Atkinson1994} \cite[225]{HairerWanner}.
In the case of the implicit midpoint rule this corresponds to cases when term II of the LTE \cref{eq:trunc-mid} is large. 
This occurs when $\pd{\fv}{\yv}\evalat{\thf{}}$ is large, \ie when the error in $\ffv{t, \yv}$ due to a small error in $\yv$ is large.

The concept of ``B-convergence'' is used to analyse this effect.
Roughly speaking, a Runge-Kutta method is B-convergent of order $r$ if the global error is  $\order{\dtx{\text{max}}^r}$ for all sufficiently smooth ODEs without any assumption about the size of $\pd{\fv}{\yv}$.
The implicit midpoint rule is B-convergent of order 1 \cite[231]{HairerWanner}\footnote{In this reference IMR is referred to as the second order Gauss method, BDF1 is Radau IIA, TR is Lobatto IIIA \cite[72-76]{HairerWanner}.}, which is one order less than its global error on typical ODEs.
In contrast, the TR and BDF methods do not suffer from order reduction \cite[159]{Atkinson1994}.
This is because all evaluations of $\fv$ are at integer time points (\ie $t_{n+1}, t_{n}, t_{n-1}, \ldots$ rather than $\thf$) and so there is no term in the LTE containing $\pd{\fv}{\yv}$.

A simple test ODE demonstrates this phenomenon \cite[157]{Atkinson2009}:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-test-order-reduction}
    f(t, y) &= -\lambda (y - g(t)) + g'(t), \\
    y(t) &= g(t), \\
  \end{aligned}
\end{equation}
for some function $g(t)$ and parameter $\lambda \geq 0$.
Note that $\pd{f}{y} = \lambda$, so we can directly control the magnitude of term (II) of the IMR truncation error \eqref{eq:trunc-mid}.

We now derive the LTE for IMR in this example when $\abs{\lambda\dtn} \gg 1$.
From \cref{eq:trunc-implicit-form} we have that
\begin{equation}
  \begin{aligned}
    (1 + \frac{\dtn \lambda}{2})\lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}. \\ 
  \end{aligned}
\end{equation}
Then using $\abs{\lambda\dtn} \gg 1$:
\begin{equation}
  \begin{aligned}
    \frac{\dtn \lambda}{2} \lte^\imr &= \frac{\dtn^3}{24}
    \bigs{g'''(\thf) - 3 \lambda g''(\thf)} + \order{\dtn^4}, \\ 
    \lte^\imr &= \frac{\dtn^2}{12\lambda} \left[g'''(\thf) - 3 \lambda g''(\thf) \right] + \order{\dtn^4}. \\
  \end{aligned}
\end{equation}
If additionally $\abs{\lambda\dtn} \gg \abs{g'''(t)}$, which will typically be true for very large $\lambda$ and $g \neq g(\lambda)$:
\begin{equation}
  \lte^\imr = \frac{-\dtn^2}{4} g''(\thf) + \order{\dtn^3}.
  \label{eq:reduced-order-imr-truncation-error}
\end{equation}
So we can see that the order has been reduced by one power of $\dtn$ compared with the standard LTE \cref{eq:trunc-final} (which is the case for $\abs{\lambda\dtn} < 1$).


\subsection{Adaptivity}
\label{sec:adaptivity}

Adaptive time integration methods automatically select time step sizes in response to an estimate of the local truncation error.
This can improve efficiency depending on the equation being solved, for example on problems where a high frequency part of the solution is gradually damped out the time step can increase by orders of magnitude at later times without loss of accuracy.
Adaptive integration also greatly simplifies the use of the simulation program: the user only has to choose the allowed local temporal error and the program will handle the rest.

Most local truncation error estimators for implicit integrators (e.g. TR, BDF2) use a Milne-device based method \cite[707-716]{GreshoSani}.
This means that they compute an explicit estimate of the solution $\yv^E_{n+1} \sim \yv(t_{n+1})$  (sometimes known as a predictor step) to the same order of accuracy as the implicit method and using only derivatives that are required for the implicit method (so that no additional $\fv$ evaluations are needed).
They then use algebraic rearrangements of the local truncation error expressions of the predictor step and the implicit step to compute an approximation to the local truncation error.

No such estimation is possible for IMR because of the specific form of the local truncation error \eqref{eq:trunc-mid}.
The details of this issue and the first (to the author's knowledge) algorithm for adaptive time integration with IMR are described in \cref{sec:adaptive-imr}.

It is also possible to create less general adaptivity methods based on error estimates specific to the equation being solved.
For example for the LLG equation (in cases where the damping is not exactly correct) the effective damping can be used as an error estimator, as proposed by Albuquerque \etal \cite{Albuquerque2001}.
The disadvantage of this method (aside from the obvious requirement that the damping be inexact) is that, depending on the discretisation used, the error estimator may be unable to distinguish between spatial errors and temporal errors.

As another example of this less general class of adaptivity methods: time error estimates for the LLG with limited effective field terms and a specific finite element discretisation were proven and used as part of an adaptive time integration algorithm with both IMR and BDF2 by Banas \cite{Banas-thesis}.
??ds do these work, why does no one use them?
These estimates have the obvious limitation that they are only (directly) applicable to the equations for which they were derived.
It is also a fairly complex derivation (at least for non-numerical analysts), and would be more difficult to experiment with because of its limited applicability.


\subsection{Magnetisation length conservation}
\label{sec:ensuring-constant-mv}

As discussed in \cref{sec:prop-cont-llg}, the length of the magnetisation, $\abs{\mv}$, at each point in space is constant over time.
However, in the approximations given by numerical methods this property is often lost, and must be enforced separately.

A simple method of dealing with the constraint is to re-normalise each $\mv$ after some number of time steps or when the error in $\abs{\mv}$ exceeds some tolerance \cite{Fidler2000}.
However this approach fundamentally changes the system of equations being solved in a non-linear and unpredictable manner \cite{Lewis2003}.

??ds write about energy? energy moves from one type to another when length changes

??ds move stuff about why we should care to here?

If we have a system with only a single macrospin (\ie a single value of $\mv$ represents the magnetisation of the entire system) then it is easy to avoid this problem by using a spherical polar coordinate system $(r,\theta,\phi)$.
In these coordinates \cref{eq:LLG} can be expressed in terms of only the angles $(\theta,\phi)$ representing the direction of $\mv$ and the length constraint is automatically enforced since
\begin{equation}
  \label{eq:40}
  \pd{\abs{\mv}}{t} = \pd{r}{t} \equiv 0.
\end{equation}
However, to extend this to systems where $\mv$ varies in space we have to use a separate spherical polar coordinate system at each point where $\dMdt$ is calculated.
In addition magnetostatic interactions between discretise points still need to be computed using a Cartesian global coordinate system.
??ds Jim--is this true? seem to remember you telling me this, got a citation?
Hence we have to convert between coordinate systems repeatedly during the simulation.
Finally problems can occur with this approach as the polar angle, $\theta$, approaches zero because $\pd{\mv}{t} \propto \frac{1}{\sin(\theta)}$ \cite{Fukushima2005} and so the derivative becomes singular.

Geometric time integration schemes aim to solve this problem by constructing a scheme that naturally preserves the value of $\abs{\mv}$.
Some examples of such schemes are based on IMR \cite{DAquino2005}, or a semi-explicit extension of IMR using explicit extrapolations of the effective field \cite{Spargo2003} \cite{Serpico2001}.
Alternatively, geometric integration methods based on Cayley transforms can be used \cite{Lewis2003} \cite{Bottauscio2011}.

A third approach is to use Lagrange multipliers to constrain the solution such that $\abs{\mv}$ is constant \cite{Szambolics2008a}.
The main problem with this method is that an additional degree of freedom is needed for every $\mv$ value in space.


\subsubsection{Conservation of $\abs{\mv}$ by IMR in a macrospin model}
\label{sec:proof-magn-length-ode-imr-llg}

??ds add  $< \ntol$ to derivation

In this section we show that the discrete form of the Landau-Lifshitz-Gilbert equation that results from the use of the implicit midpoint rule maintains the magnetisation length conservation property of the continuous form shown in \cref{sec:prop-cont-llg}.
These results are applicable to cases where, after any spatial discretisation, each spatial point is time evolved by simply integrating the LLG with the values for that point, \ie finite difference and macrospin discretisations (but not finite elements).
??ds check/clarify this
For the purposes of this section we assume that the discretised non-linear Landau-Lifshitz-Gilbert equation is satisfied exactly, \ie the linearisation method is exact.

As can be seen from \cref{eq:imr-definition} the IMR is defined by the following substitutions:
\begin{equation}
\begin{aligned}
  \label{eq:55}
  \pd{\mv}{t} &\rightarrow \frac{\mv_{n+1} - \mv_n}{\dtn}, \\
  \mv &\rightarrow \frac{\mv_{n+1} + \mv_n}{2}, \\
  t &\rightarrow \frac{t_{n+1} + t_n}{2}.
\end{aligned}
\end{equation}
So the IMR-discretised version of the LLG is
\begin{equation}
  \frac{\mv_{n+1} - \mv_n}{\dtn} = - \frac{\mv_{n+1} + \mv_n}{2} \times
  \bigb{\hv \bigs{\frac{\mv_{n+1} + \mv_n}{2}} - \dampc \frac{\mv_{n+1} - \mv_n}{\dtn}}.
  \label{eqn:disc-llg}
\end{equation}

??ds introduce inner products, linear operators, symmetry somewhere

Intuitively the conservation properties of the IMR discretisation can be seen to come from the cancellation of cross terms in $\ip{\lop[\mv]}{\dmdt}$ for any symmetrical linear operator $\lop$.
More precisely:
\begin{equation}
  \begin{aligned}
    \label{eqn:imr-linop}
    \ip{\lop \left[ \frac{\mv_{n+1} + \mv_n}{2} \right]}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}} + \ip{\mv_{n+1}}{\lop \mv_{n}} \\
    & \qquad\qquad - \ip{\mv_{n}}{\lop \mv_{n+1}} - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big] \\
    &= \frac{1}{2\dtn} \Big[
    \ip{\mv_{n+1}}{\lop \mv_{n+1}}
    - \ip{\mv_{n}}{\lop \mv_{n}}
    \Big].
  \end{aligned}
\end{equation}
This means that any orthogonality relationships between $\lop \mv$ and $\dmdt$ in the continuous case carry over to the discrete case. ??ds more detail on what this means

We can now examine the change in magnetisation length using the same technique as in \cref{sec:prop-cont-llg}: take the dot product of \cref{eqn:disc-llg} with $\frac{\mv_{n+1} + \mv_n}{2}$ on both sides and use the triple product identity \cref{eq:dot-cross-id} to get
\begin{equation}
  \label{eq:63}
  \frac{\mv_{n+1} + \mv_n}{2} \cdot \frac{\mv_{n+1} - \mv_n}{\dtn} = 0.
\end{equation}
Now using~\cref{eqn:imr-linop} with $\lop$ as the identity operator gives us
\begin{equation}
  \frac{\ip{\mv_{n+1}}{\mv_{n+1}} - \ip{\mv_n}{\mv_n} }{2 \dtn} = 0.
\end{equation}
Therefore
\begin{equation}
  \abs{\mv_{n+1}} = \abs{\mv_n},
\end{equation}
\ie the magnetisation length does not change between time steps.

Note that for other time integration schemes the property \cref{eqn:imr-linop} typically does not hold.
For example, in the case of BDF1 we have
\begin{equation}
  \begin{aligned}
    \ip{\lop \bigs{\mv_{n+1}}}{ \frac{\mv_{n+1} - \mv_n}{\dtn} }
    &= \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}} 
      - \ip{\mv_{n+1}}{\lop \mv_n} }, \\
    &\neq \frac{1}{\dtn} \bigs{ \ip{\mv_{n+1}}{\lop \mv_{n+1}}
      - \ip{\mv_n}{\lop \mv_n} },
  \end{aligned}
\end{equation}
\ie orthogonality properties from the continuous form do not carry to the discrete over in general.

Notice that the only requirement in the derivation of \cref{eq:63} is to use the midpoint approximation of magnetisation the llg itself.
The derivation does not make any assumption about how the effective field is approximated.
This explains why the semi-explicit modification to IMR also conserves the magnetisation length. 


\subsection{Energy conservation/decay}
\label{sec:energy-cons}

Another geometrical property of the Landau-Lifshitz-Gilbert equation, also discussed in \cref{sec:prop-cont-llg}, is the conservation or monotonic decay of energy depending on the value of the damping constant $\dampc$.

IMR also conserves energy when $\dampc = 0$ and it ensures that the energy is a decreasing function of time when $\dampc > 0$ \cite{DAquino2005}.
The other geometric integration schemes mentioned in \cref{sec:ensuring-constant-mv}, in particular the semi-explicit modifications of IMR, do not have this property.


\subsubsection{Proof of energy property for IMR on the ODE LLG}
\label{sec:proof-energy-prop}

??ds add  $< \ntol$ to derivation

\newcommand{\happerror}{\mathcal{E}_\text{ap}}

Before we can derive energy decay properties of the discretised LLG we need to look at the properties of the effective field when considered as an operator on $\mv$:
\begin{equation}
  \begin{aligned}
    \label{eq:hop}
    \hv(\xv, t)[\mv(\xv)] &= \hop [\mv(\xv)] + \happ(\xv, t), \\
    \hop [\mv(\xv)] &= \lap \mv + ??ds.\\
  \end{aligned}
\end{equation}
It can be shown (see \cref{sec:linear-symm-field-operators}) that $\hop[\mv]$ is a linear symmetric operator on $\mv$ provided that there is no surface anisotropy and that the magnetocrystalline anisotropy is uniaxial (extensions probably possible.. not really checked ??ds can we prove it in general for all effective fields?).
The applied field is not an operator and so will be treated separately.
The energy can be written using this notation as
\begin{equation}
  \label{eq:energy-hop}
  \e_n = \ehop_{,n} + \eapp_{,n} = - \ip{\mv_n}{\frac{\hop[\mv_n]}{2}} - \ip{\mv_n}{\happ(t_n)},
\end{equation}
where the factor of $1/2$ is carried over from the energy expressions \cref{eq:nd-e-app,eq:nd-e-ms,eq:nd-e-ex,eq:nd-e-ca}, see \cref{sec:energy-field-relation} for details.
Also note that we can expand $\mphapp$ into a midpoint-like form
\begin{equation}
  \begin{aligned}
  \mphapp &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2} 
  + \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4}, \\
  &= \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror.\\
  \label{eq:happ-midpoint}
\end{aligned}
\end{equation}
where
\begin{equation}
  \happerror = \frac{\dtn^2}{4} \evalatb{\spd{\happ}{t}}{\frac{t_{n+1} + t_n}{2}}  + \order{\dtn^4},
\end{equation}
is the error in this midpoint-like approximation of $\happ$.

In the same manner as the \cref{sec:proof-magn-length-ode-imr-llg} we derive the change in energy of the discrete LLG by taking the $\ltwo$ inner product of \cref{eqn:disc-llg} with
\begin{equation}
  \mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror - \dampc \mpdmdt,
\end{equation}
resulting in
\begin{equation}
  \begin{aligned}
    &\ltip{\mphop + \frac{\happ(t_{n+1}) + \happ(t_n)}{2} + \happerror}{\mpdmdt} \\
    & \quad - \dampc \ltip{\mpdmdt}{\mpdmdt} = 0.
    \label{eq:54}
  \end{aligned}
\end{equation}
The $\hop$ term in \cref{eq:54} can be simplified by using the orthogonality identity~\cref{eqn:imr-linop} and then written as an energy using \cref{eq:energy-hop}
\begin{equation}
  \begin{aligned}
    \ltip{\mphop}{\mpdmdt} 
    &= \frac{1}{2\dtn} \Big[\ip{\mv_{n+1}}{\hop \left[\mv_{n+1} \right]}
    - \ip{\mv_{n}}{ \hop\left[ \mv_{n} \right]} \Big], \\
    &= -\frac{\ehop_{,n+1} - \ehop_{,n}}{\dtn}.
  \end{aligned}
  \label{eq:50}
\end{equation}
Next we examine the applied field term.
Omitting the $L^2$ from the inner products and temporarily writing $\happ(t_i)$ as $\hv_i$ for brevity of notation we have
\begin{equation}
  \begin{aligned}
    \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}
    &= \ip{\hv_{n+1} + \hv_n}{\mpdmdt} \\
    & \qquad - \ip{\frac{\hv_{n+1} + \hv_n}{2}}{\mpdmdt}, \\
    &= -\frac{\eapp_{, n+1} - \eapp_{, n}}{\dtn}  + A,
  \end{aligned}
\end{equation}
where
\begin{equation}
  \begin{aligned}
    A &= \frac{1}{2\dtn}\Big[ 2\ip{\hv_n}{\mv_{n+1}} - 2\ip{\hv_{n+1}}{\mv_n} \\
    & \qquad - \ip{\hv_{n+1}}{\mv_{n+1}} +\ip{\hv_{n+1}}{\mv_n} 
    - \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= \frac{1}{2\dtn}\Big[ - \ip{\hv_{n+1}}{\mv_{n+1}} - \ip{\hv_{n+1}}{\mv_n} 
    + \ip{\hv_n}{\mv_{n+1}} + \ip{\hv_n}{\mv_n} \Big], \\
    &= - \ip{ \frac{\hv_{n+1} - \hv_n}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }. \\
    % &= - \ltip{ \frac{\happ(t_{n+1}) - \happ(t_n)}{\dtn} }{ \frac{\mv_{n+1} + \mv_n}{2} }.
    \label{eq:52}
  \end{aligned}
\end{equation}
Finally, we insert the results of \cref{eq:50,eq:52} into \cref{eq:54} to find
\begin{equation}
  \frac{\e_{n+1} - \e_n}{\dtn}
  = -\dampc \ltnorm{\mpdmdt}^2
  - \ltip{\frac{\happ(t_{n+1}) -\happ(t_n)}{\dtn}}{\frac{\mv_{n+1} + \mv_{n}}{2}}
  + \happerror.
  \label{eqn:imr-llg-energy}
\end{equation}
If the applied field is linear (and $\happerror = 0$) this is exactly the midpoint discretisation of the continuous energy balance \cref{eq:energy-decay}.
In particular the energy is conserved when $\dampc = 0$ and the applied field is constant.
This relationship is illustrated in \cref{fig:commutation-imr-energy}.

When $\happ$ is piecewise linear, for example when a field is instantaneously switched on, time steps can be trivially selected such that each non-linear change happens exactly at the start/end of a step, and so again $\happerror = 0$.
Since the time between such changes in field must be longer than the time scale of the dynamics such a choice of time steps will have no effect on the efficiency of the computation.

Also note that none of the above is applicable to the semi-explicit modification of IMR since in that case $\mphop$ is replaced by an expression of the form $a\hop \bigs{\mv_n} + b\hop \bigs{\mv_{n-1}}$.

\begin{figure}
  \centering
  \begin{tikzpicture}[node distance = 10cm, auto, >=latex']
    \node[block] (start) {Continuous LLG};
    \node[block, right of=start] (discrete) {IMR discretised LLG};
    \node[block, below of=start, node distance = 3cm] (energy) {Continuous energy};
    \node[block, right of=energy] (denergy) {IMR discretised energy};

    \path [->] (start) edge node {Discretise} (discrete);
    \path [->] (start) edge node {Derive energy} (energy);
    \path [->] (energy) edge node {Discretise} (denergy);
    \path [->] (discrete) edge node {Derive energy} (denergy); 
  \end{tikzpicture}
  \caption{Commutation relationship between the IMR discretisation and the LLG energy derivation. 
In contrast to other time integration methods the result is independent of the order of operations.}
\label{fig:commutation-imr-energy}
\end{figure}


\subsection{Symplecity}

??ds This section will probably not end up in the final thesis, although I'm a bit worried because it may turn out to be important and I don't understand it too well.
However d'Aquino didn't do any experiments on this either, so I can probably get away with skipping it!

The implicit midpoint rule with fixed step size is ``almost symplectic'' for the LLG equation with zero damping, , \ie the property equivalent to Hamiltonian flow for  dissipative systems is conserved up to $\order{??ds}$ \cite{DAquino2005} \cite{Austin1993}.

It is well known that most adaptive schemes are not symplectic \cite[91]{Iserles2009} because they constantly change the ``nearby Hamiltonian'' that is followed by the symplectic integrator.
Therefore the adaptive IMR is not expected to have the almost-symplectic property discussed in \cref{??ds}.


\subsection{Conclusions}

In the previous sections we have highlighted a number of interesting properties of the implicit midpoint rule.
We have a number of reasons to believe that the geometric properties will translate into an improvement in the accuracy and robustness of the overall solver:
\begin{itemize}
\item Errors in the energy dissipation \cite{Albuquerque2001} and magnetisation length \cite{Chantrell2001} have been successfully used as error estimators for local truncation error and so reduction or removal of these errors may reduce this error.
\item It is well known that geometric integration schemes typically result in much smaller long-timescale error build-up than schemes that do not preserve such quantities \cite[77]{Iserles2009}.
\item The non-linear modification to the Landau-Lifshitz-Gilbert equation caused by renormalisation of the magnetisation length (as commonly used to maintain correct magnetisation length in non-conservative time integrators) may cause significant changes in the magnetostatic field \cite{Lewis2003}.
Avoiding renormalisation eliminates the possibility of such errors occurring.
\item Renormalisation of the magnetisation modifies the balance between the various energy terms.
This is similar to the methods that lead to the ``flying ice cube'' problem \cite{Harvey1998} in molecular dynamics.\footnote{In such problems the rescaling of particle velocities (to maintain constant temperature despite numerical error accumulation) can result in large amounts of kinetic energy being transferred from the motion of internal degrees of freedom to motion of the centre of mass.}
Again, by avoiding renormalisation we eliminate the possibility of such errors.
\end{itemize}

Additionally, the same geometric properties enable convergence proofs of IMR when applied to a FEM semi-discretised LLG equation with a slight modification that will be discussed in \cref{sec:nodal-integration} \cite{Bartels2006}.



\section{Linearisation}
\label{sec:linearisation}

When implicit time integration methods are applied to a non-linear differential equation (such as the LLG) a non-linear system of equations must be solved.
Explicit time integration methods sidestep the issue of non-linearity by avoiding the need to solve any system.


\subsection{Functional iteration}
\label{sec:picard}

??ds fill in

Cheap iterations

Convergence proof via Banach fixed point theorem for sufficiently small time steps.

Useful for non-stiff problems (convergence requirements similar to stiffness requirements) \cite{Iserles2009}.

In practice people often terminate the Picard iteration after only one or two steps, leading to a predictor-corrector method. 
These methods have similar properties to explicit methods: cheap per step but vulnerable to stability problems.

Seem to be lots of variations of this...

\subsection{Newton-Raphson method}
\label{sec:newt-raph}

A non-linear problem:
\begin{equation}
  \begin{aligned}
    \text{Find $\yv$ such that } \fv(t, \yv, \ldots) = \gv(t, \yv, \ldots).
  \end{aligned} 
\end{equation}
can be stated as an equivalent minimisation problem:
\begin{equation}
  \label{eq:non-lin-system}
  \begin{aligned}
    \text{Find $\yv$ such that } \resi(t, \yv, \ldots) = 0.
  \end{aligned}
\end{equation}
In the context of implicit time integration of a semi-discretised PDE we write
\begin{equation}
  \yvdis_n = \set{\yv_{n,i}}_{i=0}^{i=N},
\end{equation}
for the list of spatially discrete values of $\yv$ at time step $n$.
For example, for an LLG problem consisting of a single macrospin $\yvdis_n = \mv_n$.
For a finite element semi-discretised LLG problem $\yvdis_n$ is the list of all nodal values of $\mv_n$ along with any values needed for the magnetostatic calculations.

Then the non-linear problem can be written as 
\begin{equation}
  \text{Find $\yvdis_{n+1}$ such that } \resi(t, \yvdis_{n+1}, \yvdis_n, \yvdis_{n-1}, \ldots) = 0,
\end{equation}
where the number of previous values of $\yvdis$ needed depends on the time integration scheme.

The motivation for the Newton-Raphson method comes from a simple Taylor expansion of the residual.
If the exact root of the residual is $\nexty$ and we have an initial guess for this root $\nlit{\yvdis}{0}$ then we can obtain a correction to this initial guess from:
\begin{equation}
  \begin{aligned}
    \resi(\nexty) &= 0, \\
    &= \resi(\nlit{\yvdis}{0} + \corr), \\
    &= \resi(\nlit{\yvdis}{0}) + \jac(\nlit{\yvdis}{0}) \cdot \corr + \order{\corr^2},
  \end{aligned}
\end{equation}
where $\jac(\nlit{\yvdis}{0}) = \evalatb{\pd{\resi}{\yvdis}}{\nlit{\yvdis}{0}}$ is the Jacobian matrix for the residual.
So
\begin{equation}
  \begin{aligned}
    \label{eq:49}
    \nexty &= \nlit{\yvdis}{0} + \corr, \\
    &= \nlit{\yvdis}{0} + \jac^{-1}(\nlit{\yvdis}{0}) \cdot \resi(\nlit{\yvdis}{0}) + \order{\corr^2}.
  \end{aligned}
\end{equation}

This suggests the iteration
\begin{equation}
  \nlit{\yvdis}{i+1} = \nlit{\yvdis}{i} + \jac^{-1}(\nlit{\yvdis}{i}) \cdot \resi(\nlit{\yvdis}{i}),
\end{equation}
in which, if the Taylor expansion is valid and that we can discard the higher order terms, the error is \emph{squared} after each iteration (\ie convergence is quadratic).
The assumption that we can discard terms in $\order{\corr^2}$ is fairly strong: it means that we need a good initial guess so that the initial correction, $\corr$, is small.
Fortunately in time integration problems the value at the previous time step provides a good initial guess: $\nlit{\yvdis}{0} = \yvdis_n$.
It is also possible to construct an even better initial guess by using a single cheap explicit ``predictor'' step, as used in Milne's device for adaptive time integration (and if such an adaptive scheme is in use then this improved initial guess is ``free'').

The iteration is terminated when
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \ntol,
\label{eq:newton-convergence-test}
\end{equation}
for some user defined tolerance $\ntol$.
Alternatively the absolute tolerance condition \cref{eq:newton-convergence-test} can be replaced by or combined with a relative tolerance condition
\begin{equation}
  \norm{\resi(\nlit{\yvdis}{i+1})}_{\infty} < \norm{\resi(\nlit{\yvdis}{0})}_{\infty} \nrtol.
\end{equation}
If the residual expression is well normalised, and so has initial magnitude of $\order{1}$, then the two are (roughly) equivalent. 

% For micromagnetics problems we find that this reliably converges to a tolerence of $\sim 10^{-10}$ in around 2-3 steps!

One downside of this method compared to function iteration is that it requires the assembly and solution of a system of linear equations to obtain each correction $\corr$.
This is the subject of \cref{sec:solution-lin-sys}.
Another potential downside is that the iteration is only guaranteed to converge if the initial guess ``good enough''.
However for non-linear systems resulting from time stepping we always have a very good initial guess, so this does not seem to be a real issue.

A third downside is that the resultant linear systems are more complex than those given by Picard iteration.
This can make it more difficult to construct efficient iterative solvers.


\subsection{Advanced LLG-specific linearisation methods}
\label{sec:advanced-lin}

??ds clean up or remove?

Linearisation based on choice of test functions of much interest in recent times, initially for only exchange effective field \cite{Alouges2008}.
Has recently been extended to handle general effective field contributions \cite{Banas2012}.

Advantages:
\begin{itemize}
\item Only need to solve one linear system per step (rather than $\sim 2$ for Newton's method).
\item Jacobian matrix is constant.
\end{itemize}

Disadvantages:
\begin{itemize}
\item Method is limited to first order: so need step sizes orders of magnitude smaller.
\item Have to use specialised time integration scheme (no geometric, adaptive, ... schemes).
\item Specific to the LLG equation: standard code libraries may not work, not as widely understood as standard FEM, extension to related equations (stochastic LLG, LLGS, LLB) is non-trivial.
\end{itemize}

The limitation to first order has been worked around recently, however the higher order schemes lose either stability or linearity \cite{Kritsikis2014}.
Since the advantage of the scheme over standard methods was this unique combination of properties, this appears (to me at least) to not be very useful.

Still a promising line of research though!



\section{Solution of linear systems}
\label{sec:solution-lin-sys}

??ds whole section needs more work

If we are using an implicit time integration scheme with the Newton-Raphson method for linearisation then we are left with the problem of solving a sequence of sparse linear systems.
The problem can be stated as: Given a sparse $n \times n $ matrix $\Am$ and a vector $\bv$, find $\xv$ such that
\begin{equation}
  \label{eq:linear-system}
  \Am \xv = \bv.
\end{equation}
This is a very widely studied problem, see for example \cite{Saad2000}, but efficient techniques for very large $n$ (which corresponds to a large spatial problem and/or good spatial resolution) are complex and problem dependent.


\subsection{Direct methods}
\label{sec:direct-methods}

??ds expand

Construct exact solutions to linear systems of equations

Main example is LU decomposition.

The main advantage of direct methods is their robustness: given a well-scaled, non-singular linear system a well designed direct solver will be able provide an answer.
As such direct solvers are usually the first approach tried when solving a new system of equations.

However as matrix sizes become large direct solvers become increasingly demanding in both time and memory.
The problem is that the inverse of a typical sparse matrix is significantly more dense than the starting matrix, \ie there is some amount of fill-in.
This means that the time and memory requirements for the solution can never scale as $\order{n}$.
The exact level of fill-in depends on the sparsity pattern of the initial matrix, but is typically somewhere in the range $\order{n^{3/2}}$-$\order{n^2}$.


\subsection{Multigrid methods}

??ds need to talk about how it's actually the correction we solve for on the coarser grids...

There are a number of ``simple'' iteration methods which are fairly ineffective when used directly to solve reasonably sized linear systems, but are very cheap to apply since they can be written in a form where each iteration is a matrix vector product.
??ds illconditionedness is the problem not size (Milan)
Examples are Jacobi, Gauss-Seidel and SOR methods \cite[103]{Saad2000}.

Multigrid methods are a class of linear solvers which combine these simple iteration methods with a hierarchical algorithm, resulting in highly efficient and scalable solvers for the solution of discrete elliptical PDEs \cite{multigrid-tut}.
They are based on two observations: firstly that the simple iterative solvers are very effective at reducing the high-frequency (in space) error of a solution, but fail on the low frequency parts.
The second observation is that many of the low frequency errors can be converted to high frequency errors by projecting them onto a coarser grid and solving for a correction to the solution.
So by solving the problem on a sequence of meshes all frequencies of error can be treated effectively (except the very lowest, which can be removed by a direct solve on a very coarse mesh).

Typical multigrid solvers work use something similar to the following algorithm
\begin{itemize}
\item Start with initial guess on finest mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh
\item Apply a few iterations of a simple solver
\item Project the solution onto a coarser mesh \\
  $\vdots$
\item On the coarsest mesh solve the solution directly
\item Project the solution onto a finer mesh
\item Apply a few iterations of a simple solver \\
  $\vdots$
\item End with solution on finest mesh
\end{itemize}
The details of the ``simple'' iterative solver, the projection between meshes, the selection of coarse meshes, the exact order/number of repeats of the cycle etc. all vary between methods.

??ds Milan wants an image here

It turns out that multigrid solvers can achieve computation times which scale as $\order{N \log N}$, \ie their performance does not severely degrade as the number of nodes in the mesh is increased.
??ds memory requirements?

Multigrid methods are effective very effective on the matrix resulting from discretising a Poisson problem.
However they can be extremely fickle with regards to application to other problems, and can be difficult to optimise due to the large number of input parameters.
Fortunately the Laplace operator is in some way the ``most important'' part of many elliptic PDEs since it contains the highest derivatives.

In fact it often turns out that multigrid methods are most effective when combined with another type of solver, as will be discussed below.


\subsection{Krylov solvers}
\label{sec:krylov-solvers}

??ds expand

Another class of iterative linear solvers are the Krylov solvers.
These methods are based on building an approximation to the solution from linear combinations of the Krylov subspace basis set of the matrix $\Am$.

Based on Krylov subspace (polynomial of $\Am$ multiplied by $\xv$).

The size of the Krylov subspace needed for an effective approximation (\ie the number of iterations needed) is strongly dependent on the properties of the matrix, in particular the eigenvalues and eigenvectors.
So for optimal performance a ``preconditioner'' is needed, which brings the eigenvalues of the matrix close to 1.
Essentially the preconditioner needs to be a reasonable but cheap to calculate approximation to the inverse of the matrix (since $\Am^{-1}\Am = \Im$, and the identity matrix is the matrix with all eigenvalues 1).
The construction of a preconditioner which is effective independent of the mesh size is a difficult and problem dependent task. 

% Some work has been done on preconditioning for the LLG by Suess et. al. \cite{Suess2002}.
% Also Banas et. al. \cite{Banas2008} \cite{Banas2010} created a multigrid preconditioner for the Maxwell-LLG equation with a fixed point linearisation method (and their time step sizes are a factor of 20 smaller than a similar algorithm using a Newton-Raphson linearisation on the same problem).



\section{Compatibility of methods}
\label{sec:comp-meth}

While in theory any of the methods discussed above can be combined with any other, there are certain combinations which are problematic.
In this Section we discuss which combinations of methods are particularly compatible or incompatible.

The use of integral methods with implicit time integration is problematic: the integral formulation couples magnetisation at every point to every other point, meaning that the Jacobian representing the derivatives of each magnetisation with respect to each other magnetisation is dense (and hence solution times are extremely slow)!
Explicit time integration methods avoid this issue because no system solve is required.

For problems with very simple geometry (\ie thin films, cuboid shapes) the finite difference method with FFT for magnetostatic calculations is sufficient.
Additionally: for simple geometry there is often no need for well refined meshes, so the problem can be only mildly stiff and explicit time integration methods can be efficient.
This combination of methods is very simple to implement and so is often used for GPU based code \cite{Vansteenkiste2011}.
The most well known example of such a model is OOMMF \cite{oommf-website}.

For problems with more complex geometry the finite element method must be used for spatial discretisation to obtain good accuracy. 
Magnetostatic calculations on the resulting non-uniform mesh are typically carried out using FEM/BEM (nmag \cite{Fischbacher2007}, magpar \cite{Scholz2003}, femme, \ldots) although fastmag uses a non-uniform FFT approach \cite{Chang2011}.
The use of refined meshes to fully resolve the geometry can often result in stiff problems, requiring the use of implicit time integration schemes for good efficiency (see \cref{cha:stiffn-llg-equat} or \cite{Shepherd2014}).
In all ``real world'' models using implicit time integration known to the author the Newton-Raphson method is used for linearisation.
Typically some form of preconditioned Krylov solver is then used to solve the resulting linear systems.


\subsection{Solvers and geometric integration}

It should be noted that the geometric properties of IMR derived in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} are only true up to the accuracy with which we solve the discrete LLG.
This is the tolerance of the linearisation method used if the problem is non-linear or the tolerance of the iterative solver if it is linear. 

Additionally the derivations in \cref{sec:proof-magn-length-ode-imr-llg,sec:proof-energy-prop} assume that all fields are calculated at the midpoint without any additional approximations, \ie that we are using a fully implicit method.
In contrast to this, many existing models use an explicit approximation to the magnetostatic field in order to avoid including a dense contribution due to the long-range magnetostatic interactions\footnote{At least a dense sub-block arises from all accurate magnetostatic calculation methods known to the author, in particular: FEM/BEM, FFT and FMM. See \cref{sec:magn-field-calc} for more details.} in the non-linear solve.
Unfortunately, in this case the energy properties will be lost due to the replacement of $\hop \left[ \mpm \right]$ by an alternative approximation (in a similar way to the semi-explicit modification of IMR).
However, the magnetisation length property will be retained because it only relies on $\mv$ itself being calculated implicitly, and not the effective field.

% ??ds goes in somewhere?
% However when using this method the adaptive time step calculation must be done specially (\ie not using normal adaptivity) since the system of equations contains no information on the magnetostatic field \cite{Schrefl1997}.
% Also stability is no longer guaranteeded because an ad-hoc modification to the time integration method.

One way to allow a completely implicit treatment of the magnetostatic field without a dense contribution was explored by d'Aquino \cite{DAquino2005}.
He used a quasi-Newton method where the dense contribution was simply left out of the Jacobian matrix.
This greatly slows down the convergence of the non-linear solve: in the example given in the paper 10-14 iterations were needed (compared to $\sim2$ with a full Newton method).
Additionally, in the general case it is not clear that this modified Newton method will converge at all!

In \cref{sec:fully-implicit-bem} we describe and demonstrate a novel efficient and robust solver allowing fully implicit magnetostatic calculations.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
