\chapter{Galerkin's Method for the Landau--Lifshitz--Gilbert Equation}
\label{sec:galerk-meth-llg}

\section{Introduction to the finite element method}
\label{sec:intr-finite-ele-diff}

The finite element method allow the \emph{discretisation} of a continuous differential equation to allow for solution by computational methods. 
It allows for more complex geometries than the finite difference method, at the cost of increased complexity and setup time.

A simple problem is used to illustrate these methods: solving the Poisson
equation in one spatial dimension. Find\footnote{A function $g\in C^{k}(0,1)$ if $g$ and it's first $k$ derivatives are continuous on $(0,1)$.} $y(x)\in C^{2}(0,1)$ such that
\begin{equation}
  -y''=f\qquad\text{on }(0,1)
  \label{eq:poisson1}
\end{equation}
\begin{equation*}
  y(0)=y_{a},\quad y(1)=y_{b},\quad f=f(x)\in C(0,1).
\end{equation*}


\subsection{Definitions}
\label{sec:fem-definitions}

The function space $L^{2}$ is the set of all functions $f(x)$ such
that $\intop_{-\infty}^{\infty}\abs{f(x)}^{2}dx$ is finite.

The span of a set of functions is the set of all possible linear combinations of
the functions. If a set of functions $\{\phi_{i}\}$ is such that
$V=\text{span}(\phi_{i})$ then we say that $\{\phi_{i}\}$ spans $V$ and
$\{\phi_{i}\}$ is a basis for the function space $V$. If $V$ has a basis consisting of a finite number of functions then we say that $V$ is finite dimensional, otherwise $V$ is infinite dimensional.

In finite element models we are interested in approximating an infinite
dimensional space (the space of solutions) by a finite dimensional
space (the space of approximations).

The $n$-th Sobelov space is the space of all functions such that they and their $n$-th derivatives (in all dimensions) are in $L^2$. For example
\begin{equation}
  \label{eq:H1}
  \sob^1(\magd) = \{ y \st y, \pd{y}{x_i} \in L^2(\magd) \; \forall x_i \}
\end{equation}

\subsection{The Weak Formulation \& A Discretisation}
\label{Derivation-of-weighted-residuals}

The method of weighted residuals is a way to convert a differential equation
into an integral form that can be modelled computationally. As such it is the
first step in a variety of modelling methods including the finite element method
and the spectral method.

For simplicity we start, as before, with the one-dimensional Poisson equation
\eqref{eq:poisson1}, with homogeneous Dirichlet boundary conditions
(\ie $y_{a}=y_{b}=0$).

% \footnote{The functions must be Lipschitz continous (a stronger form
% of continuity than standard). Lipschitz continuity is defined as the
% following: for $X,Y$ metric spaces with metrics $d_{x}(x_{1},x_{2})$,
% $d_{y}(y_{b},y_{2})$ respectively then a function $f:X\rightarrow Y$ is
% Lipschitz continuous if and only if $\exists K\geq0$ s.t. $\forall
% x_{1},x_{2}\in X$ $d_{y}(f(x_{1}),f(x_{2}))\leq K\, d_{x}(x_{1},x_{2})$.}

The formulation used in \eqref{eq:poisson1} is too restrictive for our purposes:
it disallows some useful approximating functions, and in higher dimensional
cases restricts the domain shapes that can be modelled
\cite{HowardElmanDavidSilvester2006}. However for well behaved functions the
following \emph{weak formulation} is equivalent: find $y\in V_{s}$ such that
\begin{equation}
  -\int_{0}^{1}y''v\, dx=\int_{0}^{1}fv\, dx \qquad \forall v\in V_{T}.
  \label{eq:26}
\end{equation}

Here $V_{T}$ is some appropriate space of \emph{test functions}. The test
functions must satisfy $V_{T}\subseteq \sob^0(0,1)$, in order to ensure that the integrals in \eqref{eq:26} are well defined. Thus
\begin{equation}
  \label{eq:28}
  V_{T}=\{v:[0,1]\rightarrow\mathbb{R}\, \st \, v \in \sob^0(0,1)\}.
\end{equation}

The space $V_{s}$ is the solution space
\begin{equation*}
  V_{s}=\{y:[0,1]\rightarrow\mathbb{R}\, \st \, y \in \sob^2(0,1),\,
  y(0) = y_{a},\, y(1) = y_{b}\},
\end{equation*}
\ie the space of functions that are sufficiently smooth for the integrals to be finite and that satisfy the boundary conditions.

To see that the weak form is equivalent consider the case when the error or \emph{residual} $f(x) -y(x)''$ is non-zero for some $x \in [0,1]$. Since equation~\eqref{eq:26} must hold for all functions $v$ we can come up with a function which is non-zero at $x$ and hence the equality in \eqref{eq:26} fails. We call this the method of weighted residuals because we use a weighted integral of the residual to approximate the true equation.\cite[210, 214]{Zeinkiewicz1967}

The smoothness restrictions on our weak form approximation can be further reduced using integration by parts
\begin{equation*}
  \int_{0}^{1}q\, dp=\Big[pq\Big]_{0}^{1}-\int_{0}^{1}p\, dq,
\end{equation*}
applied to the left hand side of equation~\eqref{eq:26}. Let $p=y'$ and $q=v$, then
\begin{equation}
  -\int_{0}^{1}y''v\, dx=-\Big[y'v\Big]_{0}^{1}+\int_{0}^{1}y'v'\, dx.
  \label{eq:29}
\end{equation}

We now introduce a second constraint to the test functions: $v=0$
at the boundaries (\ie $v_a=v_b=0$), so the bracketed term in \eqref{eq:29} is
zero and we are left with
\begin{equation*}
  -\int_{0}^{1}y''v\, dx=\int_{0}^{1}y'v'\, dx.
\end{equation*}

Hence our problem can be reduced to the following: find $y\in V_{S}'=\{y \st \, y \in \sob^1(0,1),\, y(0) = y_a, y(1) = y_b \}$
such that
\begin{equation}
  \int_{0}^{1}y'v'\, dx=\int_{0}^{1}fv\, dx\qquad\forall v\in V'_{T}=\{v \st \, v \in \sob^1(0,1),\, v_a=v_b=0\}.\label{eq:symmetric-weak-poisson}
\end{equation}

Note that this rearrangement has removed the second derivative in $y$, hence we only require $y \in \sob^1(0,1)$ instead of $y \in \sob^2(0,1)$. This allows a wider range of shape
functions to be used but comes at the expense of requiring more smoothness in
the test functions. Also note that if $y_a = y_b = 0$ then $V_{S}'=V_{T}'$, our
test and solution spaces are the same.

Equation~\eqref{eq:symmetric-weak-poisson} is still inapplicable as a
computational method since the test (and solution) space is infinite dimensional
(\ie the problem is still continuous). So for the next step we approximate
$V_{T}'$ by a finite dimensional function space $V_{T}^{h}\subset V_{T}'$ such
that
\begin{equation}
  V_{T}^{h}=\text{span} \{ \tbf_{\ndi} \}_{n=1}^{N}
  =  \{v_h \st v_h = \sum_{n=0}^{N}\alpha_{\ndi} \tbf_{\ndi},\, \alpha_{\ndi} \in \real,\, v_a = v_b =0 \},
  \label{eq:30}
\end{equation}
for some finite set of basis functions $\tbf_{\ndi}$. Similarly for $V_S$
\begin{equation}
  \label{eq:34}
  V_{S}^{h}=\text{span} \{ \sbf_{l} \}_{l=1}^{N_l}
  =  \{y_h \st y_h = \sum_{l=0}^{N_l}c_{l} \sbf_{l},\,
  c_{l} \in \real,\, y(0) = y_a,\, y(1) = y_b \}.
\end{equation}

We call $\sbf_l$ the \emph{shape functions}. The choice of $\tbf_{\ndi}$
and $\sbf_l$ lead to different modelling methods, the choice corresponding
to the finite element method will be discussed in \autoref{sub:Actual-Finite-Elements}. For now we convert the problem into a general discrete form.

We currently have the \emph{discrete weak formulation} of \eqref{eq:poisson1}: find $y_{h}\in V_{S}^{h}$ such that
\begin{equation}
  \int_{0}^{1}y'_{h}v_{h}'\, dx=\int_{0}^{1}fv_{h}\, dx\qquad\forall v_{h}\in V_{T}^{h}.
  \label{eq:discrete-weak-prob}
\end{equation}

Replacing $v_{h}$ from \eqref{eq:discrete-weak-prob} by $\tbf_{\ndi}$
gives
\begin{equation}
  \int_{0}^{1} y'_{h} \tbf_{\ndi}' \, dx = \int_{0}^{1}f\tbf_{\ndi}\, dx\qquad n=0,\ldots,N,
  \label{eq:discrete_weak_test_fns_replaced}
\end{equation}

Since $y_{h}\in V_{S}^{h}$ we can also replace $y_{h}$ by a linear combination
of the spanning functions, \ie
\begin{equation}
  y_{h}=\sum_{l=0}^{N_l}c_{l}\sbf_{l},
  \label{eq:y-spans}
\end{equation}
where $c_l \in \real$ are the (unknown) coefficients.

Substituting \eqref{eq:y-spans} into equation~\eqref{eq:discrete_weak_test_fns_replaced} gives
\begin{equation}
  \sum_{l=0}^{N_l}c_{l}\int_{0}^{1}\sbf_{l}'\tbf'_{\ndi}\, dx=\int_{0}^{1}f\tbf_{\ndi}\, dx
  \qquad n=0,\ldots,N.
  \label{eq:31}
\end{equation}

The functions $f$, $\tbf$ and $\sbf$ are known and so the integrals in \eqref{eq:31} are just numbers (which must be explicitly calculated at some point). Hence we introduce a matrix $A$ and a vector $\mathbf{b}$ containing these numbers:
\begin{equation}
  \int_{0}^{1}\sbf_{l}'\tbf'_{\ndi}\, dx=A_{l\ndi},\qquad\int_{0}^{1}f\tbf_{\ndi}\, dx=b_{j},\label{eq:Aij_bj}
\end{equation}
and the problem is reduced to a system of linear equations
\begin{equation}
  A\mathbf{c} = \mathbf{b}.
  \label{eq:final_galerkin}
\end{equation}
This can be solved to find the vector of unknown coefficients $\mathbf{c}$ which can be substituted into \eqref{eq:y-spans} to give an approximation for $y$.


\subsection{Finite Elements in One Dimension}
\label{sub:Actual-Finite-Elements}

To create a useable implementation of the methodology derived in \autoref{Derivation-of-weighted-residuals} we must first define a set of basis functions $\tbf_\ndi$ for the space $V_{T}^{h}$ and a set of $\sbf_l$ for the space $V_S^h$. For simplicity we choose the two types of basis functions to be the same: $\sbf_l = \tbf_l$. This choice defines the Galerkin method (and often results in symmetric matrix $A$).\cite[215]{Zeinkiewicz1967}


The overall aim is to approximate an arbitrary function at a finite set of \emph{nodes}, $x_\ndi$, by a linear combination of a finite set of simple functions, $\tbf_\ndi$. There are a variety of useful choices for $\tbf_\ndi$ but we will focus on a carefully constructed set of polynomials -- the Lagrange interpolation basis functions, defined as
\begin{equation*}
  L_\ndi(x)=\prod_{k\neq i}\Bigg(\dfrac{x-x_{k}}{x_\ndi-x_{k}}\Bigg).
\end{equation*}

This definition results in the following useful properties of $L_{\ndi}(x)$
\begin{equation}
  \label{eq:35}
  L_{\ndi}(x_{\ndi}) =
  \begin{cases}
    1 & k = \ndi, \\
    0 & k \neq \ndi,
  \end{cases}
\end{equation}
since at each $x_{\ndi}$ with $k \neq \ndi$ one of the numerators is zero and at
$x_{\ndi}$ all of the numerators are equal to the corresponding denominator.

Polynomials are chosen because they are easy to manipulate computationally, for example differentiation and integration are simple. Also an arbitrary level of accuracy can be achieved when approximating the a smooth function by piecewise polynomials, provided that the polynomials are only non-zero on a sufficiently small area (\ie after sufficient mesh refinement).

We now consider the simple case where we approximate the function at two points
$x_{0}$ and $x_{1}$ (linear interpolation). Then the Lagrange basis
functions are simply
\begin{equation}
  L_{0}=\dfrac{x-x_{1}}{x_{0}-x_{1}},\qquad
  L_{1}=\dfrac{x-x_{0}}{x_{1}-x_{0}}.
  \label{eq:simple_lagrange}
\end{equation}
The interpolation of a function on the interval $[x_{0},x_{1}]$ is then given by
\begin{equation*}
  y_{h}(x)=y(x_{0})L_{0}(x)+y(x_{1})L_{1}(x).
\end{equation*}

It is advantageous to work with such simple functions even for very complex
models because evaluation of integrals and interpolation is quick and easy. We
can achieve this by splitting the domain into a set of $N$ \emph{elements}
$e_{0},e_{1},\ldots,e_{N-1}$, which can be of varying size (in contrast to the
basic finite difference method where the nodes had to be equally spaced). We
then create a \emph{local} numbering scheme for the nodes and functions in each
element and a \emph{global} numbering scheme to keep track of the same nodes and
functions over the entire domain.

In one dimension the boundary of each element consists of two points, one at
each end. In the local scheme it is natural to label these points by the element
number $e$ and a local node number, \ie $x_{0}^{(e)}$ and $x_{1}^{(e)}$. In the
global scheme each node is labelled by a unique node number, \ie $x_{\ndi}$ for $\ndi=0,\ldots,N$.
The concept is best illustrated by a diagram, see \autoref{fig:local-global-numbering}.

\begin{figure}
  \center
  \includegraphics[width=0.8\textwidth]{./images/local_global_numbering}
  \caption{The local and global numbering schemes for nodes in a one-dimensional
    finite element model.}
  \label{fig:local-global-numbering}
\end{figure}

The process of assigning a global node number to each local node is known as the
local to global mapping. Good book keeping of the mapping is essential,
especially in higher dimensions when this it becomes much more complex.

We define the local basis functions in element $e$ as
\begin{equation}
  L_{0}^{(e)}(x)=\dfrac{x-x_{1}^{(e)}}{x_{0}^{(e)}-x_{1}^{(e)}},\qquad
  L_{1}^{(e)}(x)=\dfrac{x-x_{0}^{(e)}}{x_{1}^{(e)}-x_{0}^{(e)}}
  \qquad\text{for }x\in e=[x_{0}^{(e)},x_{1}^{(e)}].
  \label{eq:32}
\end{equation}
Except for the test functions when $\ndi$ is a boundary node. In this case we define $L_\ndi^{(e)} = 0$.
The $\ndi$th global basis function is then defined as:
\begin{equation}
  \label{eq:33}
  L_\ndi(x) =
  \begin{cases}
    L_\ndi^{(e)}(x) & \text{if node $\ndi$ is in element $e$,} \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation}

\begin{figure}
  \center
  % ??ds numbering: change e_i to element e, e_{i+1} to e+1, x_i to x_\ndi, x^(i) to x^(e) L_i to L_\ndi
  \includegraphics[width=0.8\textwidth]{./images/local_global_functions}
  \caption{The linear Lagrange basis functions and numbering schemes for a one-dimensional
    finite element model.\label{fig:local_global_functions}}
\end{figure}

The global basis functions for are clearly integrable (\ie in $L^2[0,1]$) since
they are just a combination of local basis functions and they satisfy the boundary conditions by definition. So the global basis functions are members of $V_T^h$ and $V_S^h$.

To calculate the matrix $A$ and the vector $\mathbf{b}$ from equation
\eqref{eq:Aij_bj} needed for \eqref{eq:final_galerkin} we first calculate the
local contributions from each element: $A^{(e)}$ and $\mathbf{b}^{(e)}$. Then the
local-to-global mapping tells us which local nodes map to a given global node
and so the global $A$ and $\mathbf{b}$ can be assembled by summing the
appropriate local contributions.

For our one-dimensional Poisson case the calculation of $A^{(e)}$ is quite
simple, substituting the two local basis functions \eqref{eq:32} into
\eqref{eq:Aij_bj} and using $h=x_{1}^{(e)}-x_{0}^{(e)}$ we are left with
\begin{equation*}
  A^{(e)} = \dfrac{1}{h}
  \left[
    \begin{array}{cc}
      1 & -1 \\ -1 & 1
    \end{array}
  \right],
\end{equation*}
where $h$ is the element size, $h = x_{1}^{(e)}-x_{0}^{(e)}$ (note that $h$ can
vary between elements). Similarly for $\mathbf{b}^{{e}}$ we obtain
\begin{equation*}
  \mathbf{b}^{(e)}=\dfrac{1}{h}\left[
    \begin{array}{c}
      -\int_{x_{0}^{(e)}}^{x_{1}^{(e)}}(x-x_{1}^{(e)})\, f(x)\, dx\\
      \int_{x_{0}^{(e)}}^{x_{1}^{(e)}}(x-x_{0}^{(e)})\, f(x)\, dx
    \end{array}\right],
\end{equation*}
which we compute numerically since the entries depend on $f(x)$ which is not yet
decided.


\subsection{Extension to other problems}

\subsubsection{Non-Dirichlet Boundary Conditions}
\label{sub:Non-Dirichlet-Boundary-Conditions}

There are two main types of boundary condition. Dirichlet conditions specify the
\emph{value} at the boundary (as used so far). Neumann conditions specify the
\emph{derivative} at the boundary. These two types can also be mixed together
either as conditions on different parts of the boundary or as a linear
combination of the two (a Robin condition).

Neumann boundary conditions can be added to the finite element method described
above by relaxing the conditions on the shape and test functions at the
boundary. Then the first term in equation~\eqref{eq:29} naturally gives an
additional equation for the Neumann condition on each boundary node.

Combinations of Neumann and Dirichlet conditions on different parts of the boundary can be treated the same way, by restricting the shape and test functions only on the Dirichlet parts.


\subsubsection{Higher Order Shape/Test Functions}
\label{sec:fem-high-order-shap}

The shape and test functions can also be chosen to be of quadratic, cubic or higher order. This increases the number of nodes required per element (to ensure uniqueness) and increases the difficulty of calculating the integrals. However in some cases the use of higher order polynomials can give a better approximation.


\subsubsection{Higher Dimensions}
\label{sec:fem-higher-dimensions}

The same basic method as given in \autoref{Derivation-of-weighted-residuals} and \autoref{sub:Actual-Finite-Elements} can be applied to two and three dimensional problems. Different polynomials for the shape/test functions must be chosen but the basic orthogonality property from equation~\eqref{eq:35} is kept. A larger variety of element shapes are possible -- typically triangular or quadrilateral elements are used in two dimensions and their higher dimension equivalents (tetrahedrons and ``bricks'') are used in three dimensions.



\section{Initial Equations}
\label{sec:llg-initial-equations}

We start with the Gilbert form of the Landau--Lifshitz--Gilbert equation~\eqref{eq:Gilbert}, the sum of effective fields \eqref{eq:Heff}, the exchange field \eqref{eq:Hex} and the potential method for calculating the magnetostatic field \eqref{eq:Hms} \& \eqref{eq:nnphim}. Note that for simplicity from now on we do not explicitly specify when things are functions of $\xv$ and $t$.

We choose the Gilbert form of the Landau--Lifshitz--Gilbert equation even though it is less immediately intuitive because it greatly reduces the complexity of all derivatations - the Landau--Lifshitz form contains a double cross product and has two terms containing the potentially complicated total field. A side effect of this choice is that explict timestepping schemes cannot be used (since the time dependence is defined implicitly), but as discussed in \autoref{sec:model-conclusions} we will always use implicit timestepping schemes anyway.

The applied field $\Happ$ is known. The crystalline anisotropy field $\Hca$ depends on the type of anisotropy in the magnetic material but it is always just an algebraic function of $\Mv$. For example the most commonly used case of perpendicular anisotropy (caused by hexagonal crystalline structure) gives equation~\eqref{eq:Hca}.

After non-dimensionalisation (see \autoref{sec:land-lifsh-gilb-normalisation}) the Landau-Lifshitz-Gilbert equation with effective fields is given by
\begin{equation}
  \begin{aligned}
    \dmdt &= - \mv \times \hv + \alpha \mv \times \dmdt, \\
    \hv &= \happ - \nabla \phim + \lap \mv + \kone (\mv \cdot \ev) \ev, \\
    \lap \phim &= \div \mv.
    \label{eqn:ndllg-starting}
  \end{aligned}
\end{equation}

\section{Boundary conditions}
\label{sec:galerkin-bound-cond}

For now we consider the magnetostatic potential only within the magnetic domain, $\magd$, with Neumann or Dirichlet boundary conditions on the boundary, $\boundd$ (see \autoref{sec:hybr-finit-elem} for details of the extension to include the external region). We define $\boundd_D$ to represent the region of the boundary domain where a Dirichlet condition is imposed on the magnetostatic potential. Similarly we define $\boundd_\Neu$ to be the region of the boundary where a Neumann condition is imposed. So $ \nabla \phim(\xv) \cdot \nv = g_\Neu(\xv) \; \forall \xv \in \boundd_\Neu$ and $\phim(\xv) = g_D(\xv) \; \forall \xv \in \boundd_D$. Typically we will have either $\boundd_D = \boundd$ or $\boundd_\Neu = \boundd$. We also define the following function spaces for convenience
\begin{align}
  \label{eq:037}
  \Dfs & = \{ v \st v(\xv) \text{ satisfies the b.c. s } \; \forall \xv \in \boundd_D \}, \\
  \Dfs_0 &= \{ v \st v(\xv) = 0 \; \forall \xv \in \boundd_D \}.
\end{align}

Recall from \autoref{sec:magn-bound-cond} that we have a boundary condition on the magnetisation of the form
\begin{equation}
  \label{eq:m-bc}
  \mv \times \dmdn = \av(\mv, \nv),
\end{equation}
where $\av$ represents any surface anisotropy, typically it is taken as $\av \equiv 0$.
It turns out that this Neumann-like condition is exactly what is needed in the derivation of the residuals.


\section{Conversion to Weak Form Residuals}

We convert some of the above equations into their residual weak forms as described in \autoref{Derivation-of-weighted-residuals}.
Each residual equation used increases the complexity of our system of equations, hence we do not rewrite the simple equations for crystalline anisotropy effective field or for the magnetostatic field (in terms of $\phim$) as residuals.


\subsection{Magnetostatic Field Residuals}
\label{sec:magn-field-resid}

The equation for the magnetostatic potential becomes \eqref{eq:phim} becomes:
\begin{gather}
  \text{given $\mv \in \sob^1(\magd)$, find $\phim \in \sob^2(\magd) \cap \Dfs$ such that:} \notag \\
  \rphi(\phim) = \int_\magd (\lap \phim) \test  \d \magd
  - \int_\magd (\nabla \cdot \mv) \test \d \magd = 0,
  \quad \forall \test \in \sob^0(\magd) \cap \Dfs_0. \label{eqn:phires1}
\end{gather}

The above equation for calculating $\phim$ contains second order derivatives.
We would like to reduce the order of these derivatives as discussed in \autoref{Derivation-of-weighted-residuals} to relieve the smoothness requirements on our solution.
We do this by ``transferring'' the derivatives onto the test functions \cite{HowardElmanDavidSilvester2006}.

First we need the following identity\footnote{This can be easily derived by applying the product rule to $\nabla \cdot (v \nabla \phim)$.}
\begin{equation}
  (\lap \phim) \test =
  \nabla \cdot (v \nabla \phim)
  - \nabla \phim \cdot \nabla v.
  \label{eq:20}
\end{equation}
Then integrating over the magnetic domain $\magd$ and applying the divergence theorem gives
\begin{equation}
  \int_\magd (\lap \phim) \test \d \magd =
  \int_{\boundd} \test (\nabla \phim \cdot \nv) \d \boundd
  - \int_\magd \nabla \phim \cdot \nabla \test \d \magd.
  \label{eqn:identitygauss}
\end{equation}

We now substitute \eqref{eqn:identitygauss} into \eqref{eqn:phires1}, giving
\begin{gather}
  \text{given $\mv \in \sob^1(\magd)$, find $\phim \in \sob^1(\magd) \cap \Dfs$ such that:} \notag \\
  \rphi(\phim) = \int_{\boundd} \test (\nabla \phim \cdot \nv) \d \boundd
  - \int_\magd \nabla \test \cdot \nabla \phim \d \magd
  - \int_\magd (\nabla \cdot \mv) \test \d \magd = 0
  , \notag \\
  \forall \test \in \sob^1(\magd) \cap \Dfs_0. \notag
\end{gather}
This contains only first order derivatives so the solution space for $\phim$ is relaxed to $\sob^1(\magd) \cap \Dfs$. However, all first partial derivatives of the test functions are now required to be integrable, \ie $v \in \sob^1(\magd)$ instead of $v \in \sob^0(\magd)$.

Note that the boundary integral is always zero on the Dirichlet region of the boundary by our definition of the test functions. Hence the boundary integral is only non-zero over $\boundd_{\Neu}$ where we know $(\nabla \phim \cdot \nv) = g_{\Neu}$. Hence we have
\begin{gather}
  \text{given $\mv \in \sob^1(\magd)$, find $\phim \in \sob^1(\magd) \cap \Dfs$ such that:} \notag \\
  \rphi(\phim) = \int_{\boundd_\Neu} \test g_\Neu \d \boundd
  - \int_\magd \nabla \test \cdot \nabla \phim \d \magd
  - \int_\magd (\nabla \cdot \mv) \test \d \magd = 0
  , \label{res:contphi} \\
  \forall \test \in \sob^1(\magd) \cap \Dfs_0. \notag
\end{gather}


\subsection{Landau-Lifshitz-Gilbert Equation Residuals}

For the Landau-Lifshitz-Gilbert equation~\eqref{eqn:ndllg-starting} we have a set of three residuals per test function. For now we sidestep the details of time discretisation by assuming $\dmdt$ to be just another function of $\xv$ that we can solve for.
\begin{gather}
  \text{given $\hv \in \sob^0(\magd)$ and $\mv \in \sob^1(\magd)$ find $\dmdt \in \sob^1(\magd)$ such that:} \notag
  \\
  \rllg(\mv) = \int_\magd \Big( \dmdt
  + (\mv \times \hca) + (\mv \times \happ) \\
  - (\mv \times \nabla \phi) + (\mv \times \lap \mv)
  - \dampc \left( \mv \times \dmdt \right)
  \Big) \cdot \testv \d\magd
  = 0, \label{res:contllg}
  \\
  \forall \testv \in (\sob^0(\magd))^3. \notag
\end{gather}

We again wish to reduce the order of the derivatives, this time on $\mv$ in
\begin{equation}
  I = \int_\magd (\mv \times \lap \mv) \cdot \testv \d\magd.
  \label{eq:46}
\end{equation}
First we reorder the terms to give
\begin{equation}
  \begin{aligned}
    I &= \intd{\lap \mv \cdot (\testv \times \mv)}, \\
      &= \intd{\lap \mv \cdot \bv}, \\
      &= \sum_{i=0}^2 \intd{\lap m_i \cdot b_i},
  \end{aligned}
\end{equation}
which is very similar to \eqref{??ds poisson}. So applying the same operations as used to obtain reduced derivatives in \autoref{Derivation-of-weighted-residuals} and \autoref{sec:magn-field-resid} we obtain
\begin{equation}
  I = \sum_{i=0}^2 \intb{b_i (\grad m_i \cdot \nv)} - \intd{\grad b_i \cdot \grad m_i}.
\end{equation}

It turns out that the first term is exactly the weak form of the boundary condition from \eqref{eq:m-bc}.
\begin{equation}
  \begin{aligned}
    \sum_{i=0}^2 \intb{b_i (\grad m_i \cdot \nv)} 
    &= \intb{\bv \cdot \pd{\mv}{\nv}}, \\
    &=  \intb{(\testv \times \mv) \cdot \pd{\mv}{\nv}}, \\
    &=  \intb{(\mv \times \pd{\mv}{\nv}) \cdot \testv}.
  \end{aligned}
\end{equation}
Hence for the case of no surface anisotropy this term is zero.

Next we need an expression for $\grad b_i$.
To avoid switching to tensor notation it is helpful to define $\grad \av = (\grad a_0, \grad a_1, \grad a_2)$.
Then using the product rule we have
\begin{equation}
  \begin{aligned}
    \grad \bv &= \grad \bigb{\testv \times \mv}, \\
    & = \grad \testv \times \mv + \testv \times \grad \mv.
  \end{aligned}
\end{equation}
Hence
\begin{equation}
  I = - \intd{\grad \mv : \bigb{\grad \testv \times \mv}} 
      - \intd{\grad \mv : \bigb{\testv \times \grad \mv}},
\end{equation}
where ``$:$'' is the componentwise scalar product $\grad \av : \grad \cv = \grad a_0 \cdot \grad b_0 + \grad a_1 \cdot \grad b_1 + \grad a_2 \cdot \grad b_2$.
The second term of this expression is zero by the properties of the triple product, so after reordering some terms we are left with
\begin{equation}
  \label{eq:final-lap-residual}
  I = - \intd{\bigb{\mv \times \grad \mv} : \grad \testv}.
\end{equation}


??ds write about switching to scalar test functions


\section{Spatial Discretisation}
\label{sec:spat-discr-resi}

The next step is to discretise the residuals in space. As in \autoref{Derivation-of-weighted-residuals} and \ref{sub:Actual-Finite-Elements} we replace continuous variables and functions by a basis representation using a finite space of shape functions. We also replace the infinite spaces of test functions used so far by finite dimensional approximations.

We choose the solution space to be the same as the test function space (except for boundary conditions), this choice makes our method a Galerkin method. We also choose the shape/test functions to be the same for all residuals/unknowns. So the infinite dimensional space used for all shape and test functions is $\sob^1(\magd)$ with appropriate boundary conditions.

We then replace the space $\sob^1(\magd)$ by the $N$-dimensional approximation $\ts \subset \sob^1(\magd)$. In this approximation the unknowns $\mv$, $\hex$ and $\phim$ can be represented anywhere in the domain as a sum over the nodal values multiplied by the shape function, $\sk \in \ts$, for that node:
\begin{gather} % \sk = shapefn_k
  \mv = \sum_{k = 0}^{N} \sk \, \mv_k, \quad
  \hex = \sum_{k = 0}^{N} \sk \, \hex_{,k}, \quad
  \phim = \sum_{k = 0}^{N} \sk \, \phim_{,k}.
  \label{eq:unknowns-basis}
\end{gather}
Similarly the test functions can be approximated by a sum over the test basis functions, $\tn \in \ts$, as
\begin{equation}
  \label{eq:47}
  \testv = \sum_{\ndi = 0}^{N} \tn \, a_\ndi.
\end{equation}
Note that in our method $\sbf_k \equiv \tbf_k$, but we continue to write the two functions differently for generality. Also the basis functions for the space of test functions are often simply refered to as the test functions since they are used equivalently.

So substituting the basis representations, \eqref{eq:unknowns-basis} and \eqref{eq:47} into the residuals we obtain a spatially discretised version of the problem.

% Note that we could move the discretised values of the unknowns outside of the integrals because they are constant in space. However we prefer to evaluate values at points within the elements where possible (by integrating using Gaussian quadrature) since some quantities are discontinous at the nodes.\footnote{For example if the basis functions are linear then derivatives are discontinous at the nodes.}

% Also note that we have $7N$ equations in $7N$ unknowns and each of the integrals in the equations can be evaluated only in terms of the shape/test functions, their derivatives, the outward unit normal vector and the Neumann boundary condition. Hence we have a system of algebraic equations which we can solve.

As described in \autoref{sub:Actual-Finite-Elements} we can convert this ``global'' representation into a number of ``local'' representations -- one on each element. We first split the domain into $N_e$ elements. We then define the basis functions such that they are only non-zero on elements in which they are contained (\ie we are using a finite element method). Then the global residuals can be split into the local contributions each element which are easy to calculate since they only depend on nodes within the element.\footnote{Unfortunately this property will be lost to some extent when we introduce the hybrid FEM/BEM in \autoref{sec:hybr-finit-elem}.}

Let $\magd_\eli$ represent the volume of element $e$, let $\boundd_\eli$ represent any part of the boundary of the element which is on $\boundd$ (nothing for most elements). Then the contribution of element $\eli$ to the residuals at node $\ndi$ is exactly as given in equations~\eqref{res:tintro}-\eqref{res:tllg} except that the integrations are performed over $\magd_\eli$ and $\boundd_\eli$ rather than $\magd$ and $\boundd$. Also note that the sums only need to consider values of $k$ such that node $k$ is in element $\eli$ and that residual contributions only need to be calculated for nodes $\ndi$ such that node $\ndi$ is in element $\eli$.


\section{Time Discretisation}
\label{sec:time-discretisation-resi}

To deal with the time derivative in the residual we apply a time discretisation scheme (\ie we use the method of lines). As discussed in \autoref{sec:model-conclusions} we aim to use the implicit midpoint rule in our final model but other methods are useful for comparison.

Let $\dtn$ be the time-step, let $\mv_k^\tl$ denote the value of $\mv_k$ at the $\tl$-th time-step, and consider only the value of $\mv_k$ at a single node. Then the mid-point method is defined by d'Aquino\cite{DAquino2005} as
\begin{equation}
  \label{eq:mid-point-scheme}
  \dmdt \left( \frac{\mv_k^{\tl+1} + \mv_k^\tl}{2} \right) = \frac{\mv_k^{\tl+1} - \mv_k^l}{ \dtn}.
\end{equation}
So by substituting equation~\eqref{eq:mid-point-scheme} into the spatially discretised residuals with $\mv_k = \frac{\mv_k^{\tl+1} + \mv^\tl_k}{2}$ we obtain a fully discretised system of equations in  $\mv_k^{\tl+1}$ and $\mv_k^\tl$.
Alternatively we can write it in terms of half a step of the first order backwards Euler method followed by a simple algebraic update \cite{Malidi2005}
\begin{equation}
  \label{eq:imr-bdf1}
  \begin{aligned}
    \dmdt \left(\mv_k^{l+\half} \right) &= \frac{\mv_k^{l+\half} - \mv_k^{l}}{\dtn/2}, \\
    \mv_k^{l+1} &= 2\mv_k^{l+\half} - \mv_k^l.
  \end{aligned}
\end{equation}


The constant time step second order backwards difference method is defined as\cite{Atkinson2009}
\begin{equation}
  \label{eq:bdf2-scheme}
  \dmdt(\mv_k^{\tl+1}) = \frac{3 \mv_k^{\tl+1} - 4 \mv_k^{\tl} + \mv_k^{\tl-1}}{2\dtn},
\end{equation}
which can similarly be substituted into the spatially discretised residuals with $\mv_k = \mv_k^{\tl+1}$ to obtain a fully discretised system of equations in of $\mv_k^{\tl+1}$, $\mv_k^\tl$ and $\mv_k^{\tl-1}$.


\section{Jacobian calculation}
\label{sec:llg-jacobian-calculation}

To solve this system by a Newton method we also need to know the Jacobian matrix of the residuals differentiated with respect to the variables at the target time step: $\phim_\ik^{\tl+1}$ and $\mv_\ik^{\tl+1}$.

\subsection{Poisson Jacobian}
\label{sec:poisson-jacobian}

Starting from the Poisson residual equation~\eqref{??ds poisson} and dropping terms that obviously contain no dependence on $\phim$ we get
\begin{equation}
  \begin{aligned}
    \label{eq:poisson-jacobian}
    \Am_{n,l} &= \pd{}{\phim_{,l}} \sum_k \intd{ -(\nabla \tbf_n \cdot \nabla \sbf_k) \phim_{,k}}, \\
    &= -\intd{\grad \tbf_n \cdot \grad \sbf_l}.
  \end{aligned}
\end{equation}
This block of the Jacobian is comparatively simple because the Poisson equation is linear. 
It corresponds to the well known discrete Laplacian operator\cite{HowardElmanDavidSilvester2006}.


\subsection{LLG Jacobian}
\label{sec:llg-jacobian}

We first note that the effect of differentiation on a single interpolated value is quite simple
\begin{equation}
  \pd{}{\mv_l} \left( \sum_k \mv_k \sk \right) = \sbf_l \text{I}_3.
\end{equation}
Unfortunately in most of the Jacobian calculations we have multiple terms depending on $\mv$ joined together by a cross product.
The process of differentiating these terms can be made much easier by making use of the ``skew operator'' which represents a cross product as a small matrix-vector multiplication.
The skew operator is given by
\begin{equation}
  \skewm{\av} = \text{skew}(\av) =
  \begin{pmatrix}
    0 & -a_3 & a_2 \\
    a_3 & 0 & -a_1 \\
    -a_2 & a_1 & 0
  \end{pmatrix}.
  \label{eqn:skew}
\end{equation}

Some properties are:
\begin{itemize}
\item Skew-matrix-vector multiplication is a cross product
  \begin{equation}
    \skewm{\av} \cdot \bv
    = \begin{pmatrix}
      0 & -a_3 & a_2 \\
      a_3 & 0 & -a_1 \\
      -a_2 & a_1 & 0
    \end{pmatrix}
    \cdot \threevec{b_1}{b_2}{b_3}
    = \threevec{-a_3b_2 + a_2b_3}{a_3b_1 - a_1b_3}{-a_2b_1 + a_1b_2}
    = \av \times \bv
  \end{equation}

\item No effect on derivatives
  \begin{equation}
    \pd{}{x} \skewm{\av} = \skewm{ \pd{\av}{x} }
  \end{equation}

\item Linearity
  \begin{equation}
    \skewm{\av + \alpha \bv} = \skewm{\av} + \alpha \skewm{\bv}
  \end{equation}

\item Easy to represent the Jacobian of a cross-product:

  \begin{equation}
    \pd{}{a_1} \skewm{\av} \cdot \bv = \begin{pmatrix}
      0 & 0 & 0 \\
      0 & 0 & -1 \\
      0 & 1 & 0
    \end{pmatrix}\bv = \threevec{0}{-b_3}{b_2}.
  \end{equation}
  Note that this is the first column of $-\skewm{\bv}$.
  It turns out that
  \begin{equation}
    \pd{}{\av} \skewm{\av} \cdot \bv = \begin{pmatrix}
      0 & b_3 & -b_2 \\
      -b_3 & 0 & b_1 \\
      b_2 & -b_1 & 0
    \end{pmatrix} = -\skewm{\bv}.
    \label{eq:61}
  \end{equation}

\item Simple behaviour when applied to interpolated values and differentiated:

  Due to linearity and the fact that all entries of $\pd{\mv_{l}}{\mv_{k}}$ are zero unless $l=k$ we have
  \begin{equation}
    \pd{}{\mv_l} \skewm{\intp{\mv}} \cdot \bv = \sum_k \sbf_k \pd{}{\mv_l} \bigb{\skewm{\mv_k} \bv} = \sbf_l \pd{}{\mv_l} \bigb{\skewm{\mv_l} \cdot \bv}.
  \end{equation}

  Then using \eqref{eq:61} we have
  \begin{equation}
    \pd{}{\mv_l} \skewm{\intp{\mv}} \cdot \bv = - \sbf_l\skewm{\bv}.
    \label{eq:diff-skew-interp}
  \end{equation}

\end{itemize}

Now note that combining the points above with the chain rule we get:
  \begin{equation}
    \begin{aligned}
      \pd{}{\mv_l} \bigb{\skewm{\mv}\cdot \gv(\mv)} 
      &= \skewm{\mv}\cdot \pd{\gv(\mv)}{\mv_l} - \sbf_l \skewm{\gv(\mv)}.
    \end{aligned}
  \end{equation}
In the context of the LLG Jacobian $\pd{\gv(\mv)}{\mv_l}$ is typically some scalar multiplied by an identity matrix.
Hence we can obtain a simple representation of most of the terms in the Jacobian in terms of the skew operator.

As a simple example of how this can be used we first examine the differentiation of the $\mv \times (\happ + \hms)$ term of the Landau-Lifshitz equation residual (note that these two fields are not (directly) dependant on $\mv$).
\begin{align}
  \circled{c} &= \intpb{\mv} \times (\happ + \hms), \notag\\
              &= \skewm{\intp{\mv}} \cdot (\happ + \hms).
\end{align}
Now it is easy to see that applying \eqref{eq:diff-skew-interp} gives us the Jacobian contribution
\begin{align}
  \pd{\circled{c}}{\mv_l} = - \sbf_l \skewm{\happ + \hms}.
\end{align}

Written using this skew operator the LLG residual is
\begin{equation}
  \begin{aligned}
    \rllg_n = \int_{\magd}
      & \dmdt \tbf_n \\
      &+ \skewm{\mv} \cdot \left( \happ + \hca - \nabla \phi - \dampc \dmdt
        \right) \tbf_n \\
      &- (\skewm{\mv} \grad \mv) : \grad \tbf_n)
      \d\magd
  \end{aligned}
\end{equation} 


Some more points before the main Jacobian calculation:
\begin{itemize}
\item Note that $\hca$ is a vector function of $\mv$ so differentiating it gives a 3x3 magnetocrystalline anisotropy Jacobian matrix $\Jmca$.

\item 
  \begin{equation}
    \begin{aligned}
      \pd{}{\mv_l} \left(\nabla \mv \compdot \nabla \tbf\right) 
      &= \pd{}{\mv_l} \threevecdup{\left( \sum_k \mv_k \grad \sbf_k \right) \cdot \grad \tbf}, \\
      &=  \sum_k \pd{\mv_k}{\mv_l} \threevecdup{\grad \sbf_k \cdot \grad \tbf}, \\
      &=  \Idm_3 \left(\nabla \sbf_l \cdot \nabla \tbf \right).
    \end{aligned}
  \end{equation}


\item We write
  \begin{equation}
    \Jmts \sbf_l = \pd{}{\mv_l} \left(\pd{\mv}{t} \right),
  \end{equation}
  where $\Jmts$ is some constant matrix that depends on the time integration scheme.
  For BDF2 $\Jmts = \frac{3}{2\dtn} \Idm_3$, for the implicit midpoint rule $\Jmts = \frac{1}{\dtn} \Idm_3$.
\end{itemize}


To simplify the calculations we write the Jacobian as the sum of simpler Jacobians:
\begin{equation}
  \begin{aligned}
    \Fm_{n,l} &= \pd{\rllg_n}{\mv_l} = A_{n,l} + B_{n,l} + C_{n,l}, \\
    A_{n,l} &= \pd{(\text{time residual})_n}{\mv_l}, \\
    B_{n,l} &= \pd{(\text{exchange residual})_n}{\mv_l}, \\
    C_{n,l} &= \pd{(\text{other residuals})_n}{\mv_l}.
  \end{aligned}
\end{equation}


First we calculate the simple Jacobian contribution due to the lone time derivative on the LHS
\begin{equation}
  \begin{aligned}
    A_{n,l} &= \pd{}{\mv_l} \intd{ \dmdt \tbf_n } \\
           &= \Jmts \intd{ \sbf_l \tbf_n}.
  \end{aligned}
\end{equation}
This is a 3x3 block diagonal matrix where each block is a simple mass matrix multiplied by the time integrator weight.

Next we calculate the contribution from damping and the various simple fields
\begin{equation}
  \begin{aligned} 
    C_{n,l} &= \pd{}{\mv_l} \intd{  \skewm{\mv} \cdot
      \left( \happ + \hca - \nabla \phi - \dampc \dmdt
      \right) \tbf_n}, \\
    &= \intd{ \tbf_n \sbf_l \left( -\skewm{\happ + \hca - \nabla \phi - \dampc \dmdt}
       + \dampc \jts \skewm{\mv}\right)}, \\
    &= \skewm{\intd{ \tbf_n  \sbf_l \left( \nabla \phi + \dampc \dmdt
          + \dampc \jts \mv - \happ - \hca \right) }}.
  \end{aligned}
\end{equation}
This is a 3x3 block skew-symmetric matrix where each block is something like a mass matrix multiplied by a collection of field/magnetisation components.

Finally we calculate the most complex term: the exchange contribution
\begin{equation}
  \begin{aligned}
    B_{n,l} &=  \pd{}{\mv_l} \intd{- \skewm{\mv} \cdot (\grad \mv \compdot \grad \tbf_n)} ,\\
    &= \intd{ \sbf_l \skewm{\grad \mv \compdot \grad \tbf_n}
       - \skewm{\mv} \left( \nabla \sbf_l \cdot \nabla \tbf \right)}, \\
     &= \skewm{ \intd{ \sbf_l \left( \grad \mv \compdot \grad \tbf_n \right)
         - \mv \left( \nabla \sbf_l \cdot \nabla \tbf \right)}}.
   \end{aligned}
 \end{equation}
We can simplify this slightly more by expanding the shape function representation and using chain rule
\begin{equation}
  \begin{aligned}
    B_{n,l} &= \skewm{ \sum_k \intd{ \sbf_l \left( \mv_k \grad \sbf_k \compdot \grad \tbf_n \right)
        - \mv_k \sbf_k \left( \nabla \sbf_l \cdot \nabla \tbf \right)}}, \\
    &= \skewm{ \sum_k \intd{ \bigb{(\sbf_l \grad \sbf_k - \sbf_k \grad \sbf_l)
        \cdot \grad \tbf_n}  \mv_k }}. \\
  \end{aligned}
\end{equation} 
So again we have a 3x3 block skew-symmetric matrix.
However this time each block is something like a Laplacian block (two derivatives).


When written in block form the total LLG Jacobian is
\begin{equation}
  \Fm =
  \begin{pmatrix}
    \jts \Mm    & -\Km_z       & \Km_y \\
    \Km_z         & \jts\Mm    & -\Km_x \\
    -\Km_y        & \Km_x        & \jts \Mm
  \end{pmatrix}, 
\end{equation}
where $\Mm$ is the mass matrix
\begin{equation}
  \label{eq:mass-matrix}
  \Mm_{i,j} = \intd{\tbf_i \sbf_j},
\end{equation}
and $\Km_i$ are the various contributions from $B$ and $C$.

\section{LLG-magnetostatics coupling Jacobians}
\label{sec:llg-magn-coupl}

Now we derive the Jacobian entries corresponding to $\Pm = \pd{\rllg}{\phim}$ and $\Qm = \pd{\rphi}{\mv}$.

First $\Pm$, obviously most of the terms in the llg residual are dropped when differentiated with respect to any $\phim_{,i}$ value so the calculation is quite simple
\begin{equation}
  \begin{aligned}
    \Pm_{n,l} &= \pd{\rllg_n}{\phim_l} 
    = \pd{}{\phim_l} \intd{ -\tbf_n \mv \times \grad \phim}, \\
    &= -\intd{ \tbf_n \mv \times \grad \sbf_l}, \\
    &= -\intd{ \tbf_n \threevec{m_y\pd{\sbf_l}{z} - m_z \pd{\sbf_l}{y}}
      {-m_x\pd{\sbf_l}{z} + m_z \pd{\sbf_l}{x}}
      {m_x\pd{\sbf_l}{y} - m_y \pd{\sbf_l}{x}}}.
  \end{aligned}
\end{equation}
Note that this is a $3 \times 1$ block matrix because we are dealing with a vector of three residuals at once.

Next $\Qm$, which will be a $1 \times 3$ block matrix because we are dealing with differentiation by three variables (the three components of $\mv$) at once.
\begin{equation}
  \begin{aligned}
    \Qm_{n,l} &= \pd{\rphi_n}{\mv_l} = \pd{}{\mv_l} \intd{ -\tbf_n \div \mv}, \\
    &= \pd{}{\mv_l} \intd{ \sum_j -\tbf_n \div (\sbf_j \mv_j)}, \\
    &= \pd{}{\mv_l} \intd{ \sum_j -\tbf_n \left( \pd{\sbf_j}{x} m_{x,j} 
        + \pd{\sbf_j}{y} m_{y,j} + \pd{\sbf_j}{z} m_{z,j} \right) }, \\
    &= -\intd{ \tbf_n \left( \pd{\sbf_l}{x}, 
        \pd{\sbf_l}{y}, \pd{\sbf_l}{z} \right) }, \\
    &= -\intd{\tbf_n (\grad \sbf_j)^T}.
  \end{aligned}
\end{equation}

So in block form the complete Jacobian is
\begin{equation}
  \Jm =
  \begin{pmatrix}
    \jts \Mm    & -\Km_z       & \Km_y      & \Pm_x \\
    \Km_z         & \jts\Mm    & -\Km_x     & \Pm_y \\
    -\Km_y        & \Km_x      & \jts \Mm & \Pm_z \\
    \Qm_x       & \Qm_y      & \Qm_z    & \Am     \\
  \end{pmatrix},
\end{equation}
where $\Am$ is the Poisson Jacobian.
Alternatively
\begin{equation}
\Jm =
  \begin{pmatrix}
    \Fm   & \Pm \\
    \Qm   & \Am \\
  \end{pmatrix}.
\end{equation}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
